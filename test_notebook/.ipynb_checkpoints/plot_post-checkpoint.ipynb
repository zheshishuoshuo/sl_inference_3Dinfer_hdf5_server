{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318b27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cafca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T03:21:30.649092Z",
     "iopub.status.busy": "2025-11-14T03:21:30.648249Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from emcee.backends import HDFBackend\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def read_run_metadata(path: str):\n",
    "    lens_num = None\n",
    "    scatter_mag = None\n",
    "    scatter_star = None\n",
    "    n_galaxy = None\n",
    "    eta = None\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if \"metadata\" in f:\n",
    "                md = f[\"metadata\"].attrs\n",
    "                for k in (\"lens_number\", \"n_lens\", \"lens_count\"):\n",
    "                    if k in md:\n",
    "                        lens_num = int(md[k])\n",
    "                        break\n",
    "                if \"scatter_mag\" in md:\n",
    "                    scatter_mag = float(md[\"scatter_mag\"])\n",
    "                if \"scatter_star\" in md:\n",
    "                    scatter_star = float(md[\"scatter_star\"])\n",
    "                if \"n_galaxy\" in md:\n",
    "                    n_galaxy = int(md[\"n_galaxy\"])\n",
    "                if \"eta\" in md:\n",
    "                    eta = bool(md[\"eta\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if lens_num is None:\n",
    "        m = re.search(r\"(\\d+)\\s*lens\", path, re.IGNORECASE)\n",
    "        if m:\n",
    "            lens_num = int(m.group(1))\n",
    "    return lens_num, scatter_mag, scatter_star, n_galaxy, eta\n",
    "\n",
    "\n",
    "def open_backend_from_runfile(path: str) -> HDFBackend:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for g in (\"chains/mcmc\", \"chains\", \"mcmc\"):\n",
    "            if g in f:\n",
    "                return HDFBackend(path, name=g, read_only=True)\n",
    "    return HDFBackend(path, read_only=True)\n",
    "\n",
    "\n",
    "def plot_pair_only(\n",
    "    hdf5_file: str,\n",
    "    discard: int = 1000,\n",
    "    thin: int = 10,\n",
    "    labels: Optional[Iterable[str]] = None,\n",
    "    truths: Optional[Iterable[float]] = None,\n",
    "    width_factor: float = 2.5,\n",
    "    first_dim_factor: float = 16.0,          # ✅ 第一维度单独放大范围\n",
    "    dead_thr: float = 1e-6,\n",
    "    figsize: Optional[Tuple[float, float]] = None,\n",
    "    cmap: str = \"Blues\",\n",
    "    savepath: Optional[str] = None,\n",
    "    bins: int = 60,\n",
    "    show_dead: bool = False\n",
    "):\n",
    "    \"\"\"绘制 MCMC 采样的 Pair 图（下三角 + 对角线），带统一图例。\"\"\"\n",
    "\n",
    "    backend = open_backend_from_runfile(hdf5_file)\n",
    "    chain = backend.get_chain(discard=discard, thin=thin, flat=False)\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "\n",
    "    # 自动 labels\n",
    "    if labels is None:\n",
    "        labels = [fr'$\\theta_{{{i}}}$' for i in range(ndim)]\n",
    "    else:\n",
    "        labels = list(labels)\n",
    "        if len(labels) != ndim:\n",
    "            raise ValueError(f\"labels 长度 {len(labels)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 可选 truths\n",
    "    if truths is not None:\n",
    "        truths = list(truths)\n",
    "        if len(truths) != ndim:\n",
    "            raise ValueError(f\"truths 长度 {len(truths)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 判定死链\n",
    "    stds_per_walker = np.std(chain, axis=0)\n",
    "    dead_mask = np.all(stds_per_walker < dead_thr, axis=1)\n",
    "    live_idx = np.where(~dead_mask)[0].tolist()\n",
    "    if len(live_idx) == 0:\n",
    "        live_idx = list(range(nwalkers))\n",
    "\n",
    "    # 展开活链\n",
    "    chain_live = chain[:, live_idx, :] if (len(live_idx) and not show_dead) else chain\n",
    "    samples = chain_live.reshape(-1, ndim)\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "    means, stds = df.mean(), df.std()\n",
    "\n",
    "    # MAP 估计（KDE mode）\n",
    "    from scipy.stats import gaussian_kde\n",
    "    map_point = []\n",
    "    for d in range(ndim):\n",
    "        kde = gaussian_kde(samples[:, d])\n",
    "        xg = np.linspace(samples[:, d].min(), samples[:, d].max(), 600)\n",
    "        map_val = xg[np.argmax(kde(xg))]\n",
    "        map_point.append(map_val)\n",
    "    map_point = np.array(map_point)\n",
    "\n",
    "    # ✅ 各维度显示范围（第一维度单独放宽）\n",
    "    xlims = {}\n",
    "    for i, lab in enumerate(labels):\n",
    "        mx, sx = means[lab], stds[lab]\n",
    "        fac = width_factor \n",
    "        xlims[lab] = (mx - fac * sx, mx + fac * sx)\n",
    "\n",
    "        # if i == 0 and truths is not None:\n",
    "        #     xlims[lab] =  (-0.06, 0.04)\n",
    "\n",
    "    # ---------- Pair 图 ----------\n",
    "    if figsize is None:\n",
    "        cell = 1.35\n",
    "        fig_w = ndim * cell\n",
    "        fig_h = ndim * cell\n",
    "        figsize = (fig_w, fig_h)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize, dpi=600)\n",
    "    gs = fig.add_gridspec(ndim, ndim, wspace=0.05, hspace=0.05)\n",
    "\n",
    "    color_kde_all = \"#46B48E\"         # KDE / scatter (Galaxy)\n",
    "    color_kde_lens_line = \"#518FDF\"   # Hist line (Lens)\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            if j > i:\n",
    "                continue\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            xlab = labels[j]\n",
    "\n",
    "            if i == j:\n",
    "                # sns.histplot(df[xlab], bins=bins, kde=True, edgecolor=\"none\",\n",
    "                #              alpha=0.55, color=color_kde_lens_line, ax=ax)\n",
    "                sns.histplot(df[xlab], bins=bins, kde=True, edgecolor=\"none\",\n",
    "                               alpha=0.55, color=color_kde_lens_line, ax=ax, stat=\"density\")\n",
    "\n",
    "                ax.set_xlim(xlims[xlab])\n",
    "                # ax.axvline(map_point[i], color=\"orange\", ls=\"-\", lw=1.0)\n",
    "                if truths is not None:\n",
    "                    ax.axvline(truths[i], color=\"red\", ls=\"--\", lw=0.8)\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            else:\n",
    "                ylab = labels[i]\n",
    "                sns.scatterplot(x=df[xlab], y=df[ylab], s=3, alpha=0.30,\n",
    "                                color=color_kde_all, ax=ax)\n",
    "                sns.kdeplot(x=df[xlab], y=df[ylab], fill=True, thresh=1,\n",
    "                            levels=[0.01, 0.05, 0.35, 1.0],\n",
    "                            cmap=cmap, alpha=0.45, ax=ax)\n",
    "                # if truths is not None:\n",
    "                #     ax.plot(truths[j], truths[i], \"r+\", markersize=4, markeredgewidth=0.9)\n",
    "                if truths is not None:\n",
    "                    # ✅ 真值改为横竖交叉红线\n",
    "                    ax.axvline(truths[j], color=\"red\", ls=\"--\", lw=0.8)\n",
    "                    ax.axhline(truths[i], color=\"red\", ls=\"--\", lw=0.8)\n",
    "\n",
    "                ax.set_xlim(xlims[xlab])\n",
    "                ax.set_ylim(xlims[ylab])\n",
    "\n",
    "            # 坐标标签控制\n",
    "            if i < ndim - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xlabel(xlab)\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.tick_params(axis=\"y\", left=False)\n",
    "            else:\n",
    "                if i != j:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # # ✅ 美观微调\n",
    "    # for ax in fig.axes:\n",
    "    #     ax.set_aspect('auto', adjustable='box')\n",
    "    #     ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    #     ax.tick_params(axis='both', which='minor', labelsize=5)\n",
    "    #     ax.xaxis.label.set_size(8)\n",
    "    #     ax.yaxis.label.set_size(8)\n",
    "\n",
    "\n",
    "\n",
    "    # ✅ 美观微调 + 设置 tick（truth ± 3σ）\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.set_aspect('auto', adjustable='box')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=5)\n",
    "        ax.xaxis.label.set_size(8)\n",
    "        ax.yaxis.label.set_size(8)\n",
    "\n",
    "    # ✅ 设置每个参数的 xtick, ytick = truth ± 3σ\n",
    "    if truths is not None:\n",
    "        for d, lab in enumerate(labels):\n",
    "            sigma = stds[lab]\n",
    "            truth = truths[d]\n",
    "            tick_vals = [truth - 3*sigma, truth, truth + 3*sigma]\n",
    "            tick_labels = [f\"{tick_vals[0]:.2f}\", f\"{tick_vals[1]:.2f}\", f\"{tick_vals[2]:.2f}\"]\n",
    "\n",
    "            # 遍历所有子图，凡是涉及该参数，就改 tick\n",
    "            for ax in fig.axes:\n",
    "                # x 轴是该参数\n",
    "                if ax.get_xlabel() == lab:\n",
    "                    ax.set_xticks(tick_vals)\n",
    "                    ax.set_xticklabels(tick_labels, rotation=30)\n",
    "                # y 轴是该参数\n",
    "                if ax.get_ylabel() == lab:\n",
    "                    ax.set_yticks(tick_vals)\n",
    "                    ax.set_yticklabels(tick_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # ✅ 添加统一 Legend\n",
    "    # legend_elements = [\n",
    "    #     Line2D([0], [0], color=color_kde_lens_line, lw=2, label=\"Lens population\"),\n",
    "    #     Line2D([0], [0], marker='s', color='w', label=\"Galaxy population\",\n",
    "    #            markerfacecolor=color_kde_all, markersize=10)\n",
    "    # ]\n",
    "    # fig.legend(\n",
    "    #     handles=legend_elements,\n",
    "    #     loc=\"upper right\",\n",
    "    #     bbox_to_anchor=(0.96, 0.96),\n",
    "    #     fontsize=14,\n",
    "    #     frameon=False,\n",
    "    #     borderpad=1.2,\n",
    "    #     labelspacing=0.8\n",
    "    # )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savepath is not None:\n",
    "        save_dir = Path(os.path.expanduser(savepath))\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", hdf5_file)\n",
    "        if timestamp:\n",
    "            fname = save_dir / f\"pair_only_{timestamp.group(0)}\"\n",
    "        else:\n",
    "            fname = save_dir / \"pair_only\"\n",
    "        out_png = fname.with_suffix(\".png\")\n",
    "        plt.savefig(out_png, dpi=300)\n",
    "        print(f\"图已保存至 {out_png}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_pair_only(\n",
    "    hdf5_file=\"../chains/chains_415lens_20251113T134634Z.h5\",\n",
    "    discard=600,\n",
    "    thin=1,\n",
    "    labels=[\n",
    "        r'$\\alpha_{\\mathrm{sps}}$',\n",
    "        r'$\\mu_{\\mathrm{h}}$',\n",
    "        # r'$\\beta_{\\mathrm{h}}$',\n",
    "        # r'$\\sigma_{\\mathrm{h}}$',\n",
    "        r'$\\mu_{\\gamma}$',\n",
    "    ],\n",
    "    truths=[0., 12.91, 1.0],\n",
    "    width_factor=5,\n",
    "    first_dim_factor=12,\n",
    "    cmap=\"GnBu\",\n",
    "    savepath=\"~/outfig\",\n",
    "    bins=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a915a-fe19-4ca0-8a80-428c3c48f291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577fa34-d8d8-427a-b34f-075f74bab19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chain_with_prior(\n",
    "    hdf5_file: str,\n",
    "    labels: Optional[List[str]] = None,\n",
    "    truths: Optional[List[float]] = None,\n",
    "    priors: Optional[List[Tuple[float, float]]] = None,\n",
    "    discard: int = 0,\n",
    "    thin: int = 1,\n",
    "    figsize: Tuple[float, float] = (8, 6),\n",
    "    alpha: float = 0.5,\n",
    "    savepath: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    绘制 MCMC chain (trace plot)，并显示先验范围。\n",
    "    priors: list of (min, max) for each parameter\n",
    "    \"\"\"\n",
    "    backend = open_backend_from_runfile(hdf5_file)\n",
    "    chain = backend.get_chain(discard=discard, thin=thin, flat=False)\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [fr'$\\theta_{{{i}}}$' for i in range(ndim)]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim, 1, figsize=figsize, sharex=True, dpi=600)\n",
    "\n",
    "    if ndim == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(ndim):\n",
    "        ax = axes[i]\n",
    "        for w in range(nwalkers):\n",
    "            ax.plot(chain[:, w, i], alpha=alpha, lw=0.8)\n",
    "\n",
    "        # 真值\n",
    "        if truths is not None:\n",
    "            ax.axhline(truths[i], color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "\n",
    "        # 先验范围\n",
    "        if priors is not None and priors[i] is not None:\n",
    "            pmin, pmax = priors[i]\n",
    "            ax.axhspan(pmin, pmax, color=\"gray\", alpha=0.2, label=\"Prior range\")\n",
    "\n",
    "        ax.set_ylabel(labels[i])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Step\")\n",
    "\n",
    "    # 图例\n",
    "    handles, labels_ = axes[0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        axes[0].legend(handles, labels_, loc=\"upper right\", fontsize=8, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savepath is not None:\n",
    "        out_path = Path(savepath).expanduser().with_suffix(\".png\")\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        print(f\"Chain plot saved to {out_path}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chain_with_prior(\n",
    "    hdf5_file=\"../chains/chains_76lens_20251111T124144Z.h5\",\n",
    "    labels=[\n",
    "        r'$\\alpha_{\\mathrm{sps}}$',\n",
    "        r'$\\mu_{\\mathrm{h}}$',\n",
    "        r'$\\mu_{\\gamma}$',\n",
    "    ],\n",
    "    truths=[0.0, 12.91, 1.0],\n",
    "    priors=[\n",
    "        (-0.3, 0.3),     # α_sps prior range\n",
    "        (12.0, 14.0),    # μ_h prior range\n",
    "        (0.5, 1.5),      # μ_gamma prior range\n",
    "    ],\n",
    "    discard=00,\n",
    "    thin=2,\n",
    "    figsize=(9, 6),\n",
    "    savepath=\"~/Desktop/outfig/chain_plot\"\n",
    ")\n",
    "    # if not (-0.3 <= alpha_sps <= 0.3):\n",
    "    #     return -np.inf\n",
    "    # if not (12.0 <= mu_h <= 14.0):\n",
    "    #     return -np.inf\n",
    "    # if not (0.5 <= mu_gamma <= 1.5):\n",
    "    #     return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881743d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac74fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from emcee.backends import HDFBackend\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "\n",
    "\n",
    "# ---------- 通用步长（可选） ----------\n",
    "def get_intervals(ndim: int) -> List[float]:\n",
    "    \"\"\"给出每个参数的可选 interval（仅在 truths 存在时作为 ±delta 参考线使用）。\n",
    "    默认返回全 0（即不画 ±interval 线）。如果你有参数边界或栅格步长，可在这里自定义。\n",
    "    \"\"\"\n",
    "    return [0.0] * ndim\n",
    "\n",
    "\n",
    "# ---------- 读取 emcee 运行的一些元数据（若存在） ----------\n",
    "def read_run_metadata(path: str):\n",
    "    lens_num = None\n",
    "    scatter_mag = None\n",
    "    scatter_star = None\n",
    "    n_galaxy = None\n",
    "    eta = None\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if \"metadata\" in f:\n",
    "                md = f[\"metadata\"].attrs\n",
    "                for k in (\"lens_number\", \"n_lens\", \"lens_count\"):\n",
    "                    if k in md:\n",
    "                        lens_num = int(md[k])\n",
    "                        break\n",
    "                if \"scatter_mag\" in md:\n",
    "                    scatter_mag = float(md[\"scatter_mag\"])\n",
    "                if \"scatter_star\" in md:\n",
    "                    scatter_star = float(md[\"scatter_star\"])\n",
    "                if \"n_galaxy\" in md:\n",
    "                    n_galaxy = int(md[\"n_galaxy\"])\n",
    "                if \"eta\" in md:\n",
    "                    eta = bool(md[\"eta\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if lens_num is None:\n",
    "        m = re.search(r\"(\\d+)\\s*lens\", path, re.IGNORECASE)\n",
    "        if m:\n",
    "            lens_num = int(m.group(1))\n",
    "    return lens_num, scatter_mag, scatter_star, n_galaxy, eta\n",
    "\n",
    "\n",
    "# ---------- 打开 emcee 后端（自动探测组名） ----------\n",
    "def open_backend_from_runfile(path: str) -> HDFBackend:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for g in (\"chains/mcmc\", \"chains\", \"mcmc\"):\n",
    "            if g in f:\n",
    "                return HDFBackend(path, name=g, read_only=True)\n",
    "    # 也可能是纯 emcee 后端文件，默认根组\n",
    "    return HDFBackend(path, read_only=True)\n",
    "\n",
    "\n",
    "# ---------- 绘图主函数：自动适配任意维度（含 6D） ----------\n",
    "def plot_pair_and_trace_side_by_side(\n",
    "    hdf5_file: str,\n",
    "    discard: int = 1000,\n",
    "    thin: int = 10,\n",
    "    labels: Optional[Iterable[str]] = None,   # 若为 None，将自动生成为 θ_0, θ_1, ...\n",
    "    truths: Optional[Iterable[float]] = None, # 可选真值\n",
    "    width_factor: float = 2.5,                # 下三角散点/等高线的坐标范围系数（均值 ± width_factor * std）\n",
    "    dead_thr: float = 1e-6,                   # 死链阈值：全维 std < dead_thr\n",
    "    last_steps: Optional[int] = None,         # 右图仅显示最后 N 步\n",
    "    figsize: Optional[Tuple[float, float]] = None,  # 不给的话自动根据维度缩放\n",
    "    cmap: str = \"Blues\",                     # 左下三角 KDE 的 colormap\n",
    "    savepath: Optional[str] = None,           # 保存目录（None 表示不保存）\n",
    "    step_range: Optional[Tuple[int, int]] = None,   # 右侧 trace 仅显示 [start:end)\n",
    "    bins: int = 60,\n",
    "    show_dead: bool = False                   # 是否显示判为“死链”的 walker（默认剔除）\n",
    "):\n",
    "    # 读取链\n",
    "    backend = open_backend_from_runfile(hdf5_file)\n",
    "    chain_all = backend.get_chain(discard=0, thin=thin, flat=False)   # (steps, walkers, ndim)\n",
    "    chain     = backend.get_chain(discard=discard, thin=thin, flat=False)\n",
    "\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "\n",
    "    # 自动 labels\n",
    "    if labels is None:\n",
    "        labels = [fr'$\\theta_{{{i}}}$' for i in range(ndim)]\n",
    "    else:\n",
    "        labels = list(labels)\n",
    "        if len(labels) != ndim:\n",
    "            raise ValueError(f\"labels 长度 {len(labels)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # truths 可选\n",
    "    if truths is not None:\n",
    "        truths = list(truths)\n",
    "        if len(truths) != ndim:\n",
    "            raise ValueError(f\"truths 长度 {len(truths)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 判定死链（对丢弃后的 chain）\n",
    "    stds_per_walker = np.std(chain, axis=0)      # (walkers, ndim)\n",
    "    dead_mask = np.all(stds_per_walker < dead_thr, axis=1)\n",
    "    live_idx = np.where(~dead_mask)[0].tolist()\n",
    "    dead_idx = np.where(dead_mask)[0].tolist()\n",
    "    if len(live_idx) == 0:\n",
    "        # 如果全部死链，则全部当作活链用，避免崩溃\n",
    "        live_idx = list(range(nwalkers))\n",
    "        dead_idx = []\n",
    "\n",
    "    # 采样展开\n",
    "    chain_live = chain[:, live_idx, :] if (len(live_idx) and not show_dead) else chain\n",
    "    samples = chain_live.reshape(-1, ndim)\n",
    "\n",
    "    # 统计量\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "    means, stds = df.mean(), df.std()\n",
    "\n",
    "    # 读取元数据\n",
    "    lens_num, scatter_mag, scatter_star, n_galaxy, eta = read_run_metadata(hdf5_file)\n",
    "\n",
    "    # 自动 figsize（左边为 ndim x ndim 的网格，为保证清晰度适当放大）\n",
    "    if figsize is None:\n",
    "        # 单元格大小 ~ 1.2-1.4 英寸：视屏幕情况可微调\n",
    "        cell = 1.35\n",
    "        left_w = ndim * cell\n",
    "        left_h = ndim * cell\n",
    "        right_w = 4.5\n",
    "        fig_w = left_w + right_w\n",
    "        fig_h = max(left_h, 6.0)\n",
    "        figsize = (fig_w, fig_h)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize, dpi=220)\n",
    "    outer = fig.add_gridspec(1, 2, width_ratios=[ndim, 3.0], wspace=0.28)\n",
    "\n",
    "    # 计算 MAP（逐维 KDE mode）\n",
    "    from scipy.stats import gaussian_kde\n",
    "    map_point = []\n",
    "    for d in range(ndim):\n",
    "        kde = gaussian_kde(samples[:, d])\n",
    "        xg = np.linspace(samples[:, d].min(), samples[:, d].max(), 600)\n",
    "        map_val = xg[np.argmax(kde(xg))]\n",
    "        map_point.append(map_val)\n",
    "    map_point = np.array(map_point)\n",
    "    # print(f\"[MAP (KDE mode)] {dict(zip(labels, map_point))}\")\n",
    "\n",
    "    # 左：Pair（对角 1D，子对角 2D）\n",
    "    gs_left = outer[0, 0].subgridspec(ndim, ndim, wspace=0.05, hspace=0.05)\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            if j > i:\n",
    "                continue\n",
    "            ax = fig.add_subplot(gs_left[i, j])\n",
    "            xlab = labels[j]\n",
    "\n",
    "            if i == j:\n",
    "                sns.histplot(df[xlab], bins=bins, kde=True, edgecolor=\"none\", alpha=0.55, color=\"#518FDF\", ax=ax)\n",
    "                ax.axvline(map_point[i], color=\"orange\", ls=\"-\", lw=1.2, label=\"MAP\")\n",
    "\n",
    "                if truths is not None:\n",
    "                    ax.axvline(truths[i], color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "                    # 可选：68% 和 95% 区间\n",
    "                q16, q84 = np.percentile(df[xlab], [16, 84])\n",
    "                q2p5, q97p5 = np.percentile(df[xlab], [2.5, 97.5])\n",
    "                ax.axvspan(q16, q84, color=\"black\", alpha=0.10, label=\"68% CI\")\n",
    "                ax.axvspan(q2p5, q97p5, color=\"gray\", alpha=0.05, label=\"95% CI\")\n",
    "\n",
    "                # if i == 0:\n",
    "                    # ax.legend(fontsize=5)\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            else:\n",
    "                ylab = labels[i]\n",
    "                sns.scatterplot(x=df[xlab], y=df[ylab], s=3, alpha=0.30, color=\"#46B48E\", ax=ax)\n",
    "                sns.kdeplot(x=df[xlab], y=df[ylab], fill=True, thresh=1, levels=[0.01, 0.05, 0.35, 1.0], cmap=cmap, alpha=0.45, ax=ax)\n",
    "                if truths is not None:\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=5, markeredgewidth=1.1)\n",
    "\n",
    "                mx, sx = means[xlab], stds[xlab]\n",
    "                my, sy = means[ylab], stds[ylab]\n",
    "                ax.set_xlim(mx - width_factor * sx, mx + width_factor * sx)\n",
    "                ax.set_ylim(my - width_factor * sy, my + width_factor * sy)\n",
    "\n",
    "            # 轴刻度收拾\n",
    "            if i < ndim - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xlabel(xlab)\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.tick_params(axis=\"y\", left=False)\n",
    "            else:\n",
    "                if i == j:\n",
    "                    ax.set_ylabel(\"\")\n",
    "                if i != j:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # 右：Trace\n",
    "    gs_right = outer[0, 1].subgridspec(ndim, 1, hspace=0.10)\n",
    "    axes_trace = [fig.add_subplot(gs_right[k, 0]) for k in range(ndim)]\n",
    "\n",
    "    # 选择显示范围\n",
    "    base = chain_all[:, live_idx, :] if (len(live_idx) and not show_dead) else chain_all\n",
    "    if step_range is not None:\n",
    "        start, end = step_range\n",
    "        cshow = base[start:end, :, :]\n",
    "    elif last_steps is not None and last_steps < base.shape[0]:\n",
    "        cshow = base[-last_steps:, :, :]\n",
    "    else:\n",
    "        cshow = base\n",
    "\n",
    "    # 调色板按维度（每维一致颜色，walker 透明）\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=max(10, ndim))\n",
    "    for d, ax in enumerate(axes_trace):\n",
    "        color_d = palette[d % len(palette)]\n",
    "        for w in range(cshow.shape[1]):\n",
    "            ax.plot(cshow[:, w, d], color=color_d, alpha=0.30, lw=0.4)\n",
    "        ax.set_ylabel(labels[d])\n",
    "        if truths is not None:\n",
    "            t = truths[d]\n",
    "            ax.axhline(t, color=\"red\", ls=\"--\", lw=0.8, alpha=0.8)\n",
    "        if d != ndim - 1:\n",
    "            ax.set_xticklabels([])\n",
    "    axes_trace[-1].set_xlabel(\"Step\")\n",
    "\n",
    "    # 顶部标题\n",
    "    if truths is not None:\n",
    "        offsets = map_point - np.asarray(truths)\n",
    "        offset_str = \", \".join([f\"{o:+.3f}\" for o in offsets])\n",
    "    else:\n",
    "        offset_str = \"N/A\"\n",
    "\n",
    "    dead_info = f\"live={len(live_idx)}/\" if (len(live_idx) and not show_dead) else \"live=all/\"\n",
    "    fig.suptitle(\n",
    "        f\"discard={discard}, thin={thin}, {dead_info}{nwalkers}, ndim={ndim}, \\n\"\n",
    "        f\"lenses={lens_num}, mag={scatter_mag}, star={scatter_star}, \\n\"\n",
    "        f\"n_galaxy={n_galaxy}, eta={eta}, offsets={offset_str}\",\n",
    "        y=0.995, fontsize=11\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存\n",
    "    if savepath is not None:\n",
    "        save_dir = Path(os.path.expanduser(savepath))\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", hdf5_file)\n",
    "        if timestamp:\n",
    "            fname = save_dir / f\"pair_trace_{timestamp.group(0)}\"\n",
    "        else:\n",
    "            fname = save_dir / \"pair_trace\"\n",
    "        out_png = fname.with_suffix(\".png\")\n",
    "        plt.savefig(out_png, dpi=300)\n",
    "        print(f\"图已保存至 {out_png}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# (alpha_sps, mu_h, beta_h, sigma_h, mu_gamma, sigma_gamma)\n",
    "# ---------------------------- 使用示例 ----------------------------\n",
    "plot_pair_and_trace_side_by_side(\n",
    "    hdf5_file=\"../chains/chains_74lens_20251113T084632Z.h5\",\n",
    "    discard=500,\n",
    "    thin=1,\n",
    "    labels=[\n",
    "        r'$\\alpha_{\\mathrm{sps}}$',\n",
    "        r'$\\mu_{\\mathrm{h}}$',\n",
    "        # r'$\\beta_{\\mathrm{h}}$',\n",
    "        # r'$\\sigma_{\\mathrm{h}}$',\n",
    "        r'$\\mu_{\\gamma}$',\n",
    "        # r'$\\sigma_{\\gamma}$',\n",
    "        ],\n",
    "    truths=[0., 12.91,\n",
    "     1.0,],\n",
    "    width_factor=5,\n",
    "    dead_thr=1e-6,\n",
    "    last_steps=None,\n",
    "    figsize=None,           # 自动按维度调节\n",
    "    step_range=(0, 1000),\n",
    "    cmap=\"Blues\",                                                                                             \n",
    "    savepath=\"~/Desktop/outfig\",\n",
    "    bins=60,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af06cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# =====================================================\n",
    "# 1. Load HDF5 grid and build interpolator\n",
    "# =====================================================\n",
    "path = \"../aeta_tables/Aeta3D_mu400_mugamma400_alpha400.h5\"     # <<< 修改路径\n",
    "\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    mu_DM_grid = np.array(f[\"grids/mu_DM_grid\"])\n",
    "    mu_gamma_grid = np.array(f[\"grids/mu_gamma_grid\"])\n",
    "    alpha_grid = np.array(f[\"grids/alpha_grid\"])\n",
    "    A_grid = np.array(f[\"grids/A_grid\"])\n",
    "\n",
    "interp = RegularGridInterpolator(\n",
    "    (mu_DM_grid, mu_gamma_grid, alpha_grid),\n",
    "    A_grid,\n",
    "    method=\"linear\",\n",
    "    bounds_error=False,\n",
    "    fill_value=None,\n",
    ")\n",
    "\n",
    "print(\"Loaded A-grid:\", A_grid.shape)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Helper: test a slice along one dimension\n",
    "# =====================================================\n",
    "\n",
    "def test_slice(slice_dim=\"alpha\"):\n",
    "    \"\"\"\n",
    "    slice_dim ∈ {\"mu_DM\", \"mu_gamma\", \"alpha\"}.\n",
    "    \"\"\"\n",
    "    # pick middle grid points for fixed dimensions\n",
    "    mu_DM0 = mu_DM_grid[len(mu_DM_grid)//2]\n",
    "    mu_g0  = mu_gamma_grid[len(mu_gamma_grid)//2]\n",
    "    a0     = alpha_grid[len(alpha_grid)//2]\n",
    "\n",
    "    if slice_dim == \"alpha\":\n",
    "        x_vals = alpha_grid\n",
    "        d = \"alpha\"\n",
    "        # find closest grid indices\n",
    "        i_DM = np.argmin(abs(mu_DM_grid - mu_DM0))\n",
    "        i_g  = np.argmin(abs(mu_gamma_grid - mu_g0))\n",
    "        A_orig = A_grid[i_DM, i_g, :]\n",
    "\n",
    "        # build query points\n",
    "        pts = np.column_stack([\n",
    "            np.full_like(x_vals, mu_DM0),\n",
    "            np.full_like(x_vals, mu_g0),\n",
    "            x_vals\n",
    "        ])\n",
    "\n",
    "    elif slice_dim == \"mu_DM\":\n",
    "        x_vals = mu_DM_grid\n",
    "        d = \"mu_DM\"\n",
    "        i_g  = np.argmin(abs(mu_gamma_grid - mu_g0))\n",
    "        i_a  = np.argmin(abs(alpha_grid - a0))\n",
    "        A_orig = A_grid[:, i_g, i_a]\n",
    "\n",
    "        pts = np.column_stack([\n",
    "            x_vals,\n",
    "            np.full_like(x_vals, mu_g0),\n",
    "            np.full_like(x_vals, a0)\n",
    "        ])\n",
    "\n",
    "    elif slice_dim == \"mu_gamma\":\n",
    "        x_vals = mu_gamma_grid\n",
    "        d = \"mu_gamma\"\n",
    "        i_DM = np.argmin(abs(mu_DM_grid - mu_DM0))\n",
    "        i_a  = np.argmin(abs(alpha_grid - a0))\n",
    "        A_orig = A_grid[i_DM, :, i_a]\n",
    "\n",
    "        pts = np.column_stack([\n",
    "            np.full_like(x_vals, mu_DM0),\n",
    "            x_vals,\n",
    "            np.full_like(x_vals, a0)\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown slice dimension\")\n",
    "\n",
    "    # interpolated values\n",
    "    A_interp = interp(pts)\n",
    "\n",
    "    # absolute & relative errors\n",
    "    abs_err = A_interp - A_orig\n",
    "    rel_err = abs_err / np.where(A_orig == 0, 1, A_orig)\n",
    "\n",
    "    # Plot: original vs interpolated\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_vals, A_orig, \"o\", label=\"original\")\n",
    "    plt.plot(x_vals, A_interp, \"-\", label=\"interp\")\n",
    "    plt.xlabel(d)\n",
    "    plt.ylabel(\"A\")\n",
    "    plt.title(f\"Slice along {d}: original vs interpolated\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot error\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_vals, rel_err, \"o\")\n",
    "    plt.axhline(0, color=\"gray\", lw=1)\n",
    "    plt.xlabel(d)\n",
    "    plt.ylabel(\"relative error\")\n",
    "    plt.title(f\"Relative error along {d}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"==== Slice {d} statistics ====\")\n",
    "    print(\"max abs error:\", np.max(np.abs(abs_err)))\n",
    "    print(\"max rel error:\", np.max(np.abs(rel_err)))\n",
    "    print()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. Run all slice tests\n",
    "# =====================================================\n",
    "\n",
    "test_slice(\"alpha\")\n",
    "test_slice(\"mu_DM\")\n",
    "test_slice(\"mu_gamma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6177f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d84087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# =====================================================\n",
    "# Load HDF5\n",
    "# =====================================================\n",
    "path = \"../aeta_tables/Aeta3D_mu400_mugamma400_alpha400.h5\"\n",
    "\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    mu_DM_grid = np.array(f[\"grids/mu_DM_grid\"])\n",
    "    mu_gamma_grid = np.array(f[\"grids/mu_gamma_grid\"])\n",
    "    alpha_grid = np.array(f[\"grids/alpha_grid\"])\n",
    "    A_grid = np.array(f[\"grids/A_grid\"])\n",
    "\n",
    "interp = RegularGridInterpolator(\n",
    "    (mu_DM_grid, mu_gamma_grid, alpha_grid),\n",
    "    A_grid,\n",
    "    method=\"linear\",\n",
    "    bounds_error=False,\n",
    "    fill_value=None,\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Midpoint generator\n",
    "# =====================================================\n",
    "def midpoints(arr):\n",
    "    return 0.5 * (arr[:-1] + arr[1:])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Slice test helper (midpoint version)\n",
    "# =====================================================\n",
    "def test_slice_mid(slice_dim):\n",
    "    if slice_dim == \"alpha\":\n",
    "        x_vals = midpoints(alpha_grid)\n",
    "        pts = np.column_stack([\n",
    "            np.full_like(x_vals, muDM0),\n",
    "            np.full_like(x_vals, muGamma0),\n",
    "            x_vals\n",
    "        ])\n",
    "\n",
    "    elif slice_dim == \"mu_DM\":\n",
    "        x_vals = midpoints(mu_DM_grid)\n",
    "        pts = np.column_stack([\n",
    "            x_vals,\n",
    "            np.full_like(x_vals, muGamma0),\n",
    "            np.full_like(x_vals, alpha0)\n",
    "        ])\n",
    "\n",
    "    elif slice_dim == \"mu_gamma\":\n",
    "        x_vals = midpoints(mu_gamma_grid)\n",
    "        pts = np.column_stack([\n",
    "            np.full_like(x_vals, muDM0),\n",
    "            x_vals,\n",
    "            np.full_like(x_vals, alpha0)\n",
    "        ])\n",
    "\n",
    "    # interpolated result\n",
    "    A_interp = interp(pts)\n",
    "\n",
    "    # ===== TRUE linear interpolation from table =====\n",
    "    # We know the grid is linear so A_true = manual 1D linear interpolation\n",
    "    A_true = []\n",
    "    for x in x_vals:\n",
    "        # find left/right indices\n",
    "        grid = {\"alpha\": alpha_grid,\n",
    "                \"mu_DM\": mu_DM_grid,\n",
    "                \"mu_gamma\": mu_gamma_grid}[slice_dim]\n",
    "\n",
    "        idx = np.searchsorted(grid, x) - 1\n",
    "        x0, x1 = grid[idx], grid[idx+1]\n",
    "        t = (x - x0) / (x1 - x0)\n",
    "\n",
    "        # extract true table values\n",
    "        if slice_dim == \"alpha\":\n",
    "            A0 = A_grid[i_DM0, i_gamma0, idx]\n",
    "            A1 = A_grid[i_DM0, i_gamma0, idx+1]\n",
    "        elif slice_dim == \"mu_DM\":\n",
    "            A0 = A_grid[idx, i_gamma0, i_alpha0]\n",
    "            A1 = A_grid[idx+1, i_gamma0, i_alpha0]\n",
    "        elif slice_dim == \"mu_gamma\":\n",
    "            A0 = A_grid[i_DM0, idx, i_alpha0]\n",
    "            A1 = A_grid[i_DM0, idx+1, i_alpha0]\n",
    "\n",
    "        A_true.append((1-t)*A0 + t*A1)\n",
    "\n",
    "    A_true = np.array(A_true)\n",
    "\n",
    "    # errors\n",
    "    abs_err = A_interp - A_true\n",
    "    rel_err = abs_err / np.where(A_true == 0, 1, A_true)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_vals, A_true, \"o\", label=\"True(linear from table)\")\n",
    "    plt.plot(x_vals, A_interp, \"-\", label=\"Interp\")\n",
    "    plt.title(f\"Midpoint slice along {slice_dim}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_vals, rel_err, \"o\")\n",
    "    plt.axhline(0, color='gray')\n",
    "    plt.title(f\"Relative error (midpoints) along {slice_dim}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[{slice_dim}] max abs err =\", np.max(np.abs(abs_err)))\n",
    "    print(f\"[{slice_dim}] max rel err =\", np.max(np.abs(rel_err)))\n",
    "    print()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Use your pivot point (nearest grid)\n",
    "# =====================================================\n",
    "target_muDM = 12.91\n",
    "target_muGamma = 1.0\n",
    "target_alpha = 0.0\n",
    "\n",
    "i_DM0 = np.argmin(abs(mu_DM_grid - target_muDM))\n",
    "i_gamma0 = np.argmin(abs(mu_gamma_grid - target_muGamma))\n",
    "i_alpha0 = np.argmin(abs(alpha_grid - target_alpha))\n",
    "\n",
    "muDM0 = mu_DM_grid[i_DM0]\n",
    "muGamma0 = mu_gamma_grid[i_gamma0]\n",
    "alpha0 = alpha_grid[i_alpha0]\n",
    "\n",
    "# =====================================================\n",
    "# Test midpoints (真正检验插值效果)\n",
    "# =====================================================\n",
    "test_slice_mid(\"alpha\")\n",
    "test_slice_mid(\"mu_DM\")\n",
    "test_slice_mid(\"mu_gamma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba598572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0032805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86971715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4c090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c2384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170cae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0f0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14371784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试画出单个sigmagamma的后验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")  # 添加上级目录到路径，方便导入模块\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sl_inference_6Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_6Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_6Dinfer_hdf5.config import SCATTER\n",
    "from sl_inference_6Dinfer_hdf5.likelihood import log_posterior\n",
    "from dataclasses import dataclass\n",
    "\n",
    "  # 导入模块\n",
    "\n",
    "\n",
    "# 设置Matplotlib后端（适用于Jupyter环境）\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DMGrid2D:\n",
    "    logMh: np.ndarray\n",
    "    gamma_h: np.ndarray\n",
    "\n",
    "\n",
    "def build_dm_grid2d(\n",
    "    *,\n",
    "    logMh_min: float = 11.0,\n",
    "    logMh_max: float = 15.0,\n",
    "    n_logMh: int = 30,\n",
    "    gamma_min: float = 0.4,\n",
    "    gamma_max: float = 1.6,\n",
    "    n_gamma: int = 30,\n",
    ") -> DMGrid2D:\n",
    "    \"\"\"Construct a reusable 2D grid over (logMh, gamma_h).\n",
    "\n",
    "    This defines only the parameter axes. No likelihood/tabulation is done here.\n",
    "    \"\"\"\n",
    "    logMh_axis = np.linspace(float(logMh_min), float(logMh_max), int(n_logMh))\n",
    "    gamma_axis = np.linspace(float(gamma_min), float(gamma_max), int(n_gamma))\n",
    "    return DMGrid2D(logMh=logMh_axis, gamma_h=gamma_axis)\n",
    "\n",
    "\n",
    "# Create once and keep module-global for reuse\n",
    "DM_GRID_2D: DMGrid2D = build_dm_grid2d()\n",
    "\n",
    "# 生成2D DM网格，假设你想计算这些参数的似然\n",
    "logMh_min = 11.0\n",
    "logMh_max = 15.0\n",
    "n_logMh = 30\n",
    "gamma_min = 0.4\n",
    "gamma_max = 1.6\n",
    "n_gamma = 30\n",
    "logMh_axis = np.linspace(float(logMh_min), float(logMh_max), int(n_logMh))\n",
    "gamma_axis = np.linspace(float(gamma_min), float(gamma_max), int(n_gamma))\n",
    "\n",
    "# 创建DM网格\n",
    "DM_GRID_2D = DMGrid2D(logMh=logMh_axis, gamma_h=gamma_axis)\n",
    "\n",
    "# 假设给定某个超参数组合\n",
    "logMh_val = 12.91  # 举例：logMh为12.91\n",
    "gamma_h_val = 1.2  # 举例：gamma_h为1.2\n",
    "\n",
    "# 模拟参数\n",
    "logalpha = 0.0\n",
    "n_galaxy = 40000\n",
    "seed = 420\n",
    "print(f\"Generating mock data with {n_galaxy} galaxies, logalpha={logalpha}, seed={seed} ...\")\n",
    "\n",
    "# 生成模拟数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    n_galaxy, logalpha=logalpha, seed=seed, nbkg=4e-4, if_source=True\n",
    ")\n",
    "\n",
    "# 计算似然：使用给定超参数logMh_val, gamma_h_val计算似然\n",
    "grids = tabulate_likelihood_grids(\n",
    "    mock_observed_data,\n",
    "    DM_GRID_2D,\n",
    "    n_jobs=None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_6Dinfer_hdf5.likelihood import log_posterior\n",
    "# def log_posterior(\n",
    "#     eta: Sequence[float],\n",
    "#     grids: Sequence[LensGrid2D],\n",
    "#     *,\n",
    "#     pool: Optional[object] = None\n",
    "# ) -> float:\n",
    "    \n",
    "print(log_posterior([0.0, 12.91, 2.04, 0.37, 1.0, 0.2], grids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1817eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmagammas = np.linspace(0.1, 0.5, 50)\n",
    "posteriorsigmag = []\n",
    "for sigmagamma in sigmagammas:\n",
    "    eta = [0.0, 12.91, 2.04, 0.37, 1.0, sigmagamma]\n",
    "    log_post = log_posterior(eta, grids)\n",
    "    posteriorsigmag.append(log_post)  # 转换为后验概率\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mugammas = np.linspace(0.5, 1.5, 50)\n",
    "posteriormug = []\n",
    "for mugamma in mugammas:\n",
    "    eta = [0.0, 12.91, 2.04, 0.37, mugamma, 0.2]\n",
    "    log_post = log_posterior(eta, grids)\n",
    "    posteriormug.append(log_post)  # 转换为后验概率\n",
    "\n",
    "\n",
    "alphas = np.linspace(-0.1, 0.1, 50)\n",
    "posterioralpha = []\n",
    "for alpha in alphas:\n",
    "    eta = [alpha, 12.91, 2.04, 0.37, 1.0, 0.2]\n",
    "    log_post = log_posterior(eta, grids)\n",
    "    posterioralpha.append(log_post)  # 转换为后验概率\n",
    "\n",
    "\n",
    "muDMs = np.linspace(12.0, 13.5, 50)\n",
    "posteriormuDM = []\n",
    "for muDM in muDMs:\n",
    "    eta = [0.0, muDM, 2.04, 0.37, 1.0, 0.2]\n",
    "    log_post = log_posterior(eta, grids)\n",
    "    posteriormuDM.append(log_post)  # 转换为后验概率\n",
    "\n",
    "betaDMs = np.linspace(1.5, 2.5, 50)\n",
    "posteriorbetaDM = []\n",
    "for betaDM in betaDMs:\n",
    "    eta = [0.0, 12.91, betaDM, 0.37, 1.0, 0.2]\n",
    "    log_post = log_posterior(eta, grids)\n",
    "    posteriorbetaDM.append(log_post)  # 转换为后验概率\n",
    "\n",
    "sigmaDMs = np.linspace(0.1, 0.5, 50)\n",
    "posteriorsigmaDM = []\n",
    "for sigmaDM in sigmaDMs:\n",
    "    eta = [0.0, 12.91, 2.04, sigmaDM, 1.0, 0.2]\n",
    "    log_post = log_posterior(eta, grids)\n",
    "    posteriorsigmaDM.append(log_post)  # 转换为后验概率\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 10))\n",
    "axs[0, 0].plot(sigmagammas, posteriorsigmag)\n",
    "axs[0, 0].set_xlabel('sigma_gamma')\n",
    "axs[0, 0].set_ylabel('Posterior Probability')   \n",
    "axs[0, 0].set_title('Posterior vs. sigma_gamma')\n",
    "axs[0, 0].axvline(0.2, color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "axs[0, 1].plot(mugammas, posteriormug)\n",
    "axs[0, 1].set_xlabel('mu_gamma')\n",
    "axs[0, 1].set_ylabel('Posterior Probability')   \n",
    "axs[0, 1].set_title('Posterior vs. mu_gamma')\n",
    "axs[0, 1].axvline(1.0, color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "axs[1, 0].plot(alphas, posterioralpha)\n",
    "axs[1, 0].set_xlabel('alpha_sps')\n",
    "axs[1, 0].set_ylabel('Posterior Probability')   \n",
    "axs[1, 0].set_title('Posterior vs. alpha_sps')\n",
    "axs[1, 0].axvline(0.0, color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "axs[1, 1].plot(muDMs, posteriormuDM)\n",
    "axs[1, 1].set_xlabel('mu_h')\n",
    "axs[1, 1].set_ylabel('Posterior Probability')   \n",
    "axs[1, 1].set_title('Posterior vs. mu_h')\n",
    "axs[1, 1].axvline(12.91, color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "axs[2, 0].plot(betaDMs, posteriorbetaDM)\n",
    "axs[2, 0].set_xlabel('beta_h')\n",
    "axs[2, 0].set_ylabel('Posterior Probability')   \n",
    "axs[2, 0].set_title('Posterior vs. beta_h')\n",
    "axs[2, 0].axvline(2.04, color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "axs[2, 1].plot(sigmaDMs, posteriorsigmaDM)\n",
    "axs[2, 1].set_xlabel('sigma_h')\n",
    "axs[2, 1].set_ylabel('Posterior Probability')   \n",
    "axs[2, 1].set_title('Posterior vs. sigma_h')\n",
    "axs[2, 1].axvline(0.37, color=\"red\", ls=\"--\", lw=1.0, label=\"Truth\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcdd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d20b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05917794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from emcee.backends import HDFBackend\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def read_run_metadata(path: str):\n",
    "    lens_num = None\n",
    "    scatter_mag = None\n",
    "    scatter_star = None\n",
    "    n_galaxy = None\n",
    "    eta = None\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if \"metadata\" in f:\n",
    "                md = f[\"metadata\"].attrs\n",
    "                for k in (\"lens_number\", \"n_lens\", \"lens_count\"):\n",
    "                    if k in md:\n",
    "                        lens_num = int(md[k])\n",
    "                        break\n",
    "                if \"scatter_mag\" in md:\n",
    "                    scatter_mag = float(md[\"scatter_mag\"])\n",
    "                if \"scatter_star\" in md:\n",
    "                    scatter_star = float(md[\"scatter_star\"])\n",
    "                if \"n_galaxy\" in md:\n",
    "                    n_galaxy = int(md[\"n_galaxy\"])\n",
    "                if \"eta\" in md:\n",
    "                    eta = bool(md[\"eta\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if lens_num is None:\n",
    "        m = re.search(r\"(\\d+)\\s*lens\", path, re.IGNORECASE)\n",
    "        if m:\n",
    "            lens_num = int(m.group(1))\n",
    "    return lens_num, scatter_mag, scatter_star, n_galaxy, eta\n",
    "\n",
    "\n",
    "def open_backend_from_runfile(path: str) -> HDFBackend:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for g in (\"chains/mcmc\", \"chains\", \"mcmc\"):\n",
    "            if g in f:\n",
    "                return HDFBackend(path, name=g, read_only=True)\n",
    "    return HDFBackend(path, read_only=True)\n",
    "\n",
    "\n",
    "def plot_pair_only(\n",
    "    hdf5_file: str,\n",
    "    discard: int = 1000,\n",
    "    thin: int = 10,\n",
    "    labels: Optional[Iterable[str]] = None,\n",
    "    truths: Optional[Iterable[float]] = None,\n",
    "    width_factor: float = 2.5,\n",
    "    first_dim_factor: float = 16.0,          # ✅ 第一维度单独放大范围\n",
    "    dead_thr: float = 1e-6,\n",
    "    figsize: Optional[Tuple[float, float]] = None,\n",
    "    cmap: str = \"Blues\",\n",
    "    savepath: Optional[str] = None,\n",
    "    bins: int = 60,\n",
    "    show_dead: bool = False\n",
    "):\n",
    "    \"\"\"绘制 MCMC 采样的 Pair 图（下三角 + 对角线），带统一图例。\"\"\"\n",
    "\n",
    "    backend = open_backend_from_runfile(hdf5_file)\n",
    "    chain = backend.get_chain(discard=discard, thin=thin, flat=False)\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "\n",
    "    # 自动 labels\n",
    "    if labels is None:\n",
    "        labels = [fr'$\\theta_{{{i}}}$' for i in range(ndim)]\n",
    "    else:\n",
    "        labels = list(labels)\n",
    "        if len(labels) != ndim:\n",
    "            raise ValueError(f\"labels 长度 {len(labels)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 可选 truths\n",
    "    if truths is not None:\n",
    "        truths = list(truths)\n",
    "        if len(truths) != ndim:\n",
    "            raise ValueError(f\"truths 长度 {len(truths)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 判定死链\n",
    "    stds_per_walker = np.std(chain, axis=0)\n",
    "    dead_mask = np.all(stds_per_walker < dead_thr, axis=1)\n",
    "    live_idx = np.where(~dead_mask)[0].tolist()\n",
    "    if len(live_idx) == 0:\n",
    "        live_idx = list(range(nwalkers))\n",
    "\n",
    "    # 展开活链\n",
    "    chain_live = chain[:, live_idx, :] if (len(live_idx) and not show_dead) else chain\n",
    "    samples = chain_live.reshape(-1, ndim)\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "    means, stds = df.mean(), df.std()\n",
    "\n",
    "    # MAP 估计（KDE mode）\n",
    "    from scipy.stats import gaussian_kde\n",
    "    map_point = []\n",
    "    for d in range(ndim):\n",
    "        kde = gaussian_kde(samples[:, d])\n",
    "        xg = np.linspace(samples[:, d].min(), samples[:, d].max(), 600)\n",
    "        map_val = xg[np.argmax(kde(xg))]\n",
    "        map_point.append(map_val)\n",
    "    map_point = np.array(map_point)\n",
    "\n",
    "    # ✅ 各维度显示范围（第一维度单独放宽）\n",
    "    xlims = {}\n",
    "    for i, lab in enumerate(labels):\n",
    "        mx, sx = means[lab], stds[lab]\n",
    "        fac = width_factor \n",
    "        xlims[lab] = (mx - fac * sx, mx + fac * sx)\n",
    "\n",
    "        # if i == 0 and truths is not None:\n",
    "        #     xlims[lab] =  (-0.06, 0.04)\n",
    "\n",
    "    # ---------- Pair 图 ----------\n",
    "    if figsize is None:\n",
    "        cell = 1.35\n",
    "        fig_w = ndim * cell\n",
    "        fig_h = ndim * cell\n",
    "        figsize = (fig_w, fig_h)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize, dpi=600)\n",
    "    gs = fig.add_gridspec(ndim, ndim, wspace=0.05, hspace=0.05)\n",
    "\n",
    "    color_kde_all = \"#46B48E\"         # KDE / scatter (Galaxy)\n",
    "    color_kde_lens_line = \"#518FDF\"   # Hist line (Lens)\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            if j > i:\n",
    "                continue\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            xlab = labels[j]\n",
    "\n",
    "            if i == j:\n",
    "                sns.histplot(df[xlab], bins=bins, kde=True, edgecolor=\"none\",\n",
    "                             alpha=0.55, color=color_kde_lens_line, ax=ax)\n",
    "                ax.set_xlim(xlims[xlab])\n",
    "                # ax.axvline(map_point[i], color=\"orange\", ls=\"-\", lw=1.0)\n",
    "                if truths is not None:\n",
    "                    ax.axvline(truths[i], color=\"red\", ls=\"--\", lw=0.8)\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            else:\n",
    "                ylab = labels[i]\n",
    "                sns.scatterplot(x=df[xlab], y=df[ylab], s=3, alpha=0.30,\n",
    "                                color=color_kde_all, ax=ax)\n",
    "                sns.kdeplot(x=df[xlab], y=df[ylab], fill=True, thresh=1,\n",
    "                            levels=[0.01, 0.05, 0.35, 1.0],\n",
    "                            cmap=cmap, alpha=0.45, ax=ax)\n",
    "                # if truths is not None:\n",
    "                #     ax.plot(truths[j], truths[i], \"r+\", markersize=4, markeredgewidth=0.9)\n",
    "                if truths is not None:\n",
    "                    # ✅ 真值改为横竖交叉红线\n",
    "                    ax.axvline(truths[j], color=\"red\", ls=\"--\", lw=0.8)\n",
    "                    ax.axhline(truths[i], color=\"red\", ls=\"--\", lw=0.8)\n",
    "\n",
    "                ax.set_xlim(xlims[xlab])\n",
    "                ax.set_ylim(xlims[ylab])\n",
    "\n",
    "            # 坐标标签控制\n",
    "            if i < ndim - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xlabel(xlab)\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.tick_params(axis=\"y\", left=False)\n",
    "            else:\n",
    "                if i != j:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # # ✅ 美观微调\n",
    "    # for ax in fig.axes:\n",
    "    #     ax.set_aspect('auto', adjustable='box')\n",
    "    #     ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    #     ax.tick_params(axis='both', which='minor', labelsize=5)\n",
    "    #     ax.xaxis.label.set_size(8)\n",
    "    #     ax.yaxis.label.set_size(8)\n",
    "\n",
    "\n",
    "\n",
    "    # ✅ 美观微调 + 设置 tick（truth ± 3σ）\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.set_aspect('auto', adjustable='box')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=5)\n",
    "        ax.xaxis.label.set_size(8)\n",
    "        ax.yaxis.label.set_size(8)\n",
    "\n",
    "    # ✅ 设置每个参数的 xtick, ytick = truth ± 3σ\n",
    "    if truths is not None:\n",
    "        for d, lab in enumerate(labels):\n",
    "            sigma = stds[lab]\n",
    "            truth = truths[d]\n",
    "            tick_vals = [truth - 3*sigma, truth, truth + 3*sigma]\n",
    "            tick_labels = [f\"{tick_vals[0]:.2f}\", f\"{tick_vals[1]:.2f}\", f\"{tick_vals[2]:.2f}\"]\n",
    "\n",
    "            # 遍历所有子图，凡是涉及该参数，就改 tick\n",
    "            for ax in fig.axes:\n",
    "                # x 轴是该参数\n",
    "                if ax.get_xlabel() == lab:\n",
    "                    ax.set_xticks(tick_vals)\n",
    "                    ax.set_xticklabels(tick_labels, rotation=30)\n",
    "                # y 轴是该参数\n",
    "                if ax.get_ylabel() == lab:\n",
    "                    ax.set_yticks(tick_vals)\n",
    "                    ax.set_yticklabels(tick_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # ✅ 添加统一 Legend\n",
    "    # legend_elements = [\n",
    "    #     Line2D([0], [0], color=color_kde_lens_line, lw=2, label=\"Lens population\"),\n",
    "    #     Line2D([0], [0], marker='s', color='w', label=\"Galaxy population\",\n",
    "    #            markerfacecolor=color_kde_all, markersize=10)\n",
    "    # ]\n",
    "    # fig.legend(\n",
    "    #     handles=legend_elements,\n",
    "    #     loc=\"upper right\",\n",
    "    #     bbox_to_anchor=(0.96, 0.96),\n",
    "    #     fontsize=14,\n",
    "    #     frameon=False,\n",
    "    #     borderpad=1.2,\n",
    "    #     labelspacing=0.8\n",
    "    # )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savepath is not None:\n",
    "        save_dir = Path(os.path.expanduser(savepath))\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", hdf5_file)\n",
    "        if timestamp:\n",
    "            fname = save_dir / f\"pair_only_{timestamp.group(0)}\"\n",
    "        else:\n",
    "            fname = save_dir / \"pair_only\"\n",
    "        out_png = fname.with_suffix(\".png\")\n",
    "        plt.savefig(out_png, dpi=300)\n",
    "        print(f\"图已保存至 {out_png}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_pair_only(\n",
    "    hdf5_file=\"../chains/chains_1238lens_noeta_20251109T122335Z_420.h5\",\n",
    "    discard=500,\n",
    "    thin=10,\n",
    "    labels=[\n",
    "        r'$\\alpha_{\\mathrm{sps}}$',\n",
    "        r'$\\mu_{\\mathrm{h}}$',\n",
    "        r'$\\beta_{\\mathrm{h}}$',\n",
    "        r'$\\sigma_{\\mathrm{h}}$',\n",
    "        r'$\\mu_{\\gamma}$',\n",
    "        r'$\\sigma_{\\gamma}$',\n",
    "    ],\n",
    "    truths=[0., 12.91, 2.04, 0.37, 1.0, 0.2],\n",
    "    width_factor=5,\n",
    "    first_dim_factor=12,\n",
    "    cmap=\"GnBu\",\n",
    "    savepath=\"~/Desktop/outfig\",\n",
    "    bins=60,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4253656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d215e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e11a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42da79d7",
   "metadata": {},
   "source": [
    "Plot Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from emcee.backends import HDFBackend\n",
    "from typing import Iterable, Optional, Tuple\n",
    "\n",
    "def plot_backend_pair_trace(\n",
    "    backend_file: str,\n",
    "    discard: int = 500,\n",
    "    thin: int = 1,\n",
    "    labels: Optional[Iterable[str]] = None,\n",
    "    truths: Optional[Iterable[float]] = None,\n",
    "    width_factor: float = 3.0,\n",
    "    bins: int = 20,\n",
    "    cmap: str = \"Blues\",\n",
    "    last_steps: Optional[int] = None,\n",
    "    figsize: Optional[Tuple[float, float]] = None,\n",
    "    savepath=None\n",
    "):\n",
    "    \"\"\"Pairplot + traceplot for emcee backend only\"\"\"\n",
    "    try:\n",
    "        backend = HDFBackend(backend_file, name=\"mcmc\", read_only=True)\n",
    "    except:\n",
    "        backend = HDFBackend(backend_file, read_only=True)\n",
    "\n",
    "    chain_full = backend.get_chain(discard=discard, thin=thin, flat=False)\n",
    "    nsteps, nwalkers, ndim = chain_full.shape\n",
    "    samples = chain_full.reshape(-1, ndim) if last_steps is None else chain_full[-last_steps:].reshape(-1, ndim)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "\n",
    "    means = np.mean(samples, axis=0)\n",
    "    stds = np.std(samples, axis=0)\n",
    "\n",
    "    if figsize is None:\n",
    "        width = ndim * 1.3 + 4\n",
    "        height = max(ndim * 1.3, 6)\n",
    "        figsize = (width, height)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize, dpi=220)\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[ndim, 3], wspace=0.25)\n",
    "\n",
    "    gs_left = gs[0, 0].subgridspec(ndim, ndim, wspace=0.05, hspace=0.05)\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            ax = fig.add_subplot(gs_left[i, j])\n",
    "            if j > i:\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            if i == j:\n",
    "                sns.histplot(samples[:, j], bins=bins, kde=True, color=\"#5199D9\", alpha=0.6, ax=ax)\n",
    "                if truths is not None:\n",
    "                    ax.axvline(truths[j], color=\"red\", ls=\"--\", lw=1)\n",
    "                ax.set_yticks([])\n",
    "            else:\n",
    "                sns.kdeplot(x=samples[:, j], y=samples[:, i], fill=True, cmap=cmap, levels=15, thresh=0.05, ax=ax)\n",
    "                if truths is not None:\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", ms=5, mew=1.2)\n",
    "                ax.set_xlim(means[j] - width_factor * stds[j], means[j] + width_factor * stds[j])\n",
    "                ax.set_ylim(means[i] - width_factor * stds[i], means[i] + width_factor * stds[i])\n",
    "\n",
    "            if i < ndim - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xlabel(labels[j])\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                ax.set_ylabel(labels[i])\n",
    "\n",
    "    gs_right = gs[0, 1].subgridspec(ndim, 1, hspace=0.15)\n",
    "    for k in range(ndim):\n",
    "        ax = fig.add_subplot(gs_right[k, 0])\n",
    "        for w in range(nwalkers):\n",
    "            ax.plot(chain_full[:, w, k], alpha=0.3, lw=0.4)\n",
    "        if truths is not None:\n",
    "            ax.axhline(truths[k], color=\"red\", ls=\"--\", lw=0.8)\n",
    "        ax.set_ylabel(labels[k])\n",
    "        if k != ndim - 1:\n",
    "            ax.set_xticklabels([])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "\n",
    "    fig.suptitle(f\"emcee backend: {backend_file}\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    if savepath is not None:\n",
    "        save_dir = Path(os.path.expanduser(savepath))\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", backend_file)\n",
    "        if timestamp:\n",
    "            fname = save_dir / f\"pair_trace_{timestamp.group(0)}\"\n",
    "        else:\n",
    "            fname = save_dir / \"pair_trace\"\n",
    "        out_png = fname.with_suffix(\".png\")\n",
    "        plt.savefig(out_png, dpi=300)\n",
    "        print(f\"图已保存至 {out_png}\")\n",
    "\n",
    "    plt.show()\n",
    "plot_backend_pair_trace(\n",
    "    backend_file=\"../chains/chains_1238lens_noeta_20251109T122335Z_420.h5\",\n",
    "    discard=00,\n",
    "    thin=1,\n",
    "    labels=[\n",
    "        r\"$\\alpha_{\\mathrm{sps}}$\", r\"$\\mu_{\\mathrm{h}}$\", r\"$\\beta_{\\mathrm{h}}$\",\n",
    "        r\"$\\sigma_{\\mathrm{h}}$\", r\"$\\mu_{\\gamma}$\", r\"$\\sigma_{\\gamma}$\"\n",
    "    ],\n",
    "    truths=[0., 12.91, 2.04, 0.37,\n",
    "     1.0,\n",
    "     0.2],\n",
    "    last_steps=1500,\n",
    "    width_factor=4,\n",
    "    savepath=\"~/Desktop/outfig\",\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea882398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15e0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c616ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# === 输入文件 ===\n",
    "aeta_file = \"../Aeta6D_balanced_muHi.h5\"  # 换成你的文件\n",
    "\n",
    "# === 读取 A(η) 数据 ===\n",
    "with h5py.File(aeta_file, \"r\") as f:\n",
    "    A = f[\"/A_eta\"][:]                   # 6D array\n",
    "    mu_dm = f[\"/mu_dm_grid\"][:]\n",
    "    beta_dm = f[\"/beta_dm_grid\"][:]\n",
    "    sigma_dm = f[\"/sigma_dm_grid\"][:]\n",
    "    alpha = f[\"/alpha_grid\"][:]\n",
    "    mu_gamma = f[\"/mu_gamma_grid\"][:]\n",
    "    sigma_gamma = f[\"/sigma_gamma_grid\"][:]\n",
    "\n",
    "print(f\"[info] loaded A_eta shape = {A.shape}\")\n",
    "\n",
    "# === 选择切片位置 ===\n",
    "# 方案1：手动指定\n",
    "# i_sigma, i_alpha, i_mug, i_sigg = 2, 2, 2, 2\n",
    "\n",
    "# 方案2：自动选择中间切片（推荐）\n",
    "i_sigma = len(sigma_dm) // 2\n",
    "i_alpha = len(alpha) // 2\n",
    "i_mug = len(mu_gamma) // 2\n",
    "i_sigg = len(sigma_gamma) // 2\n",
    "\n",
    "print(f\"[slice] fixing σ_DM={sigma_dm[i_sigma]:.3f}, α={alpha[i_alpha]:.3f}, μγ={mu_gamma[i_mug]:.3f}, σγ={sigma_gamma[i_sigg]:.3f}\")\n",
    "\n",
    "# === 取切片 A(μ_DM, β_DM) 面 ===\n",
    "A_slice = A[:, :, i_sigma, i_alpha, i_mug, i_sigg]\n",
    "\n",
    "# === 画图 ===\n",
    "plt.figure(figsize=(6.5, 5.5))\n",
    "im = plt.imshow(\n",
    "    A_slice.T,\n",
    "    origin=\"lower\",\n",
    "    extent=[mu_dm.min(), mu_dm.max(), beta_dm.min(), beta_dm.max()],\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "plt.colorbar(im, label=\"A(η)\")\n",
    "plt.xlabel(\"μ_DM\", fontsize=12)\n",
    "plt.ylabel(\"β_DM\", fontsize=12)\n",
    "plt.title(\"Slice of A(η) at fixed σ_DM, α, μγ, σγ\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "✅ TEST START\n",
    "Processing lenses (process=10): 100%|██████████| 1000/1000 [00:01<00:00, 562.05it/s]\n",
    "✅ Mock OK, got lenses: 6\n",
    "Lenses: 100%|██████████| 6/6 [01:08<00:00, 11.43s/it]\n",
    "✅ Tabulation OK, get grids: 6\n",
    "✅ K table loaded\n",
    "✅ logL = -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"A min/max:\", np.min(A), np.max(A))\n",
    "print(\"zeros ratio:\", np.mean(A == 0))\n",
    "\n",
    "import h5py\n",
    "with h5py.File(\"../../bank_uniform1e5.h5\",\"r\") as f:\n",
    "    sel = f[\"/select/pass_selection\"][:]\n",
    "    print(\"selection pass =\", sel.sum(), \"/\", len(sel))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f07f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pipeline.py\n",
    "import numpy as np\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "import sys\n",
    "sys.path.insert(0, project_root)\n",
    "from sl_inference_4Dinfer_hdf5.build_k_table import load_K_interpolator\n",
    "\n",
    "# ==== 文件路径 ====\n",
    "mock_path = \"../../bank_uniform6M_test.h5\"\n",
    "ktable_path = \"../K_mu0_10000_midRes_K_table_mu50000_ms10000.h5\"\n",
    "\n",
    "print(\"==== 加载 mock 数据 ====\")\n",
    "with h5py.File(mock_path, \"r\") as f:\n",
    "    mu1 = f[\"/lens/mu1\"][:]\n",
    "    mu2 = f[\"/lens/mu2\"][:]\n",
    "    betamax = f[\"/lens/betamax\"][:]\n",
    "    sel_flag = f[\"/select/pass_selection\"][:]\n",
    "    print(f\"样本数量: {len(mu1)}\")\n",
    "    print(f\"selection pass 数量: {sel_flag.sum()} / {len(sel_flag)}\")\n",
    "\n",
    "print(\"\\n==== 检查放大率 μ1, μ2 ====\")\n",
    "print(\"μ1 负值比例:\", np.mean(mu1 < 0))\n",
    "print(\"μ2 负值比例:\", np.mean(mu2 < 0))\n",
    "print(\"μ1 范围: \", np.nanmin(mu1), \"→\", np.nanmax(mu1))\n",
    "print(\"μ2 范围: \", np.nanmin(mu2), \"→\", np.nanmax(mu2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import h5py  # 需要加上这个导入\n",
    "\n",
    "# ==== 之前的代码这里略 ====\n",
    "\n",
    "# ==== 查看 selection 通过后的 μ1 μ2 ====\n",
    "valid = sel_flag == 1  # 或者 sel_flag.astype(bool)\n",
    "\n",
    "print(\"\\n==== selection 通过后的 μ1/μ2 分布 ====\")\n",
    "print(f\"有效样本数量: {valid.sum()} / {len(valid)}\")\n",
    "print(\"μ1 范围:\", np.nanmin(mu1[valid]), \"→\", np.nanmax(mu1[valid]))\n",
    "print(\"μ2 范围:\", np.nanmin(mu2[valid]), \"→\", np.nanmax(mu2[valid]))\n",
    "print(\"μ1 负值比例:\", np.mean(mu1[valid] < 0))\n",
    "print(\"μ2 负值比例:\", np.mean(mu2[valid] < 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "mock_path = \"../../bank_uniform6M_test.h5\"   # 换成你的文件\n",
    "\n",
    "with h5py.File(mock_path, \"r\") as f:\n",
    "    mu1 = f[\"/lens/mu1\"][:]\n",
    "    mu2 = f[\"/lens/mu2\"][:]\n",
    "    ms  = f[\"/base/ms\"][:]                   # 源星等\n",
    "    sel = f[\"/select/pass_selection\"][:] > 0 # 通过 selection 的样本\n",
    "\n",
    "# 1) 通过 selection 的样本中，μ2 的最小值所在样本（对应你看到的 ~0.025…）\n",
    "valid_idx = np.where(sel)[0]\n",
    "i_min_in_valid = np.nanargmin(mu2[sel])\n",
    "i0 = valid_idx[i_min_in_valid]\n",
    "\n",
    "print(\"== 最小 μ2 的样本（在 selection 通过者中）==\")\n",
    "print(\"index:\", i0)\n",
    "print(\"mu1:\", float(mu1[i0]))\n",
    "print(\"mu2:\", float(mu2[i0]))\n",
    "print(\"ms :\", float(ms[i0]))\n",
    "\n",
    "# 2) 如果你想“找 μ≈0.02 附近”的一批样本（宽一点，比如 μ2 ∈ [0.02, 0.03)）\n",
    "mask_band = sel & (mu2 >= 0.02) & (mu2 < 0.03)\n",
    "idxs = np.where(mask_band)[0]\n",
    "print(\"\\n== μ2 ∈ [0.02, 0.03) 且通过 selection 的样本数量 ==\", idxs.size)\n",
    "\n",
    "# 只展示前几个，看看它们的 ms、mu1、mu2\n",
    "for j in idxs[:10]:\n",
    "    print(f\"[{j}] mu1={mu1[j]:.6g}, mu2={mu2[j]:.6g}, ms={ms[j]:.3f}\")\n",
    "\n",
    "# 3) 直观检查：把这些样本的像A/B视星等算一下（假设像通量∝μ，视星等 m = ms - 2.5 log10(μ)）\n",
    "#    这样可解释：虽然 μ2 很小，但 μ1 可能很大，导致有像能被探测到\n",
    "def m_from_mu(ms_val, mu_val):\n",
    "    if mu_val <= 0 or not np.isfinite(mu_val):\n",
    "        return np.inf\n",
    "    return ms_val - 2.5*np.log10(mu_val)\n",
    "\n",
    "if idxs.size > 0:\n",
    "    j = idxs[0]\n",
    "    mA = m_from_mu(ms[j], abs(mu1[j]))\n",
    "    mB = m_from_mu(ms[j], abs(mu2[j]))\n",
    "    print(f\"\\n示例样本 {j} 的像视星等估计： mA={mA:.3f}, mB={mB:.3f}, 源星等 ms={ms[j]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8647c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pipeline.py\n",
    "import numpy as np\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "import sys\n",
    "sys.path.insert(0, project_root)\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "from sl_inference_4Dinfer_hdf5.build_k_table import load_K_interpolator\n",
    "from sl_inference_4Dinfer_hdf5.cached_A import cached_A_interp\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.main import DM_GRID_2D\n",
    "\n",
    "print(\"✅ TEST START\")\n",
    "\n",
    "# 1. Try mock data\n",
    "df_lens, mock_lens_data, mock_observed_data, samples= run_mock_simulation(1000, nbkg=4e-4, process=10, if_source=True)\n",
    "print(\"✅ Mock OK, got lenses:\", len(mock_observed_data))\n",
    "\n",
    "# 2. Try tabulation\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, dm_grid=DM_GRID_2D, n_jobs=8)\n",
    "print(\"✅ Tabulation OK, get grids:\", len(grids))\n",
    "\n",
    "# # 3. Try loading K table\n",
    "# K = load_K_interpolator()\n",
    "# print(\"✅ K table loaded\")\n",
    "\n",
    "\n",
    "# # 5. Test likelihood on a dummy η\n",
    "# eta = np.array([0.15, 12.5, 1.6, 0.4, 1.0, 0.1])  # sample hyperparameters\n",
    "# logL = log_likelihood(eta, grids)\n",
    "# print(\"✅ logL =\", logL)\n",
    "\n",
    "# # 3. Try loading K table\n",
    "# K = load_K_interpolator(\"../K_mu0_10000_midRes_K_table_mu50000_ms10000.h5\")\n",
    "# print(\"✅ K table loaded\")\n",
    "\n",
    "\n",
    "# # 5. Test likelihood on a dummy η\n",
    "# eta = np.array([0.15, 12.5, 1.6, 0.4, 1.0, 0.1])  # sample hyperparameters\n",
    "# logL = log_likelihood(eta, grids)\n",
    "# print(\"✅ logL =\", logL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a65e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.lens_solver import solve_lens_parameters_from_obs\n",
    "from sl_inference_4Dinfer_hdf5.main import DM_GRID_2D\n",
    "\n",
    "print(\"=== STEP 1: 生成 mock 数据 ===\")\n",
    "df_lens, mock_lens_data, mock_observed_data, samples = run_mock_simulation(\n",
    "    1000, nbkg=4e-4, if_source=True\n",
    ")\n",
    "print(f\"mock_observed_data shape = {mock_observed_data.shape}\")\n",
    "print(mock_observed_data.head())\n",
    "\n",
    "# 过滤无效行\n",
    "mock_observed_data = mock_observed_data.dropna()\n",
    "print(f\"有效 lens 数量 = {len(mock_observed_data)}\")\n",
    "\n",
    "print(\"\\n=== STEP 2: 测试 solve_lens_parameters_from_obs 是否正常 ===\")\n",
    "row = mock_observed_data.iloc[0]\n",
    "\n",
    "mock_lens_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74257f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = mock_lens_data.iloc[0]\n",
    "logM_star_solved, beta_unit, caustic_max, muA, muB = solve_lens_parameters_from_obs(\n",
    "    xA_obs=row[\"xA\"],\n",
    "    xB_obs=row[\"xB\"],\n",
    "    logRe_obs=row[\"logRe\"],\n",
    "    logM_halo=row[\"logM_halo\"],\n",
    "    zl=row[\"zl\"],\n",
    "    zs=row[\"zs\"],\n",
    "    gamma_in=row[\"gamma_in\"],\n",
    ")\n",
    "print(f\"Solved logM_star = {logM_star_solved}, beta_unit = {beta_unit}, caustic_max = {caustic_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf4e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d321b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = mock_observed_data.iloc[0]\n",
    "xA, xB = row['xA'], row['xB']\n",
    "logRe = row['logRe']\n",
    "zl, zs = 0.3, 2.0\n",
    "\n",
    "fail = 0\n",
    "total = 0\n",
    "for logMh in DM_GRID_2D.logMh:\n",
    "    for gamma in DM_GRID_2D.gamma_h:\n",
    "        total += 1\n",
    "        try:\n",
    "            res = solve_lens_parameters_from_obs(xA, xB, logRe, logMh, zl, zs, gamma)\n",
    "            if np.any(np.isnan(res)):\n",
    "                fail += 1\n",
    "        except:\n",
    "            fail += 1\n",
    "\n",
    "print(f\"测试 solve_lens_parameters_from_obs: 失败率 = {fail}/{total} = {fail/total:.2%}\")\n",
    "\n",
    "print(\"\\n=== STEP 3: 跑 tabulate_likelihood_grids ===\")\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, DM_GRID_2D, n_jobs=1)\n",
    "\n",
    "g0 = grids[0]\n",
    "print(\"\\n第一个 lens 的 tabulation 结果统计：\")\n",
    "print(\"logM_star_true NaN比例:\", np.isnan(g0.logM_star_true).mean())\n",
    "print(\"muA NaN比例:\", np.isnan(g0.muA).mean())\n",
    "print(\"muB NaN比例:\", np.isnan(g0.muB).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5271fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df0304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdec2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab55b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ec2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bd1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import _solve_logMstar_and_magnifications_2d\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.main import DM_GRID_2D\n",
    "\n",
    "# 取最小样本\n",
    "df_lens, _, mock_obs, _ = run_mock_simulation(1000, process=1, if_source=True)\n",
    "lens = mock_obs.iloc[0]  # 第一个 lens\n",
    "\n",
    "# print(\"Testing lens_id =\", lens[\"lens_id\"])\n",
    "\n",
    "# 尝试生成二维表（不并行）\n",
    "logM_star_true, muA, muB = _solve_logMstar_and_magnifications_2d(\n",
    "    xA=lens[\"xA\"],\n",
    "    xB=lens[\"xB\"],\n",
    "    logRe=lens[\"logRe\"],\n",
    "    logMh_axis=DM_GRID_2D.logMh,\n",
    "    gamma_axis=DM_GRID_2D.gamma_h,\n",
    "    zl=0.3,\n",
    "    zs=2.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "logM_star_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9844f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _solve_logMstar_and_magnifications_2d(\n",
    "    xA: float,\n",
    "    xB: float,\n",
    "    logRe: float,\n",
    "    logMh_axis: np.ndarray,\n",
    "    gamma_axis: np.ndarray,\n",
    "    zl: float,\n",
    "    zs: float,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Stable per-grid solution of logM_star_true and magnifications using LensModel.\n",
    "\n",
    "    For each (logMh, gamma_h) grid point, we:\n",
    "      - Build a halo-only LensModel (negligible stellar mass) to obtain s_cr and halo Sigma splines.\n",
    "      - Compute halo deflections alpha_halo(xA), alpha_halo(xB) via splint of Sigma(R) * R.\n",
    "      - Use the analytic two-image solution to obtain M_star_true.\n",
    "      - Build a full LensModel with that M_star_true and compute muA, muB.\n",
    "    \"\"\"\n",
    "    logMh_axis = np.asarray(logMh_axis, dtype=float)\n",
    "    gamma_axis = np.asarray(gamma_axis, dtype=float)\n",
    "    nMh = logMh_axis.size\n",
    "    nG = gamma_axis.size\n",
    "\n",
    "    Re = 10.0 ** float(logRe)  # [kpc]\n",
    "    xA_abs = abs(float(xA))\n",
    "    xB_abs = abs(float(xB))\n",
    "\n",
    "    # Precompute deV unit responses (independent of DM params)\n",
    "    # These use s_cr per grid point; so compute alpha_star_unit inside loop after s_cr known.\n",
    "    sigma_star_unit_A_const = deV.Sigma(xA_abs, Re)\n",
    "    sigma_star_unit_B_const = deV.Sigma(xB_abs, Re)\n",
    "\n",
    "    # Allocate outputs\n",
    "    logM_star_true = np.full((nMh, nG), np.nan, dtype=float)\n",
    "    muA = np.full((nMh, nG), np.nan, dtype=float)\n",
    "    muB = np.full((nMh, nG), np.nan, dtype=float)\n",
    "\n",
    "    for i, logMh in enumerate(logMh_axis):\n",
    "        for j, gamma_in in enumerate(gamma_axis):\n",
    "            try:\n",
    "                # Halo-only model to fetch halo SigmaR spline and s_cr\n",
    "                tmp_model = LensModel(\n",
    "                    logM_star=-99.0,\n",
    "                    logM_halo=float(logMh),\n",
    "                    logRe=float(logRe),\n",
    "                    zl=float(zl),\n",
    "                    zs=float(zs),\n",
    "                    gamma_in=float(gamma_in),\n",
    "                    c=DEFAULT_C,\n",
    "                )\n",
    "                s_cr = tmp_model.s_cr\n",
    "                sigmaR_spline = tmp_model.sigmaR_spline\n",
    "\n",
    "                def alpha_star_unit(x):\n",
    "                    return deV.fast_M2d(abs(x) / Re) / (np.pi * x * s_cr)\n",
    "\n",
    "                def alpha_halo(x):\n",
    "                    m2d_halo = 2 * np.pi * splint(0.0, abs(x), sigmaR_spline)\n",
    "                    return m2d_halo / (np.pi * x * s_cr)\n",
    "\n",
    "                denom = (alpha_star_unit(xA_abs) - alpha_star_unit(xB_abs))\n",
    "                if not np.isfinite(denom) or abs(denom) < 1e-30:\n",
    "                    continue\n",
    "                M_star = ((xA_abs - xB_abs) + (alpha_halo(xB_abs) - alpha_halo(xA_abs))) / denom\n",
    "                if not np.isfinite(M_star) or M_star <= 0:\n",
    "                    continue\n",
    "                logM_star = float(np.log10(M_star))\n",
    "\n",
    "                # Full model with solved stellar mass\n",
    "                model = LensModel(\n",
    "                    logM_star=logM_star,\n",
    "                    logM_halo=float(logMh),\n",
    "                    logRe=float(logRe),\n",
    "                    zl=float(zl),\n",
    "                    zs=float(zs),\n",
    "                    gamma_in=float(gamma_in),\n",
    "                    c=DEFAULT_C,\n",
    "                )\n",
    "                mu1 = model.mu_from_rt(xA_abs)\n",
    "                mu2 = model.mu_from_rt(xB_abs)\n",
    "\n",
    "                # Store\n",
    "                logM_star_true[i, j] = logM_star\n",
    "                muA[i, j] = mu1 if np.isfinite(mu1) else np.nan\n",
    "                muB[i, j] = mu2 if np.isfinite(mu2) else np.nan\n",
    "            except Exception:\n",
    "                # leave NaNs on failure\n",
    "                continue\n",
    "\n",
    "    return logM_star_true, muA, muB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b83074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")  # 根据你的 notebook 路径设置\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.lens_model import LensModel\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.lens_solver import solve_single_lens, solve_lens_parameters_from_obs\n",
    "\n",
    "\n",
    "model = LensModel(logM_star=11.5, logM_halo=13.0, c=5, logRe=0.8,gamma_in=1, zl=0.3, zs=2.0)\n",
    "xA, xB = solve_single_lens(model,0.5)\n",
    "xA, xB\n",
    "\n",
    "# def solve_lens_parameters_from_obs(xA_obs, xB_obs, logRe_obs, logM_halo, zl, zs, gamma_in: float = 1.0, c=None):\n",
    "logM_star_solved, beta_unit, caustic_max= solve_lens_parameters_from_obs(xA, xB, 0.8, 13, 0.3, 2, gamma_in=1)\n",
    "print(logM_star_solved, beta_unit, caustic_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .lens_model import LensModel\n",
    "# from scipy.interpolate import splrep, splint\n",
    "# from ..sl_cosmology import Dang, Mpc, c, G, M_Sun, rhoc\n",
    "# import numpy as np\n",
    "# from ..sl_profiles import nfw, gnfw, deVaucouleurs as deV\n",
    "# from scipy.optimize import brentq\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")  # 根据你的 notebook 路径设置\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.lens_model import LensModel\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.lens_solver import solve_single_lens, solve_lens_parameters_from_obs\n",
    "\n",
    "\n",
    "model = LensModel(logM_star=11.5, logM_halo=13.0, c=5, logRe=0.8,gamma_in=1, zl=0.3, zs=2.0)\n",
    "xA, xB = solve_single_lens(model,0.5)\n",
    "xA, xB\n",
    "from sl_inference_4Dinfer_hdf5.sl_cosmology import Dang, Mpc, c, G, M_Sun, rhoc\n",
    "from sl_inference_4Dinfer_hdf5.sl_profiles import nfw, gnfw, deVaucouleurs as deV\n",
    "from scipy.interpolate import splrep, splint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# (7.54723560589012, -3.2173453450811778)\n",
    "\n",
    "\n",
    "logRe_obs = 0.8\n",
    "logM_halo = 13.0\n",
    "zl = 0.3\n",
    "zs = 2.0\n",
    "gamma_in = 1.0\n",
    "c_halo = 5.0\n",
    "xA_obs = xA\n",
    "xB_obs = xB\n",
    "\n",
    "\"\"\"\n",
    "用 gNFW 替代 NFW，并加入 gamma_in（暗物质内斜率）\n",
    "\"\"\"\n",
    "# ---------- 星系参数 ----------\n",
    "Re = 10**logRe_obs  # [kpc]\n",
    "M_halo = 10**logM_halo  # [Msun]\n",
    "\n",
    "# ---------- 计算 cosmology ----------\n",
    "dd = Dang(zl)  # [Mpc]\n",
    "ds = Dang(zs)  # [Mpc]\n",
    "dds = Dang(zs, zl)  # [Mpc]\n",
    "kpc = Mpc / 1000.  # [kpc/Mpc]\n",
    "s_cr = c**2 / (4*np.pi*G) * ds/dds/dd / Mpc / M_Sun * kpc**2  # [Msun/kpc^2]\n",
    "rhoc_z = rhoc(zl)\n",
    "\n",
    "# ---------- gNFW halo ----------\n",
    "r200 = (M_halo * 3./(4*np.pi*200*rhoc_z))**(1./3.) * 1000.  # [kpc]\n",
    "rs = r200 / c_halo  # scale radius\n",
    "gnfw_norm = M_halo / gnfw.M3d(r200, rs, gamma_in)\n",
    "\n",
    "R2d = np.logspace(-3, 2, 1001)  # [R/Re] 无单位\n",
    "Rkpc = R2d * rs  # [kpc]\n",
    "Sigma_halo = gnfw_norm * gnfw.fast_Sigma(Rkpc, rs, gamma_in)\n",
    "sigmaR_spline = splrep(Rkpc, Sigma_halo * Rkpc)\n",
    "\n",
    "# ---------- alpha ----------\n",
    "def alpha_star_unit(x):\n",
    "    return deV.fast_M2d(abs(x)/Re) / (np.pi * x * s_cr)\n",
    "\n",
    "def alpha_halo(x):\n",
    "    m2d = 2*np.pi * splint(0., abs(x), sigmaR_spline)\n",
    "    return m2d / (np.pi * x * s_cr)\n",
    "\n",
    "# ---------- 解 M_star ----------\n",
    "M_star_solved = ((xA_obs - xB_obs) + alpha_halo(xB_obs) - alpha_halo(xA_obs)) / \\\n",
    "                (alpha_star_unit(xA_obs) - alpha_star_unit(xB_obs))\n",
    "\n",
    "beta_solved = -(alpha_star_unit(xA_obs)*(xB_obs-alpha_halo(xB_obs)) -\n",
    "                alpha_star_unit(xB_obs)*(xA_obs-alpha_halo(xA_obs))) / \\\n",
    "                (alpha_star_unit(xB_obs) - alpha_star_unit(xA_obs))\n",
    "\n",
    "logM_star_solved = np.log10(M_star_solved)\n",
    "model = LensModel(\n",
    "    logM_star=logM_star_solved,\n",
    "    logM_halo=logM_halo,\n",
    "    logRe=logRe_obs,\n",
    "    zl=zl, zs=zs,\n",
    "    gamma_in=gamma_in,  # ✅ 加入 gamma_in\n",
    "    c_halo=c_halo\n",
    ")\n",
    "caustic_max = model.solve_ycaustic()\n",
    "beta_unit = beta_solved / caustic_max\n",
    "\n",
    "\n",
    "logM_star_solved, beta_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48901966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94142f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728432ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior_mu_gamma.py\n",
    "import os, sys, numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # 如需交互可换成 Qt5Agg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 让包可被导入：根据你的 test_pipeline 的做法加入两级上层目录 ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# --- 导入你现有代码中的接口 ---\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_posterior\n",
    "from sl_inference_4Dinfer_hdf5.main import DM_GRID_2D\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mass_sampler import MODEL_PARAMS\n",
    "\n",
    "\n",
    "n_lens=10000             # 生成的 mock lens 数（可适当调小以加速）\n",
    "seed=42\n",
    "nbkg=4e-4\n",
    "process=10\n",
    "if_source=True\n",
    "use_first_k=12        # 参与似然的前K个 lens\n",
    "mu_gamma_grid=(0.5, 1.5, 201)  # 扫描网格 (lo, hi, n)\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "print(\"=== Step 1: 生成 mock 数据 ===\")\n",
    "df_lens, mock_lens_data, mock_observed_data, samples = run_mock_simulation(\n",
    "    n_lens, nbkg=nbkg, process=process, if_source=if_source\n",
    ")\n",
    "print(f\"生成 lens 数 = {len(mock_observed_data)}\")\n",
    "\n",
    "# 取前 use_first_k 个 lens 做后验（加速）\n",
    "mock_observed_data = mock_observed_data.iloc[:use_first_k].copy()\n",
    "print(f\"用于似然的 lens 数 = {len(mock_observed_data)}\")\n",
    "\n",
    "print(\"=== Step 2: 在 2D (logMh, gamma_h) 网格上做核表 ===\")\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, dm_grid=DM_GRID_2D, n_jobs=1, show_progress=True)\n",
    "print(f\"得到 grids 数 = {len(grids)}\")\n",
    "\n",
    "# --- 真值（与 mock 生成保持一致） ---\n",
    "model_p = MODEL_PARAMS[\"deVauc\"]\n",
    "# alpha_sps 的真值 = mock 里用到的 logalpha；mock 返回的 mock_lens_data 里应有一列 'logalpha_sps' 的标量真值\n",
    "# 但 run_mock_simulation 里你是以单一 logalpha 全体共享的，所以我们从 mock_lens_data（或 df_lens）里取第一个即可\n",
    "if \"logalpha_sps\" in mock_lens_data.columns:\n",
    "    alpha_true = float(mock_lens_data[\"logalpha_sps\"].iloc[0])\n",
    "else:\n",
    "    # 兜底：不少脚本把 true alpha 叫 logalpha，且 main.py 里也用这个真值对照\n",
    "    # 如果都没有，就设为 0.15（请按你的 mock 设置改掉）\n",
    "    alpha_true = 0.15\n",
    "    print(\"[WARN] 未在 mock_lens_data 中找到 logalpha_sps 列，临时用 alpha_true=0.15\")\n",
    "\n",
    "mu_h_true     = float(model_p[\"mu_h0\"])\n",
    "beta_h_true   = float(model_p[\"beta_h\"])\n",
    "sigma_h_true  = float(model_p[\"sigma_h\"])\n",
    "mu_gamma_true = 1.0\n",
    "sigma_gamma_true = 0.2\n",
    "\n",
    "# --- 构造 mu_gamma 扫描网格 ---\n",
    "lo, hi, n = mu_gamma_grid\n",
    "mu_g_grid = np.linspace(lo, hi, n)\n",
    "\n",
    "# --- 其他参数固定为真值，仅对 mu_gamma 做一维后验 ---\n",
    "print(\"=== Step 3: 计算 mu_gamma 的一维后验（其它参数用真值）===\")\n",
    "logpost = np.empty_like(mu_g_grid)\n",
    "theta = np.array([alpha_true, mu_h_true, beta_h_true, sigma_h_true, mu_gamma_true, sigma_gamma_true], dtype=float)\n",
    "\n",
    "# 注意：likelihood.log_posterior 内部会做：\n",
    "#  - 每个 lens 的 2D 栅格上，合成 F(logMh, gamma_h)；\n",
    "#  - 乘上 P(logMh|η) 与 P(gamma_h|η)（这里给定 mu_gamma,sigma_gamma）；\n",
    "#  - 对 (logMh, gamma_h) 用梯形权重积分；\n",
    "#  - 累加所有 lens；A(η) 在当前版本被置为 1，不影响一维形状；\n",
    "for i, mu_g in enumerate(mu_g_grid):\n",
    "    theta[4] = mu_g\n",
    "    lp = log_posterior(theta, grids, eta=False)  # A(η) 先关掉，避免任何表的问题\n",
    "    logpost[i] = lp\n",
    "\n",
    "\n",
    "\n",
    "logpost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=220)\n",
    "plt.plot(mu_g_grid, logpost - np.max(logpost), \"-o\")\n",
    "plt.xlabel(r'$\\mu_{\\gamma}$')\n",
    "plt.ylabel(\"log Posterior (relative)\")\n",
    "plt.savefig(\"posterior_mu_gamma.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60605d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_4Dinfer_hdf5.likelihood import _single_lens_likelihood\n",
    "print([_single_lens_likelihood(g, theta) for g in grids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_4Dinfer_hdf5.likelihood import log_prior, log_likelihood\n",
    "\n",
    "print(\"log_prior =\", log_prior(theta))\n",
    "print(\"log_likelihood =\", log_likelihood(theta, grids, eta=False))\n",
    "print(\"log_posterior =\", log_posterior(theta, grids, eta=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 作图 ---\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(mu_g_grid, logpost, lw=2)\n",
    "ax.axvline(mu_gamma_true, ls=\"--\", color=\"r\", lw=1, label=r\"truth $\\mu_\\gamma$\")\n",
    "\n",
    "ax.set_xlabel(r\"$\\mu_\\gamma$\")\n",
    "ax.set_ylabel(\"Posterior (normalized)\")\n",
    "ax.set_title(r\"1D posterior of $\\mu_\\gamma$ (others fixed to truth)\")\n",
    "ax.legend()\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb8d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 作图 ---\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(mu_g_grid, logpost, lw=2)\n",
    "ax.axvline(mu_gamma_true, ls=\"--\", color=\"r\", lw=1, label=r\"truth $\\mu_\\gamma$\")\n",
    "\n",
    "ax.set_xlabel(r\"$\\mu_\\gamma$\")\n",
    "ax.set_ylabel(\"Posterior (normalized)\")\n",
    "ax.set_title(r\"1D posterior of $\\mu_\\gamma$ (others fixed to truth)\")\n",
    "ax.legend()\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c1d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9edae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior_mu_gamma.py\n",
    "import os, sys, numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # 如需交互可换成 Qt5Agg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 让包可被导入：根据你的 test_pipeline 的做法加入两级上层目录 ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# --- 导入你现有代码中的接口 ---\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_posterior\n",
    "from sl_inference_4Dinfer_hdf5.main import DM_GRID_2D\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mass_sampler import MODEL_PARAMS\n",
    "\n",
    "\n",
    "n_lens=1000             # 生成的 mock lens 数（可适当调小以加速）\n",
    "seed=42\n",
    "nbkg=4e-4\n",
    "process=10\n",
    "if_source=True\n",
    "use_first_k=12        # 参与似然的前K个 lens\n",
    "mu_gamma_grid=(0.5, 1.5, 201)  # 扫描网格 (lo, hi, n)\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "print(\"=== Step 1: 生成 mock 数据 ===\")\n",
    "df_lens, mock_lens_data, mock_observed_data, samples = run_mock_simulation(\n",
    "    n_lens, nbkg=nbkg, process=process, if_source=if_source\n",
    ")\n",
    "print(f\"生成 lens 数 = {len(mock_observed_data)}\")\n",
    "\n",
    "# 取前 use_first_k 个 lens 做后验（加速）\n",
    "mock_observed_data = mock_observed_data.iloc[:use_first_k].copy()\n",
    "print(f\"用于似然的 lens 数 = {len(mock_observed_data)}\")\n",
    "\n",
    "print(\"=== Step 2: 在 2D (logMh, gamma_h) 网格上做核表 ===\")\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, dm_grid=DM_GRID_2D, n_jobs=1, show_progress=True)\n",
    "print(f\"得到 grids 数 = {len(grids)}\")\n",
    "\n",
    "# --- 真值（与 mock 生成保持一致） ---\n",
    "model_p = MODEL_PARAMS[\"deVauc\"]\n",
    "# alpha_sps 的真值 = mock 里用到的 logalpha；mock 返回的 mock_lens_data 里应有一列 'logalpha_sps' 的标量真值\n",
    "# 但 run_mock_simulation 里你是以单一 logalpha 全体共享的，所以我们从 mock_lens_data（或 df_lens）里取第一个即可\n",
    "if \"logalpha_sps\" in mock_lens_data.columns:\n",
    "    alpha_true = float(mock_lens_data[\"logalpha_sps\"].iloc[0])\n",
    "else:\n",
    "    # 兜底：不少脚本把 true alpha 叫 logalpha，且 main.py 里也用这个真值对照\n",
    "    # 如果都没有，就设为 0.15（请按你的 mock 设置改掉）\n",
    "    alpha_true = 0.15\n",
    "    print(\"[WARN] 未在 mock_lens_data 中找到 logalpha_sps 列，临时用 alpha_true=0.15\")\n",
    "\n",
    "mu_h_true     = float(model_p[\"mu_h0\"])\n",
    "beta_h_true   = float(model_p[\"beta_h\"])\n",
    "sigma_h_true  = float(model_p[\"sigma_h\"])\n",
    "mu_gamma_true = 1.0\n",
    "sigma_gamma_true = 0.2\n",
    "\n",
    "# --- 构造 mu_gamma 扫描网格 ---\n",
    "lo, hi, n = mu_gamma_grid\n",
    "mu_g_grid = np.linspace(lo, hi, n)\n",
    "\n",
    "# --- 其他参数固定为真值，仅对 mu_gamma 做一维后验 ---\n",
    "print(\"=== Step 3: 计算 mu_gamma 的一维后验（其它参数用真值）===\")\n",
    "logpost = np.empty_like(mu_g_grid)\n",
    "theta = np.array([alpha_true, mu_h_true, beta_h_true, sigma_h_true, mu_gamma_true, sigma_gamma_true], dtype=float)\n",
    "\n",
    "# 注意：likelihood.log_posterior 内部会做：\n",
    "#  - 每个 lens 的 2D 栅格上，合成 F(logMh, gamma_h)；\n",
    "#  - 乘上 P(logMh|η) 与 P(gamma_h|η)（这里给定 mu_gamma,sigma_gamma）；\n",
    "#  - 对 (logMh, gamma_h) 用梯形权重积分；\n",
    "#  - 累加所有 lens；A(η) 在当前版本被置为 1，不影响一维形状；\n",
    "\n",
    "lp = log_posterior(theta, grids, eta=False)  # A(η) 先关掉，避免任何表的问题\n",
    "\n",
    "\n",
    "\n",
    "lp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a927fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21632d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90188079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de4412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5edcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7190130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889c1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58487b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a54be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97023570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d5975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b58d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b202c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33fa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4feffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from emcee.backends import HDFBackend\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "# ---------- 打开 emcee 后端（自动探测组名） ----------\n",
    "def open_backend_from_runfile(path: str) -> HDFBackend:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for g in (\"chains/mcmc\", \"chains\", \"mcmc\"):\n",
    "            if g in f:\n",
    "                return HDFBackend(path, name=g, read_only=True)\n",
    "    # 也可能是纯 emcee 后端文件，默认根组\n",
    "    return HDFBackend(path, read_only=True)\n",
    "\n",
    "\n",
    "# ---------- 读取元数据（优先 /metadata，兜底从文件名解析 lenses） ----------\n",
    "def read_run_metadata(path: str):\n",
    "    lens_num = None\n",
    "    scatter_mag = None\n",
    "    scatter_star = None\n",
    "    n_galaxy = None\n",
    "    eta = None\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if \"metadata\" in f:\n",
    "                md = f[\"metadata\"].attrs\n",
    "                for k in (\"lens_number\", \"n_lens\", \"lens_count\"):\n",
    "                    if k in md:\n",
    "                        lens_num = int(md[k])\n",
    "                        break\n",
    "                if \"scatter_mag\" in md:\n",
    "                    scatter_mag = float(md[\"scatter_mag\"])\n",
    "                if \"scatter_star\" in md:\n",
    "                    scatter_star = float(md[\"scatter_star\"])\n",
    "                if \"n_galaxy\" in md:\n",
    "                    print(\"yes\")\n",
    "                    n_galaxy = int(md[\"n_galaxy\"])\n",
    "                if \"eta\" in md:\n",
    "                    eta = bool(md[\"eta\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if lens_num is None:\n",
    "        m = re.search(r\"(\\d+)\\s*lens\", path, re.IGNORECASE)\n",
    "        if m:\n",
    "            lens_num = int(m.group(1))\n",
    "    return lens_num, scatter_mag, scatter_star, n_galaxy, eta\n",
    "\n",
    "\n",
    "# ---------- 主函数：左 pairplot + 右 trace，并在图上彩色标注元数据 ----------\n",
    "def plot_pair_and_trace_side_by_side(\n",
    "    hdf5_file: str,\n",
    "    discard: int = 1000,\n",
    "    thin: int = 10,\n",
    "    labels=None,\n",
    "    truths=None,\n",
    "    width_factor: float = 2.0,\n",
    "    dead_thr: float = 1e-6,   # 判定死链阈值（全维 std < dead_thr）\n",
    "    last_steps: int | None = None,  # 右图仅显示最后 N 步\n",
    "    figsize=(14, 6),\n",
    "    cmap: str = \"Blues\",      # 左下三角 KDE 的 colormap\n",
    "    savepath=None,\n",
    "    step_range=None\n",
    "):\n",
    "    # 读取链\n",
    "    backend = open_backend_from_runfile(hdf5_file)\n",
    "    # chain = backend.get_chain(discard=discard, thin=thin, flat=False)  # (steps, walkers, ndim)\n",
    "    chain_all = backend.get_chain(discard=0, thin=thin, flat=False)   # 全部保留\n",
    "    chain     = backend.get_chain(discard=discard, thin=thin, flat=False)  # 左图用\n",
    "\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "\n",
    "    # 丢弃“死链”\n",
    "    live_idx = [w for w in range(nwalkers) if not np.all(np.std(chain[:, w, :], axis=0) < dead_thr)]\n",
    "\n",
    "    if len(live_idx) == 0:\n",
    "        raise RuntimeError(\"所有 walker 都被判定为死链（调整 dead_thr 再试）\")\n",
    "    chain_live = chain[:, live_idx, :]\n",
    "    samples = chain_live.reshape(-1, ndim)\n",
    "\n",
    "    # 标签 & 真值\n",
    "    if labels is None:\n",
    "        labels = [fr'$\\theta_{i}$' for i in range(ndim)]\n",
    "    if truths is not None and len(truths) != ndim:\n",
    "        raise ValueError(f\"truths 长度 {len(truths)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 统计量\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "    means, stds = df.mean(), df.std()\n",
    "\n",
    "    # 读取元数据\n",
    "    lens_num, scatter_mag, scatter_star, n_galaxy, eta = read_run_metadata(hdf5_file)\n",
    "\n",
    "    # 画布（dpi=300）\n",
    "    fig = plt.figure(figsize=figsize, dpi=300)\n",
    "    outer = fig.add_gridspec(1, 2, width_ratios=[1., 1.0], wspace=0.25)\n",
    "\n",
    "\n",
    "    # 计算 MAP：直接最大化 emcee 的 log_prob（与 samples 展平顺序保持一致）\n",
    "    logp = backend.get_log_prob(discard=discard, thin=thin, flat=False)  # (steps, walkers)\n",
    "    logp_live = logp[:, live_idx]                                        # 只取活链\n",
    "    logp_flat = logp_live.reshape(-1, order=\"C\")                         # 与 chain_live.reshape 一致\n",
    "    map_flat_idx = int(np.argmax(logp_flat))\n",
    "    map_point = samples[map_flat_idx]                                    # shape: (ndim,)\n",
    "    print(f\"[MAP] {dict(zip(labels, map_point))}\")\n",
    "\n",
    "\n",
    "    # ---- 左：手工“PairGrid” ----\n",
    "    gs_left = outer[0, 0].subgridspec(ndim, ndim, wspace=0.05, hspace=0.05)\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            if j > i:   # 只画对角+下三角\n",
    "                continue\n",
    "            ax = fig.add_subplot(gs_left[i, j])\n",
    "            xlab = labels[j]\n",
    "\n",
    "            if i == j:\n",
    "                # 对角：直方图 + KDE\n",
    "                sns.histplot(df[xlab], bins=100, kde=True, edgecolor=\"none\",\n",
    "                            alpha=0.5, color=\"#518FDF\", ax=ax)\n",
    "\n",
    "                # 计算 MAP\n",
    "                ax.axvline(map_point[i], color=\"orange\", ls=\"-\", lw=1.4, label=\"MAP\")\n",
    "                if truths is not None:\n",
    "                    ax.axvline(truths[i], color=\"red\", ls=\"--\", lw=1, label=\"Truth\")\n",
    "                    offsets = map_point - np.asarray(truths)\n",
    "\n",
    "                # 计算 68% 和 95% CI\n",
    "                q16, q84 = np.percentile(df[xlab], [16, 84])     # 68% 区间\n",
    "                q2p5, q97p5 = np.percentile(df[xlab], [2.5, 97.5])  # 95% 区间\n",
    "\n",
    "                # 42% CI\n",
    "                # q35_lo, q35_up = np.percentile(df[xlab], [29, 71])\n",
    "                \n",
    "                # ax.axvline(q35_lo, color=\"green\", ls=\"-.\", lw=1, label=\"35% CI\")\n",
    "                # ax.axvline(q35_up, color=\"green\", ls=\"-.\", lw=1)\n",
    "                # ax.axvspan(q35_lo, q35_up, color=\"black\", alpha=0.4, label=\"42% CI\")\n",
    "\n",
    "                # 画竖线\n",
    "                # ax.axvline(q16, color=\"black\", ls=\":\", lw=1)\n",
    "                # ax.axvline(q84, color=\"black\", ls=\":\", lw=1)\n",
    "                # ax.axvline(q2p5, color=\"gray\", ls=\"--\", lw=1)\n",
    "                # ax.axvline(q97p5, color=\"gray\", ls=\"--\", lw=1)\n",
    "\n",
    "                # 可选：画区间阴影（更直观）\n",
    "                ax.axvspan(q16, q84, color=\"black\", alpha=0.1, label=\"68% CI\")\n",
    "                ax.axvspan(q2p5, q97p5, color=\"gray\", alpha=0.05, label=\"95% CI\")\n",
    "\n",
    "                if i == 0:\n",
    "                    ax.legend(fontsize=12)\n",
    "\n",
    "                ax.set_yticks([])\n",
    "\n",
    "\n",
    "            else:\n",
    "                ylab = labels[i]\n",
    "                # 下三角：散点\n",
    "                sns.scatterplot(x=df[xlab], y=df[ylab], s=2, alpha=0.35,\n",
    "                                color=\"#46B48E\", ax=ax)\n",
    "                # 叠加 KDE 填色（注意：用 cmap 而不是 colors=）\n",
    "                sns.kdeplot(\n",
    "                    x=df[xlab], y=df[ylab], fill=True, thresh=1,\n",
    "                    levels=[0.01, 0.05, 0.35, 1.0],\n",
    "                    cmap=cmap, alpha=0.5, ax=ax\n",
    "                )\n",
    "                if truths is not None:\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=6, markeredgewidth=1.2)\n",
    "                mx, sx = means[xlab], stds[xlab]\n",
    "                my, sy = means[ylab], stds[ylab]\n",
    "                ax.set_xlim(mx - width_factor*sx, mx + width_factor*sx)\n",
    "                ax.set_ylim(my - width_factor*sy, my + width_factor*sy)\n",
    "\n",
    "            # 边缘标签\n",
    "            if i < ndim-1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xlabel(xlab)\n",
    "            # 在画直方图和散点后统一处理\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylabel(\"\")        # 去掉 ylabel\n",
    "                ax.tick_params(axis=\"y\", left=False)  # 去掉 y 轴刻度线\n",
    "            else:\n",
    "                if i == j:\n",
    "                    ax.set_ylabel(\"\")    # 对角线直方图也不要 ylabel（避免重复的 Count）\n",
    "                if i != j:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # 在左区顶端用不同颜色标注元数据\n",
    "    ax_meta = fig.add_subplot(gs_left[0, 0], frame_on=False)\n",
    "    ax_meta.axis(\"off\")\n",
    "    y0, dy = 0.8, 0.15\n",
    "    x = 2\n",
    "    fsize = 15\n",
    "\n",
    "    # if lens_num is not None:\n",
    "    #     ax_meta.text(x, y0, f\"lenses={lens_num}\", color=\"green\",\n",
    "    #                  fontsize=fsize, transform=ax_meta.transAxes, ha=\"left\", va=\"bottom\")\n",
    "    #     y0 -= dy\n",
    "    # if scatter_mag is not None:\n",
    "    #     ax_meta.text(x, y0, f\"scatter_mag={scatter_mag:.3f}\", color=\"blue\",\n",
    "    #                  fontsize=fsize, transform=ax_meta.transAxes, ha=\"left\", va=\"bottom\")\n",
    "    #     y0 -= dy\n",
    "    # if scatter_star is not None:\n",
    "    #     ax_meta.text(x, y0, f\"scatter_star={scatter_star:.3f}\", color=\"purple\",\n",
    "    #                  fontsize=fsize, transform=ax_meta.transAxes, ha=\"left\", va=\"bottom\")\n",
    "\n",
    "    # ---- 右：Trace（按维度纵向堆叠） ----\n",
    "    gs_right = outer[0, 1].subgridspec(ndim, 1, hspace=0.15)\n",
    "    axes_trace = [fig.add_subplot(gs_right[k, 0]) for k in range(ndim)]\n",
    "\n",
    "\n",
    "    chain_live_all = chain_all[:, live_idx, :]   # 所有步数（无 discard）\n",
    "    if step_range is not None:\n",
    "        start, end = step_range\n",
    "        cshow = chain_live_all[start:end, :, :]\n",
    "    elif last_steps is not None and last_steps < chain_live_all.shape[0]:\n",
    "        cshow = chain_live_all[-last_steps:, :, :]\n",
    "    else:\n",
    "        cshow = chain_live_all\n",
    "\n",
    "\n",
    "\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=ndim)\n",
    "    for d, ax in enumerate(axes_trace):\n",
    "        for w in range(cshow.shape[1]):\n",
    "            ax.plot(cshow[:, w, d], color=palette[d], alpha=0.35, lw=0.5)\n",
    "        ax.set_ylabel(labels[d])\n",
    "        if truths is not None:\n",
    "            ax.axhline(truths[d], color=\"red\", ls=\"--\", lw=0.8, alpha=0.8)\n",
    "        if d != ndim-1:\n",
    "            ax.set_xticklabels([])\n",
    "    axes_trace[-1].set_xlabel(\"Step\")\n",
    "\n",
    "    # 顶部简单说明（采样参数等）\n",
    "    # fig.suptitle(\n",
    "    #     f\"discard={discard}, thin={thin}, live_walkers={len(live_idx)}/{nwalkers}, lenses={lens_num}, mag={scatter_mag}, star={scatter_star}, n_galaxy={n_galaxy}, eta={eta}, offsets={'f'{offsets:+.2f}' if truths is not None else 'N/A'}\",\n",
    "    #     y=0.995, fontsize=11\n",
    "\n",
    "    # )\n",
    "    fig.suptitle(\n",
    "        f\"discard={discard}, thin={thin}, live_walkers={len(live_idx)}/{nwalkers}, \"\n",
    "        f\"lenses={lens_num}, mag={scatter_mag}, star={scatter_star}, \"\n",
    "        f\"n_galaxy={n_galaxy}, eta={eta}, \"\n",
    "        f\"offsets={float(np.abs(offsets)):+.3f}\" if truths is not None else \"offsets=N/A\",\n",
    "        y=0.995, fontsize=11\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 时间戳\n",
    "    timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", hdf5_file)\n",
    "\n",
    "    # if savepath is not None:\n",
    "    #     # savepath = \"~/Desktop\"   # 不要加 / 也没关系\n",
    "    #     save_dir = Path(os.path.expanduser(savepath))  # 展开 ~\n",
    "    #     save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #     out_png = save_dir / \"pair_trace.png\"\n",
    "    #     plt.savefig(out_png, dpi=300)\n",
    "    #     fname = f\"{savepath}/pair_trace_{timestamp.group(0)}\" if timestamp is not None else f\"{savepath}/pair_trace\"\n",
    "    #     plt.savefig(f\"{fname}.png\", dpi=300)\n",
    "    #     # plt.savefig(f\"{fname}.pdf\")\n",
    "    #     print(f\"图已保存至 {fname}.png 和 {fname}.pdf\")\n",
    "\n",
    "    if savepath is not None:\n",
    "        save_dir = Path(os.path.expanduser(savepath))  # 展开 ~\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 时间戳\n",
    "        timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", hdf5_file)\n",
    "\n",
    "        if timestamp:\n",
    "            fname = save_dir / f\"pair_trace_{timestamp.group(0)}\"\n",
    "        else:\n",
    "            fname = save_dir / \"pair_trace\"\n",
    "\n",
    "        plt.savefig(fname.with_suffix(\".png\"), dpi=300)\n",
    "        # plt.savefig(fname.with_suffix(\".pdf\"))\n",
    "        print(f\"图已保存至 {fname.with_suffix('.png')} 和 {fname.with_suffix('.pdf')}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------- 使用示例 ----------------\n",
    "plot_pair_and_trace_side_by_side(\n",
    "    hdf5_file=\"../chains/chains_3lens_noeta_20250916T185538Z_8732.h5\",\n",
    "    discard=800,\n",
    "    thin=10,\n",
    "    labels=[r\"$\\mu_{DM0}$\",\"\"],\n",
    "    truths=[12.91, 0.15],\n",
    "    width_factor=5,\n",
    "    dead_thr=1e-6,\n",
    "    last_steps=None,     # 或 2000 仅看末尾 2000 步\n",
    "    figsize=(14, 6),\n",
    "    step_range=(0, 1000),\n",
    "    cmap=\"Blues\",\n",
    "    savepath=\"~/Desktop/outfig\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804424e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from emcee.backends import HDFBackend\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def get_intervals():\n",
    "    return {\n",
    "        0: (13.5 - 12.0) / (31 - 1),\n",
    "        1: (3.0 - 1.0) / (41 - 1),\n",
    "        2: (0.45 - 0.3) / (31 - 1),\n",
    "        3: (0.2 - 0.1) / (21 - 1),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def read_run_metadata(path: str):\n",
    "    lens_num = None\n",
    "    scatter_mag = None\n",
    "    scatter_star = None\n",
    "    n_galaxy = None\n",
    "    eta = None\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if \"metadata\" in f:\n",
    "                md = f[\"metadata\"].attrs\n",
    "                for k in (\"lens_number\", \"n_lens\", \"lens_count\"):\n",
    "                    if k in md:\n",
    "                        lens_num = int(md[k])\n",
    "                        break\n",
    "                if \"scatter_mag\" in md:\n",
    "                    scatter_mag = float(md[\"scatter_mag\"])\n",
    "                if \"scatter_star\" in md:\n",
    "                    scatter_star = float(md[\"scatter_star\"])\n",
    "                if \"n_galaxy\" in md:\n",
    "                    print(\"yes\")\n",
    "                    n_galaxy = int(md[\"n_galaxy\"])\n",
    "                if \"eta\" in md:\n",
    "                    eta = bool(md[\"eta\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if lens_num is None:\n",
    "        m = re.search(r\"(\\d+)\\s*lens\", path, re.IGNORECASE)\n",
    "        if m:\n",
    "            lens_num = int(m.group(1))\n",
    "    return lens_num, scatter_mag, scatter_star, n_galaxy, eta\n",
    "\n",
    "# ---------- 打开 emcee 后端（自动探测组名） ----------\n",
    "def open_backend_from_runfile(path: str) -> HDFBackend:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for g in (\"chains/mcmc\", \"chains\", \"mcmc\"):\n",
    "            if g in f:\n",
    "                return HDFBackend(path, name=g, read_only=True)\n",
    "    # 也可能是纯 emcee 后端文件，默认根组\n",
    "    return HDFBackend(path, read_only=True)\n",
    "\n",
    "def plot_pair_and_trace_side_by_side(\n",
    "    hdf5_file: str,\n",
    "    discard: int = 1000,\n",
    "    thin: int = 10,\n",
    "    labels=None,\n",
    "    truths=None,\n",
    "    width_factor: float = 2.0,\n",
    "    dead_thr: float = 1e-6,   # 判定死链阈值（全维 std < dead_thr）\n",
    "    last_steps: int | None = None,  # 右图仅显示最后 N 步\n",
    "    figsize=(14, 6),\n",
    "    cmap: str = \"Blues\",      # 左下三角 KDE 的 colormap\n",
    "    savepath=None,\n",
    "    step_range=None,\n",
    "    bins=100\n",
    "):\n",
    "    # 读取链\n",
    "    backend = open_backend_from_runfile(hdf5_file)\n",
    "    chain_all = backend.get_chain(discard=0, thin=thin, flat=False)   # 全部保留\n",
    "    chain     = backend.get_chain(discard=discard, thin=thin, flat=False)  # 左图用\n",
    "\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "\n",
    "    # # 丢弃“死链”\n",
    "    # live_idx = [w for w in range(nwalkers) if not np.all(np.std(chain[:, w, :], axis=0) < dead_thr)]\n",
    "    # if len(live_idx) == 0:\n",
    "    #     raise RuntimeError(\"所有 walker 都被判定为死链（调整 dead_thr 再试）\")\n",
    "\n",
    "    # chain_live = chain[:, live_idx, :]\n",
    "    chain_live = chain\n",
    "    samples = chain_live.reshape(-1, ndim)\n",
    "\n",
    "    # 标签 & 真值\n",
    "    if labels is None:\n",
    "        labels = [fr'$\\theta_{i}$' for i in range(ndim)]\n",
    "    if truths is not None and len(truths) != ndim:\n",
    "        raise ValueError(f\"truths 长度 {len(truths)} 与参数维度 {ndim} 不一致\")\n",
    "\n",
    "    # 统计量\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "    means, stds = df.mean(), df.std()\n",
    "\n",
    "    # 读取元数据\n",
    "    lens_num, scatter_mag, scatter_star, n_galaxy, eta = read_run_metadata(hdf5_file)\n",
    "\n",
    "    # 画布\n",
    "    fig = plt.figure(figsize=figsize, dpi=300)\n",
    "    outer = fig.add_gridspec(1, 2, width_ratios=[1., 1.0], wspace=0.25)\n",
    "\n",
    "    # 计算 MAP\n",
    "    # logp = backend.get_log_prob(discard=discard, thin=thin, flat=False)  # (steps, walkers)\n",
    "    # logp_live = logp[:, live_idx]\n",
    "    # logp_flat = logp_live.reshape(-1, order=\"C\")\n",
    "    # map_flat_idx = int(np.argmax(logp_flat))\n",
    "    # map_point = samples[map_flat_idx]\n",
    "    # print(f\"[MAP] {dict(zip(labels, map_point))}\")\n",
    "\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    map_point = []\n",
    "    for d in range(ndim):\n",
    "        kde = gaussian_kde(samples[:, d])\n",
    "        x_grid = np.linspace(samples[:, d].min(), samples[:, d].max(), 500)\n",
    "        map_val = x_grid[np.argmax(kde(x_grid))]\n",
    "        map_point.append(map_val)\n",
    "    map_point = np.array(map_point)\n",
    "\n",
    "    print(f\"[MAP (KDE mode)] {dict(zip(labels, map_point))}\")\n",
    "\n",
    "\n",
    "    # ---- 左：手工 PairGrid ----\n",
    "    gs_left = outer[0, 0].subgridspec(ndim, ndim, wspace=0.05, hspace=0.05)\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            if j > i:\n",
    "                continue\n",
    "            ax = fig.add_subplot(gs_left[i, j])\n",
    "            xlab = labels[j]\n",
    "\n",
    "            if i == j:\n",
    "                sns.histplot(df[xlab], bins=bins, kde=True, edgecolor=\"none\",\n",
    "                             alpha=0.5, color=\"#518FDF\", ax=ax)\n",
    "                ax.axvline(map_point[i], color=\"orange\", ls=\"-\", lw=1.4, label=\"MAP\")\n",
    "                if truths is not None:\n",
    "                    ax.axvline(truths[i], color=\"red\", ls=\"--\", lw=1, label=\"Truth\")\n",
    "                    offsets = map_point - np.asarray(truths)\n",
    "\n",
    "                    delta = intervals[i]\n",
    "                    ax.axvline(truths[i] - delta, color=\"red\", ls=\":\", lw=0.8, alpha=0.6)\n",
    "                    ax.axvline(truths[i] + delta, color=\"red\", ls=\":\", lw=0.8, alpha=0.6)\n",
    "\n",
    "                    offsets = map_point - np.asarray(truths)\n",
    "\n",
    "                q16, q84 = np.percentile(df[xlab], [16, 84])\n",
    "                q2p5, q97p5 = np.percentile(df[xlab], [2.5, 97.5])\n",
    "                ax.axvspan(q16, q84, color=\"black\", alpha=0.1, label=\"68% CI\")\n",
    "                ax.axvspan(q2p5, q97p5, color=\"gray\", alpha=0.05, label=\"95% CI\")\n",
    "\n",
    "                if i == 0:\n",
    "                    ax.legend(fontsize=4)\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            else:\n",
    "                ylab = labels[i]\n",
    "                sns.scatterplot(x=df[xlab], y=df[ylab], s=2, alpha=0.35,\n",
    "                                color=\"#46B48E\", ax=ax)\n",
    "                sns.kdeplot(\n",
    "                    x=df[xlab], y=df[ylab], fill=True, thresh=1,\n",
    "                    levels=[0.01, 0.05, 0.35, 1.0],\n",
    "                    cmap=cmap, alpha=0.5, ax=ax\n",
    "                )\n",
    "                if truths is not None:\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=6, markeredgewidth=1.2)\n",
    "\n",
    "                    # delta = intervals[i]\n",
    "                    # ax.axvline(truths[i] - delta, color=\"red\", ls=\":\", lw=0.8, alpha=0.6)\n",
    "                    # ax.axvline(truths[i] + delta, color=\"red\", ls=\":\", lw=0.8, alpha=0.6)\n",
    "\n",
    "                    # offsets = map_point - np.asarray(truths)\n",
    "\n",
    "\n",
    "                mx, sx = means[xlab], stds[xlab]\n",
    "                my, sy = means[ylab], stds[ylab]\n",
    "                ax.set_xlim(mx - width_factor*sx, mx + width_factor*sx)\n",
    "                ax.set_ylim(my - width_factor*sy, my + width_factor*sy)\n",
    "\n",
    "            if i < ndim-1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xlabel(xlab)\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.tick_params(axis=\"y\", left=False)\n",
    "            else:\n",
    "                if i == j:\n",
    "                    ax.set_ylabel(\"\")\n",
    "                if i != j:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # ---- 右：Trace ----\n",
    "    gs_right = outer[0, 1].subgridspec(ndim, 1, hspace=0.15)\n",
    "    axes_trace = [fig.add_subplot(gs_right[k, 0]) for k in range(ndim)]\n",
    "    chain_live_all = chain_all[:, live_idx, :]\n",
    "    if step_range is not None:\n",
    "        start, end = step_range\n",
    "        cshow = chain_live_all[start:end, :, :]\n",
    "    elif last_steps is not None and last_steps < chain_live_all.shape[0]:\n",
    "        cshow = chain_live_all[-last_steps:, :, :]\n",
    "    else:\n",
    "        cshow = chain_live_all\n",
    "\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=ndim)\n",
    "    for d, ax in enumerate(axes_trace):\n",
    "        for w in range(cshow.shape[1]):\n",
    "            ax.plot(cshow[:, w, d], color=palette[d], alpha=0.35, lw=0.5)\n",
    "        ax.set_ylabel(labels[d])\n",
    "        if truths is not None:\n",
    "                    # ---- 添加参数边界线 ----\n",
    "            if d == 0:  # muDM\n",
    "                ax.axhline(12.0, color=\"black\", ls=\":\", lw=0.8)\n",
    "                ax.axhline(13.5, color=\"black\", ls=\":\", lw=0.8)\n",
    "            elif d == 1:  # betaDM\n",
    "                ax.axhline(1.0, color=\"black\", ls=\":\", lw=0.8)\n",
    "                ax.axhline(3.0, color=\"black\", ls=\":\", lw=0.8)\n",
    "            elif d == 2:  # sigmaDM\n",
    "                ax.axhline(0.3, color=\"black\", ls=\":\", lw=0.8)\n",
    "                ax.axhline(0.45, color=\"black\", ls=\":\", lw=0.8)\n",
    "            elif d == 3:  # alpha\n",
    "                ax.axhline(0.1, color=\"black\", ls=\":\", lw=0.8)\n",
    "                ax.axhline(0.2, color=\"black\", ls=\":\", lw=0.8)\n",
    "\n",
    "                    # 真值 ± interval\n",
    "            t = truths[d]\n",
    "            ax.axhline(t, color=\"red\", ls=\"--\", lw=0.8, alpha=0.8)\n",
    "            delta = intervals[d]\n",
    "            ax.axhline(t - delta, color=\"red\", ls=\":\", lw=0.8, alpha=0.6)\n",
    "            ax.axhline(t + delta, color=\"red\", ls=\":\", lw=0.8, alpha=0.6)\n",
    "\n",
    "\n",
    "        if d != ndim-1:\n",
    "            ax.set_xticklabels([])\n",
    "    axes_trace[-1].set_xlabel(\"Step\")\n",
    "\n",
    "    # ---- 顶部标题（支持二维 offset） ----\n",
    "    if truths is not None:\n",
    "        offsets = map_point - np.asarray(truths)\n",
    "        if len(offsets) == 2:\n",
    "            offset_mu, offset_alpha = offsets\n",
    "            offset_str = f\"offset_mu={offset_mu:+.3f}, offset_alpha={offset_alpha:+.3f}\"\n",
    "        else:\n",
    "            offset_str = \", \".join([f\"{o:+.3f}\" for o in offsets])\n",
    "    else:\n",
    "        offset_str = \"N/A\"\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"discard={discard}, thin={thin}, live_walkers={len(live_idx)}/{nwalkers}, \"\n",
    "        f\"lenses={lens_num}, mag={scatter_mag}, star={scatter_star}, \"\n",
    "        f\"n_galaxy={n_galaxy}, eta={eta}, offsets={offset_str}\",\n",
    "        y=0.995, fontsize=11\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savepath is not None:\n",
    "        save_dir = Path(os.path.expanduser(savepath))\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = re.search(r\"(\\d{8}T\\d{6}Z)\", hdf5_file)\n",
    "        if timestamp:\n",
    "            fname = save_dir / f\"pair_trace_{timestamp.group(0)}\"\n",
    "        else:\n",
    "            fname = save_dir / \"pair_trace\"\n",
    "        plt.savefig(fname.with_suffix(\".png\"), dpi=300)\n",
    "        print(f\"图已保存至 {fname.with_suffix('.png')}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = get_intervals()\n",
    "\n",
    "plot_pair_and_trace_side_by_side(\n",
    "    hdf5_file=\"../chains/chains_eta_no_eta.h5\",\n",
    "    discard=1500,          # 前期 burn-in 丢掉多少步\n",
    "    thin=10,              # 每隔多少步取一次样本\n",
    "    # labels=[r\"$\\mu_{\\rm DM}$\", r\"$\\beta_{\\rm DM}$\", r\"$\\sigma_{\\rm DM}$\", r\"$\\alpha$\"],\n",
    "    # truths=[12.91, 2.04, 0.37, 0.15], # 真实值 (muDM, alpha)\n",
    "    width_factor=5,       # 横纵坐标显示范围\n",
    "    dead_thr=1e-6,        # 死链判断阈值\n",
    "    last_steps=None,      # =2000 只看最后2000步；None 显示全程\n",
    "    figsize=(14, 6),\n",
    "    step_range=(0, 1000), # 右边 trace 只显示前1000步\n",
    "    cmap=\"Blues\",\n",
    "    savepath=\"~/Desktop/outfig\",   # 保存图片目录，None=不保存\n",
    "    bins=60\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8a22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309e3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc542532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde218da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta: Sequence[float]) -> float:\n",
    "    \"\"\"Flat prior for 4D parameters (muDM, betaDM, sigmaDM, alpha).\"\"\"\n",
    "    if len(theta) != 4:\n",
    "        return -np.inf\n",
    "    muDM, betaDM, sigmaDM, alpha = map(float, theta)\n",
    "    if not (11 < muDM < 14):\n",
    "        return -np.inf\n",
    "    if not (1 < betaDM < 3):\n",
    "        return -np.inf\n",
    "    if not (0.2 < sigmaDM < 0.6):\n",
    "        return -np.inf\n",
    "    if not (0.0 < alpha < 0.3):\n",
    "        return -np.inf\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55d7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_4Dinfer_hdf5.mock_tabulate_manager import generate_and_save_mock_tabulate\n",
    "\n",
    "generate_and_save_mock_tabulate(\n",
    "    output_file=\"mock_tabulate_big_new.h5\",\n",
    "    n_lens=100000,\n",
    "    batch_size=1000,\n",
    "    seed=42,\n",
    "    logalpha=0.15,\n",
    "    nbkg=1e-3,\n",
    "    zl=1.0,\n",
    "    zs=4.0,\n",
    "    n_jobs=8   # 使用 8 核并行\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd12215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = \"../bank_uniform.h5\"\n",
    "nbkg = 4e-4   # mock 时设的背景源密度\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    betamax = f[\"/lens/betamax\"][:]\n",
    "    \n",
    "\n",
    "betamax = betamax[np.isfinite(betamax)]\n",
    "\n",
    "# 计算 λ_i\n",
    "lambda_i = np.pi * (betamax**2) * nbkg\n",
    "\n",
    "# -------- 限制到 95% 分位 --------\n",
    "b95 = np.quantile(betamax, 0.95)\n",
    "l95 = np.quantile(lambda_i, 0.95)\n",
    "\n",
    "# -------- betamax 分布 --------\n",
    "plt.hist(betamax, bins=100, histtype=\"step\", density=True, range=(0, b95))\n",
    "plt.xlabel(\"betamax\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.title(\"Distribution of betamax (0–95% quantile)\")\n",
    "plt.show()\n",
    "\n",
    "# -------- λ 分布 --------\n",
    "plt.hist(lambda_i, bins=100, histtype=\"step\", density=True, range=(0, l95))\n",
    "plt.xlabel(\"λ (expected number of sources per lens)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.title(\"Distribution of λ (0–95% quantile)\")\n",
    "plt.show()\n",
    "\n",
    "# -------- 总体期望 --------\n",
    "print(f\"平均期望源数 E[N]: {np.mean(lambda_i):.4f}\")\n",
    "print(f\"最大期望源数 max λ: {np.max(lambda_i):.4f}\")\n",
    "print(f\"最小期望源数 min λ: {np.min(lambda_i):.4e}\")\n",
    "print(f\"95% 分位 λ: {l95:.4f}\")\n",
    "\n",
    "# -------- 关系图 --------\n",
    "mask = (betamax <= b95) & (lambda_i <= l95)\n",
    "plt.scatter(betamax[mask], lambda_i[mask], s=5, alpha=0.3)\n",
    "plt.xlabel(\"betamax\")\n",
    "plt.ylabel(\"λ (expected N)\")\n",
    "plt.title(\"λ vs betamax (0–95% quantile)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"../bank_uniform.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    betamax = f[\"/lens/betamax\"][:]\n",
    "    logM_star = f[\"/base/logM_star\"][:]\n",
    "    logM_halo = f[\"/base/logMh\"][:]\n",
    "    logRe = f[\"/base/logRe\"][:]\n",
    "    zl = f[\"/base/zl\"][:]\n",
    "    zs = f[\"/base/zs\"][:]\n",
    "\n",
    "# === 过滤 NaN ===\n",
    "mask = np.isfinite(betamax)\n",
    "print(f\"原始 lens 数: {len(betamax)}，去掉 NaN 后: {mask.sum()}\")\n",
    "\n",
    "betamax = betamax[mask]\n",
    "logM_star = logM_star[mask]\n",
    "logM_halo = logM_halo[mask]\n",
    "logRe = logRe[mask]\n",
    "zl = zl[mask]\n",
    "zs = zs[mask]\n",
    "\n",
    "# 转 DataFrame 方便分析\n",
    "df = pd.DataFrame({\n",
    "    \"betamax\": betamax,\n",
    "    \"logM_star\": logM_star,\n",
    "    \"logM_halo\": logM_halo,\n",
    "    \"logRe\": logRe,\n",
    "    \"zl\": zl,\n",
    "    \"zs\": zs\n",
    "})\n",
    "\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0106ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_mock_from_make_mock(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        base = {k: f[\"base\"][k][:] for k in f[\"base\"].keys()}\n",
    "        lens = {k: f[\"lens\"][k][:] for k in f[\"lens\"].keys()}\n",
    "    df = pd.concat([pd.DataFrame(base), pd.DataFrame(lens)], axis=1)\n",
    "    return df\n",
    "\n",
    "# ===== 使用 =====\n",
    "filename = \"../bank_uniform.h5\"   # 或 contrmock_0_pop.hdf5\n",
    "df = load_mock_from_make_mock(filename)\n",
    "\n",
    "# 去掉 NaN（betamax 失败的情况）\n",
    "df = df[np.isfinite(df[\"betamax\"])]\n",
    "\n",
    "# 找到 logM_halo 最大的样本\n",
    "top_halo = df.sort_values(\"logMh\", ascending=False).head(10)\n",
    "\n",
    "print(\"最大 halo 的前10个 lens：\")\n",
    "print(top_halo[[\"logMh\", \"logM_star\", \"logRe\", \"betamax\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef622e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"logMh\"], bins=100, histtype=\"step\", density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15da807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25efdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553a2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = \"../bank_uniform.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    print(\"顶层 keys:\", list(f.keys()))\n",
    "    if \"lens\" in f:\n",
    "        print(\"lens keys:\", list(f[\"lens\"].keys()))\n",
    "    if \"base\" in f:\n",
    "        print(\"base keys:\", list(f[\"base\"].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04eac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"mock_tabulate_big.h5\", \"r\") as f:\n",
    "    print(\"顶层 keys:\", list(f.keys()))\n",
    "    print(\"mock_observed 子组数量:\", len(f[\"mock_observed\"]))\n",
    "    print(\"grids 子组数量:\", len(f[\"grids\"]))\n",
    "    print(\"metadata 内容:\")\n",
    "    for k, v in f[\"metadata\"].items():\n",
    "        print(f\"  {k}: {v[()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def load_mock_distribution(filename, max_batches=None):\n",
    "    all_data = []\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        obs_grp = f[\"mock_observed\"]\n",
    "        batch_keys = list(obs_grp.keys())\n",
    "        if max_batches:\n",
    "            batch_keys = batch_keys[:max_batches]\n",
    "\n",
    "        for bk in batch_keys:\n",
    "            arr = obs_grp[bk][\"table\"][:]  # structured array\n",
    "            df_batch = pd.DataFrame.from_records(arr)\n",
    "            all_data.append(df_batch)\n",
    "\n",
    "    df = pd.concat(all_data, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# ===== 测试 =====\n",
    "filename = \"mock_tabulate_big.h5\"\n",
    "df = load_mock_distribution(filename, max_batches=10)  # 先读前10批看看\n",
    "print(\"总 mock 数:\", len(df))\n",
    "print(\"字段:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfa75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be1d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcf221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a888c5",
   "metadata": {},
   "source": [
    "# tabulate grid of mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_4Dinfer_hdf5.mock_tabulate_manager import generate_and_save_mock_tabulate\n",
    "\n",
    "generate_and_save_mock_tabulate(\n",
    "    output_file=\"mock_tabulate_big_new1.h5\",\n",
    "    n_lens=100000,\n",
    "    batch_size=1000,\n",
    "    seed=42,\n",
    "    logalpha=0.15,\n",
    "    nbkg=1e-3,\n",
    "    zl=1.0,\n",
    "    zs=4.0,\n",
    "    n_jobs=8   # 使用 8 核并行\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630c417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f00e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sl_inference_4Dinfer_hdf5.mock_tabulate_manager import load_mock_and_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "filename = \"mock_tabulate_big_new.h5\"\n",
    "\n",
    "# 选前 200 个 lens\n",
    "lens_indices = np.arange(0, 2000)\n",
    "\n",
    "# 读取对应 mock + grids\n",
    "mock_df, grids = load_mock_and_grids(filename, lens_indices)\n",
    "logM_sps_obs = mock_df[\"logM_star_sps_observed\"].values\n",
    "\n",
    "print(\"载入 lens 数:\", len(logM_sps_obs), \"对应 grids 数:\", len(grids))\n",
    "\n",
    "# 扫描 alpha\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 50)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood with subset of mock\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sl_inference_4Dinfer_hdf5.mock_tabulate_manager import load_mock_and_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "filename = \"mock_tabulate_big.h5\"\n",
    "# 任选一个 index（比如第 100 个事件）\n",
    "idx = [100]\n",
    "mock_df, grids = load_mock_and_grids(filename, idx, seed=0)\n",
    "logM = mock_df[\"logM_star_sps_observed\"].values\n",
    "\n",
    "alphas = np.linspace(0.10, 0.20, 21)\n",
    "def scan(eta_flag):\n",
    "    return [log_likelihood([12.91,2.04,0.37,a], grids, logM, eta=eta_flag) for a in alphas]\n",
    "\n",
    "logL_noeta = scan(False)\n",
    "logL_eta   = scan(True)\n",
    "\n",
    "print(\"argmax(no-eta) α =\", alphas[np.argmax(logL_noeta)])\n",
    "print(\"argmax(eta)   α =\", alphas[np.argmax(logL_eta)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38006cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "\n",
    "# 用刚才的 mock_df 这一行，现算一遍 grid\n",
    "logMh_grid = np.linspace(8.0, 18.0, 1000)\n",
    "grids_now = tabulate_likelihood_grids(mock_df, logMh_grid, n_jobs=None)\n",
    "\n",
    "# 两条α-曲线对比（eta=True）\n",
    "L_file = [log_likelihood([12.91,2.04,0.37,a], grids,     logM, eta=True) for a in alphas]\n",
    "L_now  = [log_likelihood([12.91,2.04,0.37,a], grids_now, logM, eta=True) for a in alphas]\n",
    "print(\"max|ΔlogL| =\", float(np.max(np.abs(np.array(L_file)-np.array(L_now)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机抽而不是取前 N 个\n",
    "import numpy as np\n",
    "total = 1906  # 你文件里实际事件数\n",
    "rng = np.random.default_rng(42)\n",
    "indices = rng.choice(total, size=500, replace=False)\n",
    "\n",
    "mock_df, grids = load_mock_and_grids(filename, indices, seed=42)\n",
    "logM = mock_df[\"logM_star_sps_observed\"].values\n",
    "\n",
    "L = [log_likelihood([12.91,2.04,0.37,a], grids, logM, eta=True) for a in alphas]\n",
    "print(\"argmax α =\", alphas[np.argmax(L)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78945ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_4Dinfer_hdf5.mock_tabulate_manager import compute_posterior_from_file\n",
    "\n",
    "theta = [12.91, 2.04, 0.37, 0.15]  # 真值\n",
    "logL_true = compute_posterior_from_file(\"mock_tabulate_big.h5\", theta, use_all=True)\n",
    "print(\"logL at true α=0.15:\", logL_true)\n",
    "\n",
    "theta = [12.91, 2.04, 0.37, 0.2]\n",
    "logL_shift = compute_posterior_from_file(\"mock_tabulate_big.h5\", theta, use_all=True)\n",
    "print(\"logL at α=0.2:\", logL_shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7768f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bb4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sl_inference_4Dinfer_hdf5.mock_tabulate_manager import compute_posterior_from_file\n",
    "\n",
    "# # 固定的三个参数\n",
    "# mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37  \n",
    "\n",
    "# # alpha 网格\n",
    "# alphas = np.linspace(0.0, 0.3, 5)\n",
    "\n",
    "# logLs = []\n",
    "# for a in alphas:\n",
    "#     theta = [mu_true, beta_true, sigma_true, a]\n",
    "#     logL = compute_posterior_from_file(\n",
    "#         filename=\"mock_tabulate_big_new.h5\",\n",
    "#         theta=theta,\n",
    "#         use_all=True,       # 用全部 mock 数据\n",
    "#         seed=42\n",
    "#     )\n",
    "#     print(f\"alpha={a:.3f}, logL={logL:.3f}\")\n",
    "#     logLs.append(logL)\n",
    "\n",
    "# logLs = np.array(logLs)\n",
    "# logLs -= np.max(logLs)   # 归一化，使最大值为 0\n",
    "\n",
    "# # 画图\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(alphas, logLs, \"-o\")\n",
    "# plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "# plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "# plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "10000/50*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d96f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "seed = 47\n",
    "lens = 10000\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    lens,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=seed,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "print(\"Mock data generated. Sample size:\", len(logM_sps_obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_lens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb5e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460205f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51291d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出df lens 的logM_star, logM_halo\n",
    "df_lens_sub = df_lens[[\"logM_star\", \"logM_halo\"]].copy()\n",
    "df_lens_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33434d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出df lens 的logM_star, logM_halo\n",
    "df_lens_sub = df_lens[[\"logM_star\", \"logM_halo\"]].copy()\n",
    "df_lens_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2f870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "seed = 47\n",
    "lens = 100000\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict, N_i_array = run_mock_simulation(\n",
    "    lens,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=seed,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True,\n",
    "    return_N_i=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e75d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(N_i_array[N_i_array>1], bins=30, histtype=\"step\", density=True)\n",
    "plt.xlabel(\"Number of observed sources per lens (N_i)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.title(\"Distribution of observed sources per lens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_lens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "seed = 47\n",
    "lens = 10000\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    lens,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=seed,\n",
    "    nbkg=1e-3,\n",
    "    if_source=True,\n",
    "    process=8\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "print(\"Mock data generated. Sample size:\", len(logM_sps_obs))\n",
    "\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 1000)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(f\"Profile likelihood for α (μ, β, σ fixed), seed={seed}, lens={lens}\")\n",
    "# plt.savefig(f\"./fig/likelihood_alpha_profile_seed{seed}.png\", dpi=300)\n",
    "\n",
    "if not os.path.exists(\"./fig\"):\n",
    "    os.makedirs(\"./fig\")\n",
    "\n",
    "if not os.path.exists(f\"./fig/likelihood_alpha_profile_seed{seed}_lens{lens}.png\"):\n",
    "    plt.savefig(f\"./fig/likelihood_alpha_profile_seed{seed}_lens{lens}.png\", dpi=300)\n",
    "else:\n",
    "    for i in range(1, 100):\n",
    "        if not os.path.exists(f\"./fig/likelihood_alpha_profile_seed{seed}_lens{lens}_{i}.png\"):\n",
    "            plt.savefig(f\"./fig/likelihood_alpha_profile_seed{seed}_lens{lens}_{i}.png\", dpi=300)\n",
    "            break\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fbf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预先计算的mock和tabulate数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc80dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0516dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afed01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 1000)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(f\"Profile likelihood for α (μ, β, σ fixed), seed={seed}\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207b391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7df27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 1000)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64aa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214249c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    5000,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=42,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 1000)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    5000,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=42,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 1000)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c4239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0894cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501bc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6106e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625e841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582b14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae88b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    50000,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=42,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 300)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aafb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56af92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    50000,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=42,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 300)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44528f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    120000,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=420,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 300)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ddc3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sl_inference_4Dinfer_hdf5.mock_generator.mock_generator import run_mock_simulation\n",
    "from sl_inference_4Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_4Dinfer_hdf5.likelihood import log_likelihood\n",
    "\n",
    "# Step 1. 生成 mock 数据\n",
    "df_lens, mock_lens_data, mock_observed_data, samples_dict = run_mock_simulation(\n",
    "    100000,      # 样本数，可以小一些，方便调试\n",
    "    logalpha=0.15,      # 真值\n",
    "    seed=42,\n",
    "    nbkg=4e-4,\n",
    "    if_source=True\n",
    ")\n",
    "logM_sps_obs = mock_observed_data[\"logM_star_sps_observed\"].values\n",
    "\n",
    "# Step 2. 构建 halo mass grid 并 tabulate\n",
    "logMh_grid = np.linspace(8.0, 18.0, 300)\n",
    "grids = tabulate_likelihood_grids(mock_observed_data, logMh_grid, n_jobs=None)\n",
    "\n",
    "# Step 3. 扫描 alpha 的似然\n",
    "mu_true, beta_true, sigma_true = 12.91, 2.04, 0.37\n",
    "alphas = np.linspace(0.0, 0.3, 300)\n",
    "\n",
    "logLs = []\n",
    "for a in alphas:\n",
    "    theta = [mu_true, beta_true, sigma_true, a]\n",
    "    logL = log_likelihood(theta, grids, logM_sps_obs, eta=True)\n",
    "    logLs.append(logL)\n",
    "\n",
    "logLs = np.array(logLs) - np.max(logLs)\n",
    "\n",
    "# Step 4. 画图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(alphas, logLs, \"-o\")\n",
    "plt.xlabel(r\"$\\alpha_{\\rm SPS}$\")\n",
    "plt.ylabel(r\"$\\Delta \\log \\mathcal{L}$\")\n",
    "plt.title(\"Profile likelihood for α (μ, β, σ fixed)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0b020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098db655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf10056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a16ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens多的时候还是会偏，是Aeta精度不够吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40225ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "path = \"../aeta_tables/Aeta_4D.h5\"   # 改成你实际的文件路径\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    mu = f[\"mu_dm_grid\"][:]\n",
    "    beta = f[\"beta_dm_grid\"][:]\n",
    "    sigma = f[\"sigma_dm_grid\"][:]\n",
    "    alpha = f[\"alpha_grid\"][:]\n",
    "    A = f[\"A_eta\"][:]\n",
    "\n",
    "print(\"mu range:\", mu.min(), \"→\", mu.max(), \"N =\", len(mu))\n",
    "print(\"beta range:\", beta.min(), \"→\", beta.max(), \"N =\", len(beta))\n",
    "print(\"sigma range:\", sigma.min(), \"→\", sigma.max(), \"N =\", len(sigma))\n",
    "print(\"alpha range:\", alpha.min(), \"→\", alpha.max(), \"N =\", len(alpha))\n",
    "print(\"A_eta shape:\", A.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f362c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486e720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772a541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0173e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5a7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25bab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de332617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0aa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "\n",
    "def plot_chains(hdf5_file, discard=0, thin=1, labels=None, max_steps=None):\n",
    "    \"\"\"\n",
    "    画出 emcee 的采样链条（trace plot），每个 walker 一条线。\n",
    "\n",
    "    参数：\n",
    "    - hdf5_file: HDFBackend 文件路径\n",
    "    - discard: 丢弃 burn-in 步数\n",
    "    - thin: 稀疏采样间隔\n",
    "    - labels: 每个参数的名字（list[str]）\n",
    "    - max_steps: 最多显示多少步（用于调试）\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    chain = backend.get_chain(discard=discard, thin=thin)  # shape: (nsteps, nwalkers, ndim)\n",
    "\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "    print(ndim)\n",
    "\n",
    "    if max_steps:\n",
    "        chain = chain[:max_steps]\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim, 1, figsize=(10, 2.2 * ndim), sharex=True)\n",
    "\n",
    "    if ndim == 1:\n",
    "        ax = axes\n",
    "        for walker in range(nwalkers):\n",
    "            ax.plot(chain[:, walker, 0], alpha=0.4, lw=0.5)\n",
    "        ax.set_ylabel(labels[0])\n",
    "        return 0\n",
    "        \n",
    "    for i in range(ndim):\n",
    "        ax = axes[i]\n",
    "        for walker in range(nwalkers):\n",
    "            ax.plot(chain[:, walker, i], alpha=0.4, lw=0.5)\n",
    "        ax.set_ylabel(labels[i])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Step number\")\n",
    "    plt.suptitle(\"MCMC Chains (Trace Plot)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_chains(\n",
    "    hdf5_file=\"../chains/chains_3lens_noeta_20250916T185538Z_8732.h5\",\n",
    "    discard=0,\n",
    "    thin=1,\n",
    "    # labels=[r\"$\\mu_{DM0}$\", r\"$\\beta_{DM}$\", r\"$\\sigma_{DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    max_steps=None # 可选\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.aeta_mock_bank import open_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sl_inference_2Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_2Dinfer_hdf5.likelihood import log_posterior\n",
    "\n",
    "# ==== 1. 构造一个只有一条的 mock lens DataFrame ====\n",
    "mock_data = pd.DataFrame([{\n",
    "    \"xA\": 1.2,   # 图像 A 位置 (单位: arcsec/kpc, 看你代码定义)\n",
    "    \"xB\": -0.8,  # 图像 B 位置\n",
    "    \"logRe\": 0.5,   # 有效半径 log10(Re/kpc)\n",
    "    \"magnitude_observedA\": 24.5,\n",
    "    \"magnitude_observedB\": 25.0,\n",
    "}])\n",
    "\n",
    "# ==== 2. 定义 halo mass 网格 ====\n",
    "logMh_grid = np.linspace(11.0, 14.0, 50)  # 50 个格点\n",
    "\n",
    "# ==== 3. 生成 LensGrid ====\n",
    "grids = tabulate_likelihood_grids(\n",
    "    mock_data,\n",
    "    logMh_grid,\n",
    "    zl=0.3,\n",
    "    zs=2.0,\n",
    "    sigma_m=0.1,\n",
    "    n_jobs=1,          # 单线程，避免并行干扰\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# ==== 4. 打印格点质量控制 ====\n",
    "g = grids[0]\n",
    "print(\"LensGrid logM_star 有限值比例:\", np.mean(np.isfinite(g.logM_star)))\n",
    "print(\"LensGrid sample_factor 总和:\", np.nansum(g.sample_factor))\n",
    "\n",
    "# ==== 5. 调用似然函数 ====\n",
    "logM_sps_obs = np.array([11.0])  # 假设观测星系的 log10(Msps) = 11\n",
    "lp = log_posterior([12.5,2, 0.37, 0.15], grids, logM_sps_obs, alpha_obs=None, eta=True)\n",
    "print(\"log posterior =\", lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for theta in [\n",
    "    [12.5, 2.0, 0.3, 0.15],\n",
    "    [13.0, 2.1, 0.4, 0.2],\n",
    "    [12.8, 1.5, 0.35, 0.1],\n",
    "]:\n",
    "    print(theta, log_likelihood(theta, grids, logM_sps_obs, eta=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sl_inference_2Dinfer_hdf5 import likelihood\n",
    "from sl_inference_2Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "\n",
    "# === 1. 构造 mock 数据 ===\n",
    "mock_data = pd.DataFrame([{\n",
    "    \"xA\": 1.2,\n",
    "    \"xB\": -0.9,\n",
    "    \"logRe\": 0.5,\n",
    "    \"magnitude_observedA\": 24.5,\n",
    "    \"magnitude_observedB\": 25.0,\n",
    "    \"logM_sps_obs\": 11.2   # 换成真实 mock 数据里的值会更准确\n",
    "}])\n",
    "\n",
    "logMh_grid = np.linspace(11.0, 14.0, 50)\n",
    "\n",
    "grids = tabulate_likelihood_grids(\n",
    "    mock_data,\n",
    "    logMh_grid,\n",
    "    zl=0.3, zs=2.0,\n",
    "    sigma_m=0.1,\n",
    "    n_jobs=1,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "grid = grids[0]\n",
    "logM_obs = mock_data[\"logM_sps_obs\"].values[0]\n",
    "\n",
    "# === 2. 定义 debug 版本的 likelihood ===\n",
    "def debug_single_lens(grid, logM_obs, theta):\n",
    "    muDM, alpha = theta\n",
    "    sigmaDM = likelihood.SIGMA_DM\n",
    "    betaDM = likelihood.BETA_DM\n",
    "\n",
    "    # 权重分量\n",
    "    sample_factor = grid.sample_factor\n",
    "    detJ = grid.detJ\n",
    "    logM_star = grid.logM_star\n",
    "\n",
    "    mask = np.isfinite(sample_factor) & np.isfinite(detJ) & np.isfinite(logM_star)\n",
    "    if not np.any(mask):\n",
    "        print(\"❌ 全部无效点\")\n",
    "        return 0.0\n",
    "\n",
    "    # 各分量\n",
    "    sf = sample_factor[mask]\n",
    "    dj = detJ[mask]\n",
    "    ms = logM_star[mask]\n",
    "\n",
    "    # p(Msps_obs | Msps, alpha) 这一项\n",
    "    diff = ms + alpha - logM_obs\n",
    "    p_obs = np.exp(-0.5 * (diff/0.1)**2)  # 假设 sigma_m=0.1\n",
    "    p_obs /= np.sqrt(2*np.pi) * 0.1\n",
    "\n",
    "    # 先验 p(Msps | muDM)\n",
    "    p_prior = np.exp(-0.5 * ((ms - muDM)/0.3)**2)\n",
    "    p_prior /= np.sqrt(2*np.pi) * 0.3\n",
    "\n",
    "    Z = sf * dj * p_obs * p_prior\n",
    "\n",
    "    integral = np.trapz(Z, ms)\n",
    "\n",
    "    print(f\"--- DEBUG for muDM={muDM}, alpha={alpha} ---\")\n",
    "    print(\"sample_factor sum =\", np.nansum(sf))\n",
    "    print(\"detJ sum          =\", np.nansum(dj))\n",
    "    print(\"p_obs max         =\", np.max(p_obs))\n",
    "    print(\"p_prior max       =\", np.max(p_prior))\n",
    "    print(\"Integral          =\", integral)\n",
    "\n",
    "    return integral\n",
    "\n",
    "# === 3. 测试几组点 ===\n",
    "for mu in [12.5, 12.9, 13.2]:\n",
    "    val = debug_single_lens(grid, logM_obs, [mu, 0.15])\n",
    "    A_eta = likelihood.safe_eval_A(mu, 0.15)\n",
    "    if val > 0 and A_eta > 0:\n",
    "        logL = np.log(val) - np.log(A_eta)\n",
    "    else:\n",
    "        logL = -np.inf\n",
    "    print(f\"Final logL(mu={mu}, alpha=0.15) = {logL}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sl_inference_2Dinfer_hdf5.main import run_mock_simulation\n",
    "from sl_inference_2Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "from sl_inference_2Dinfer_hdf5 import likelihood\n",
    "from sl_inference_2Dinfer_hdf5.run_mcmc import run_mcmc\n",
    "\n",
    "def test_pipeline(n_lenses=1, nsteps=200, seed=1234, twoD=True):\n",
    "    \"\"\"\n",
    "    最小化 pipeline 测试:\n",
    "    1. 生成 mock 数据\n",
    "    2. 构建 likelihood grids\n",
    "    3. 跑短 MCMC\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n=== Running pipeline test (twoD={twoD}) ===\")\n",
    "\n",
    "    # 1. 生成 mock 数据\n",
    "    mock_lens_data, mock_observed_data= run_mock_simulation(\n",
    "        n_lenses,\n",
    "        logalpha=0.15,\n",
    "        seed=seed,\n",
    "    )\n",
    "    print(\"Generated mock_observed_data columns:\", mock_observed_data.columns)\n",
    "\n",
    "    # 2. 构建 grids\n",
    "    logMh_grid = np.linspace(11.0, 14.0, 50)\n",
    "    grids = tabulate_likelihood_grids(\n",
    "        mock_observed_data,\n",
    "        logMh_grid,\n",
    "        zl=0.3, zs=2.0,\n",
    "        sigma_m=0.1,\n",
    "        n_jobs=1,\n",
    "        show_progress=False,\n",
    "    )\n",
    "    logM_sps_obs = mock_lens_data[\"logM_star_sps\"].values\n",
    "    print(\"Constructed grids for\", len(grids), \"lenses\")\n",
    "\n",
    "    # 3. 跑短链\n",
    "    if twoD:\n",
    "        theta0 = [12.5, 0.15]   # (muDM, alpha)\n",
    "        alpha_obs = None\n",
    "    else:\n",
    "        theta0 = [12.5]         # 只拟合 muDM\n",
    "        alpha_obs = mock_lens_data[\"logalpha_sps\"].values\n",
    "\n",
    "    sampler = run_mcmc(\n",
    "        grids,\n",
    "        logM_sps_obs,\n",
    "        nsteps=nsteps,\n",
    "        initial_guess=theta0,\n",
    "        alpha_obs=alpha_obs,\n",
    "        # save_chain=False,\n",
    "    )\n",
    "\n",
    "    # 打印 logL 范围\n",
    "    log_probs = sampler.get_log_prob(flat=True)\n",
    "    print(\"logL min =\", np.nanmin(log_probs), \"max =\", np.nanmax(log_probs))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 测试 1D\n",
    "    test_pipeline(twoD=False)\n",
    "\n",
    "    # 测试 2D\n",
    "    test_pipeline(twoD=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ce6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_lens_data, mock_observed_data= run_mock_simulation(\n",
    "        1,\n",
    "        logalpha=0.15,\n",
    "        seed=1,\n",
    "    )\n",
    "mock_lens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125097f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d2de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b307e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_inference_2Dinfer_hdf5 import likelihood\n",
    "\n",
    "# 用上面加的 safe_eval_A\n",
    "print(\"A_INTERP ndim =\", len(likelihood.A_INTERP.grid))\n",
    "print(\"mu grid range:\", likelihood.A_INTERP.grid[0][0], \"to\", likelihood.A_INTERP.grid[0][-1])\n",
    "if len(likelihood.A_INTERP.grid) == 3:\n",
    "    print(\"sigma grid:\", likelihood.A_INTERP.grid[1])\n",
    "print(\"alpha grid range:\", likelihood.A_INTERP.grid[-1][0], \"to\", likelihood.A_INTERP.grid[-1][-1])\n",
    "\n",
    "# 测试几个点\n",
    "for mu in [11.5, 12.5, 13.5]:\n",
    "    for alpha in [0.05, 0.15, 0.25]:\n",
    "        val = likelihood.safe_eval_A(mu, alpha)\n",
    "        print(f\"A({mu:.2f}, {alpha:.2f}) = {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4009f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c12f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sl_inference_2Dinfer_hdf5 import likelihood\n",
    "from sl_inference_2Dinfer_hdf5.make_tabulate import tabulate_likelihood_grids\n",
    "\n",
    "# === 1. 构造一个 DataFrame 而不是 list ===\n",
    "mock_data = pd.DataFrame([{\n",
    "    \"xA\": 1.2,\n",
    "    \"xB\": -0.9,\n",
    "    \"logRe\": 0.5,\n",
    "    \"magnitude_observedA\": 24.5,\n",
    "    \"magnitude_observedB\": 25.0,\n",
    "}])\n",
    "\n",
    "logMh_grid = np.linspace(11.0, 14.0, 50)\n",
    "\n",
    "# === 2. 调用 tabulate_likelihood_grids ===\n",
    "grids = tabulate_likelihood_grids(\n",
    "    mock_data,\n",
    "    logMh_grid,\n",
    "    zl=0.3, zs=2.0,\n",
    "    sigma_m=0.1,\n",
    "    n_jobs=1,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# === 3. 假设观测到的 Msps ===\n",
    "logM_sps_obs = np.array([11.0])\n",
    "\n",
    "# === 4. 测试 log_likelihood ===\n",
    "theta = [12.5, 0.15]  # (muDM, alpha)\n",
    "\n",
    "logL_total = 0.0\n",
    "for i, (grid, logM_obs) in enumerate(zip(grids, logM_sps_obs)):\n",
    "    L_i = likelihood._single_lens_likelihood(grid, logM_obs, theta)\n",
    "    A_eta = likelihood.safe_eval_A(theta[0], theta[1])\n",
    "\n",
    "    print(f\"Lens {i}: integral L_i={L_i:.3e}, A_eta={A_eta:.3e}\")\n",
    "    if L_i <= 0 or not np.isfinite(L_i):\n",
    "        print(\"  ⚠️ 积分为 0 或 nan → logL = -inf\")\n",
    "    elif A_eta <= 0 or not np.isfinite(A_eta):\n",
    "        print(\"  ⚠️ A_eta 异常 → logL = -inf\")\n",
    "    else:\n",
    "        logL_total += np.log(L_i) - np.log(A_eta)\n",
    "\n",
    "print(\"Final log-likelihood =\", logL_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu in [11.5, 12.0, 12.5, 13.0]:\n",
    "    for a in [0.05, 0.15, 0.25]:\n",
    "        val = likelihood._single_lens_likelihood(grids[0], logM_sps_obs[0], [mu, a])\n",
    "        print(f\"mu={mu:.2f}, alpha={a:.2f} -> L={val:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sl_inference_2Dinfer_hdf5 import likelihood\n",
    "\n",
    "# === 1. 从 A_INTERP 提取网格范围 ===\n",
    "mu_grid = likelihood.A_INTERP.grid[0]\n",
    "alpha_grid = likelihood.A_INTERP.grid[-1]\n",
    "\n",
    "print(\"muDM range:\", mu_grid[0], \"to\", mu_grid[-1], \"len=\", len(mu_grid))\n",
    "print(\"alpha range:\", alpha_grid[0], \"to\", alpha_grid[-1], \"len=\", len(alpha_grid))\n",
    "\n",
    "# === 2. 构建 A(mu, alpha) 数组 ===\n",
    "MU, ALPHA = np.meshgrid(mu_grid[::20], alpha_grid[::10], indexing=\"ij\")  # 下采样避免太大\n",
    "A_vals = np.zeros_like(MU)\n",
    "\n",
    "for i in range(MU.shape[0]):\n",
    "    for j in range(MU.shape[1]):\n",
    "        A_vals[i,j] = likelihood.safe_eval_A(MU[i,j], ALPHA[i,j])\n",
    "\n",
    "print(\"A_eta min:\", np.min(A_vals), \"max:\", np.max(A_vals))\n",
    "\n",
    "# === 3. 画热力图 ===\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(np.log10(A_vals+1e-30), \n",
    "           extent=[alpha_grid[0], alpha_grid[-1], mu_grid[0], mu_grid[-1]],\n",
    "           origin=\"lower\", aspect=\"auto\", cmap=\"viridis\")\n",
    "plt.colorbar(label=\"log10 A_eta\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"muDM\")\n",
    "plt.title(\"A_eta(muDM, alpha) from 2D table\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07112412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec775a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ---- 打开 HDF5 文件 ----\n",
    "bank_path = \"bank.h5\"\n",
    "with h5py.File(bank_path, \"r\") as f:\n",
    "    logMh      = f[\"/base/logMh\"][:]\n",
    "    logM_star  = f[\"/base/logM_star\"][:]\n",
    "    logRe      = f[\"/base/logRe\"][:]\n",
    "    beta       = f[\"/base/beta\"][:]\n",
    "    ms         = f[\"/base/ms\"][:]\n",
    "    ok_mask    = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "    betamax    = f[\"/lens/betamax\"][:] if \"/lens/betamax\" in f[\"/lens\"] else f[\"/lens/ycaustic_kpc\"][:]\n",
    "    mu1        = f[\"/lens/mu1\"][:]\n",
    "    mu2        = f[\"/lens/mu2\"][:]\n",
    "    detJ       = f[\"/lens/detJ\"][:]\n",
    "\n",
    "# ---- 筛选成功样本 ----\n",
    "valid = ok_mask & np.isfinite(detJ)\n",
    "print(f\"有效样本: {valid.sum()}/{len(valid)} ({100*valid.mean():.1f}%)\")\n",
    "\n",
    "# ---- 可视化分布 ----\n",
    "df = pd.DataFrame({\n",
    "    \"logMh\": logMh[valid],\n",
    "    \"logM_star\": logM_star[valid],\n",
    "    \"logRe\": logRe[valid],\n",
    "    \"beta\": beta[valid],\n",
    "    \"betamax\": betamax[valid],\n",
    "    \"mu1\": mu1[valid],\n",
    "    \"mu2\": mu2[valid],\n",
    "    \"detJ\": detJ[valid],\n",
    "})\n",
    "\n",
    "# 1. 单变量直方图\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "for ax, col in zip(axes.ravel(), [\"logMh\", \"logM_star\", \"logRe\", \"beta\", \"mu1\", \"mu2\"]):\n",
    "    sns.histplot(df[col], kde=True, ax=ax, bins=50)\n",
    "    ax.set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. 多维联合分布\n",
    "sns.pairplot(df[[\"logMh\", \"logM_star\", \"logRe\"]].sample(5000), corner=True, diag_kind=\"kde\")\n",
    "plt.suptitle(\"Joint distribution of logMh, logM_star, logRe\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 3. β / β_max 的覆盖率\n",
    "plt.figure()\n",
    "plt.hist(df[\"beta\"] / df[\"betamax\"], bins=50, density=True)\n",
    "plt.xlabel(r\"$\\beta / \\beta_{\\max}$\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.title(\"Source plane radius distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da174d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b034921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bank_path = \"bank.h5\"\n",
    "\n",
    "with h5py.File(bank_path, \"r\") as f:\n",
    "    logMh = f[\"/base/logMh\"][:]\n",
    "    logRe = f[\"/base/logRe\"][:]\n",
    "    beta  = f[\"/base/beta\"][:]\n",
    "    ok_mask = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "    is_double = f[\"/select/is_double\"][:] if \"/select/is_double\" in f else np.zeros_like(logMh, dtype=bool)\n",
    "\n",
    "valid = ok_mask\n",
    "lensed = valid & (is_double == 1)\n",
    "\n",
    "# 随机抽样减少绘图压力（否则几百万点会卡死）\n",
    "Nmax = 50000\n",
    "if len(logMh) > Nmax:\n",
    "    idx = np.random.choice(len(logMh), size=Nmax, replace=False)\n",
    "    logMh, logRe, beta, lensed = logMh[idx], logRe[idx], beta[idx], lensed[idx]\n",
    "\n",
    "# 构建 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"logMh\": logMh,\n",
    "    \"logRe\": logRe,\n",
    "    \"beta\": beta,\n",
    "    \"Lensed\": np.where(lensed, \"Lensed\", \"Unlensed\")\n",
    "})\n",
    "\n",
    "# 使用 seaborn pairplot，可按 hue 着色\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.pairplot(\n",
    "    df,\n",
    "    vars=[\"logMh\", \"logRe\", \"beta\"],\n",
    "    hue=\"Lensed\",\n",
    "    palette={\"Lensed\": \"red\", \"Unlensed\": \"gray\"},\n",
    "    plot_kws={\"alpha\": 0.4, \"s\": 10},\n",
    "    diag_kws={\"fill\": True, \"common_norm\": False},\n",
    "    corner=True\n",
    ")\n",
    "\n",
    "g.fig.suptitle(\"Lens vs Non-lens Distribution\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f235a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bank_path = \"bank.h5\"\n",
    "with h5py.File(bank_path, \"r\") as f:\n",
    "    ok_mask = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "    is_double = f[\"/select/is_double\"][:] if \"/select/is_double\" in f else np.zeros_like(ok_mask)\n",
    "\n",
    "valid = ok_mask\n",
    "lensed = valid & (is_double == 1)\n",
    "unlensed = valid & (is_double == 0)\n",
    "\n",
    "counts = [lensed.sum(), unlensed.sum(), (~valid).sum()]\n",
    "labels = [\"Lensed (双像)\", \"Unlensed (单像或无解)\", \"Solver fail (NaN)\"]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(counts, labels=labels, autopct=\"%1.2f%%\", colors=[\"red\", \"gray\", \"black\"], startangle=90)\n",
    "plt.title(\"Lens vs Non-lens Proportion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fa95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(labels, counts, color=[\"red\", \"gray\", \"black\"])\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Lens vs Non-lens count\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353deaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4737c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a121b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "bank_path = \"bank.h5\"\n",
    "with h5py.File(bank_path, \"r\") as f:\n",
    "    ok = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "    logMh = f[\"/base/logMh\"][:][ok]\n",
    "    logMstar = f[\"/base/logM_star\"][:][ok]\n",
    "    logRe = f[\"/base/logRe\"][:][ok]\n",
    "    beta = f[\"/base/beta\"][:][ok]\n",
    "\n",
    "    # theta1 = f[\"/lens/xA\"][:][ok]\n",
    "    # theta2 = f[\"/lens/xB\"][:][ok]\n",
    "    mu1 = f[\"/lens/mu1\"][:][ok]\n",
    "    mu2 = f[\"/lens/mu2\"][:][ok]\n",
    "    betamax = f[\"/lens/betamax\"][:][ok]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 去掉mu1大于100的点\n",
    "mask = mu1 < 100\n",
    "plt.hexbin(logMh[mask], mu1[mask], gridsize=100, cmap=\"viridis\", bins=\"log\")\n",
    "\n",
    "# plt.hexbin(logMh, mu1, gridsize=100, cmap=\"viridis\", bins=\"log\")\n",
    "plt.xlabel(r\"$\\log M_h$\")\n",
    "plt.ylabel(r\"$\\mu_1$\")\n",
    "plt.colorbar(label=\"counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"logMh\": logMh,\n",
    "    \"logMstar\": logMstar,\n",
    "    \"logRe\": logRe,\n",
    "    \"beta\": beta,\n",
    "    # \"theta1\": theta1,\n",
    "    # \"theta2\": theta2,\n",
    "    \"mu1\": mu1,\n",
    "    \"mu2\": mu2,\n",
    "    \"betamax\": betamax,\n",
    "})\n",
    "\n",
    "# 筛选mu1小于100的点\n",
    "df = df[df[\"mu1\"] < 100]\n",
    "\n",
    "sns.pairplot(df.sample(5000), corner=True, plot_kws={\"s\": 5, \"alpha\": 0.3})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按 logMh 排序后差分\n",
    "mu1_low100 = mu1[mu1 < 100]\n",
    "mu2_low100 = mu2[mu2 < 100]\n",
    "logMh_low100 = logMh[mu1 < 100]\n",
    "\n",
    "idx = np.argsort(logMh_low100)\n",
    "dmu1 = np.diff(mu1_low100[idx])\n",
    "dmu2 = np.diff(mu2_low100[idx])\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.hist(dmu1, bins=100, alpha=0.5, label=r\"$\\Delta \\mu_1$\")\n",
    "ax2.hist(dmu2, bins=100, alpha=0.5, label=r\"$\\Delta \\mu_2$\")\n",
    "ax1.set_xlabel(r\"$\\Delta \\mu$\")\n",
    "ax1.set_ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb665ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953595b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import importlib\n",
    "\n",
    "# # 获取当前脚本所在目录的父目录和目录名\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))  # 当前脚本所在文件夹\n",
    "# package_name = os.path.basename(os.path.dirname(current_dir))  # ../ 的文件夹名\n",
    "\n",
    "# # 动态导入包中的模块\n",
    "# mod1 = importlib.import_module(f\"{package_name}.module1\")\n",
    "# mod2 = importlib.import_module(f\"{package_name}.module2\")\n",
    "\n",
    "# # 使用模块中的函数\n",
    "# mod1.some_function()\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 自动找到项目根目录\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"../\"))  # 上一级目录就是项目根\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# 用完整包路径导入\n",
    "# from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606ec1f",
   "metadata": {},
   "source": [
    "! pwd\n",
    "/Users/shuo/Github/sl_inference_1Dinfer_hdf5_fix_A_runonlinux/test_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28279c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "从 mock bank HDF5 构建可用的“插值器（代理表）”，并对比插值 vs 直接求解/银行真值。\n",
    "特点：\n",
    "- 直接从 bank.h5 流式聚合到规则网格（不会把全部数据一次性载入内存）\n",
    "- betamax 建 3D 网格：f(logMh, logM_star, logRe)\n",
    "- mu1/mu2 建 4D 网格：f(logMh, logM_star, logRe, beta_norm)；其中 beta_norm = beta / betamax（样本自身的 betamax）\n",
    "- 为稳定起见，对 mu 做 asinh 变换后再取网格均值；评估时再反变换\n",
    "- 对网格中缺失/稀疏单元用最近邻填充，保证 RegularGridInterpolator 可用\n",
    "- 提供 CLI：\n",
    "    1) build  : 由 bank 构建插值器表（axes + grids）存到 interp.h5\n",
    "    2) bench  : 从 bank 抽样，对比插值 vs 直接求解（或 vs bank 字段）并出误差指标\n",
    "    3) eval   : 读 CSV 批量评估插值器，输出预测 CSV\n",
    "\n",
    "用法例：\n",
    "  构建：\n",
    "    python build_interpolator_from_bank.py build \\\n",
    "        --bank bank.h5 --out interp.h5 \\\n",
    "        --bins 64 64 64 64 \\\n",
    "        --qclip 0.001 0.999 \\\n",
    "        --min-count 20 \\\n",
    "        --sample-for-quantile 1000000\n",
    "\n",
    "  基于银行字段做快速对比（无需真·求解器）：\n",
    "    python build_interpolator_from_bank.py bench \\\n",
    "        --interp interp.h5 --bank bank.h5 --n 10000 --mode bank\n",
    "\n",
    "  基于直接求解器做严格对比（需要你的 lens_model/lens_solver）：\n",
    "    python build_interpolator_from_bank.py bench \\\n",
    "        --interp interp.h5 --bank bank.h5 --n 2000 --mode solver\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 可用则启用（用于最近邻填洞与KDE之类），没有也能跑（会退化到简单填充）\n",
    "try:\n",
    "    from scipy.interpolate import RegularGridInterpolator\n",
    "    from scipy.ndimage import distance_transform_edt\n",
    "except Exception:\n",
    "    RegularGridInterpolator = None\n",
    "    distance_transform_edt = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 工具：安全读取（存在才读），返回 None 或数组句柄\n",
    "# -----------------------------\n",
    "def _get_opt_dset(h5: h5py.File, path: str):\n",
    "    try:\n",
    "        return h5[path]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 工具：随机抽样索引（不载入数据）\n",
    "# -----------------------------\n",
    "def _random_indices(n: int, k: int, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    k = min(k, n)\n",
    "    return np.sort(rng.choice(n, size=k, replace=False))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 近似分位点（从银行流式抽样一部分做分位点）\n",
    "# -----------------------------\n",
    "def estimate_quantiles_from_bank(\n",
    "    bank_path: str,\n",
    "    sample_size: int = 1_000_000,\n",
    "    seed: int = 123,\n",
    ") -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    从 bank 里随机抽样，估计各特征的分位点范围（避免极端值影响网格边界）\n",
    "    返回：{name: (q001, q999)}\n",
    "    \"\"\"\n",
    "    with h5py.File(bank_path, \"r\") as f:\n",
    "        N = len(f[\"/base/logMh\"])\n",
    "        idx = _random_indices(N, sample_size, seed=seed)\n",
    "\n",
    "        logMh = f[\"/base/logMh\"][idx]\n",
    "        logMs = f[\"/base/logM_star\"][idx]\n",
    "        logRe = f[\"/base/logRe\"][idx]\n",
    "        beta  = f[\"/base/beta\"][idx]\n",
    "        betamax = (_get_opt_dset(f, \"/lens/betamax\") or _get_opt_dset(f, \"/lens/ycaustic_kpc\"))[idx]\n",
    "\n",
    "        # 只看有效 beta_norm\n",
    "        mask = np.isfinite(beta) & np.isfinite(betamax) & (betamax > 0)\n",
    "        beta_norm = np.clip(beta[mask] / betamax[mask], 0.0, 1.0)\n",
    "\n",
    "        q = {}\n",
    "        def q2(x):\n",
    "            return (np.quantile(x, 0.001), np.quantile(x, 0.999))\n",
    "\n",
    "        q[\"logMh\"] = q2(logMh[np.isfinite(logMh)])\n",
    "        q[\"logM_star\"] = q2(logMs[np.isfinite(logMs)])\n",
    "        q[\"logRe\"] = q2(logRe[np.isfinite(logRe)])\n",
    "        # beta_norm 固定在 [0,1]（也可收紧一点点避免边界效应）\n",
    "        lo = max(0.0, np.quantile(beta_norm, 0.0005) if beta_norm.size else 0.0)\n",
    "        hi = min(1.0, np.quantile(beta_norm, 0.9995) if beta_norm.size else 1.0)\n",
    "        q[\"beta_norm\"] = (lo, hi)\n",
    "    return q\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 将样本聚合到规则网格（流式）\n",
    "# -----------------------------\n",
    "def build_grids_from_bank(\n",
    "    bank_path: str,\n",
    "    bins: Tuple[int, int, int, int] = (64, 64, 64, 64),\n",
    "    qclip: Dict[str, Tuple[float, float]] = None,\n",
    "    min_count: int = 20,\n",
    "    chunk: int = 1_000_000,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    从 bank 流式聚合到规则网格：\n",
    "      - betamax_grid: (nMh, nMs, nRe)\n",
    "      - mu1_grid, mu2_grid: (nMh, nMs, nRe, nB)\n",
    "      - axes: logMh, logM_star, logRe, beta_norm\n",
    "\n",
    "    返回 dict，包括 grids 和 axes\n",
    "    \"\"\"\n",
    "    if RegularGridInterpolator is None:\n",
    "        raise RuntimeError(\"需要 SciPy：from scipy.interpolate import RegularGridInterpolator\")\n",
    "\n",
    "    (nMh, nMs, nRe, nB) = bins\n",
    "\n",
    "    # 估计分位点范围\n",
    "    if qclip is None:\n",
    "        qclip = estimate_quantiles_from_bank(bank_path)\n",
    "\n",
    "    lo_Mh, hi_Mh = qclip[\"logMh\"]\n",
    "    lo_Ms, hi_Ms = qclip[\"logM_star\"]\n",
    "    lo_Re, hi_Re = qclip[\"logRe\"]\n",
    "    lo_Bn, hi_Bn = qclip[\"beta_norm\"]\n",
    "\n",
    "    axes_logMh   = np.linspace(lo_Mh, hi_Mh, nMh, dtype=np.float32)\n",
    "    axes_logMstar= np.linspace(lo_Ms, hi_Ms, nMs, dtype=np.float32)\n",
    "    axes_logRe   = np.linspace(lo_Re, hi_Re, nRe, dtype=np.float32)\n",
    "    axes_beta_n  = np.linspace(lo_Bn, hi_Bn, nB,  dtype=np.float32)\n",
    "\n",
    "    # 网格累加器（计数 + 和）\n",
    "    # betamax 3D\n",
    "    sum_bmax = np.zeros((nMh, nMs, nRe), dtype=np.float64)\n",
    "    cnt_bmax = np.zeros((nMh, nMs, nRe), dtype=np.int64)\n",
    "\n",
    "    # mu 4D（先做 asinh 变换再均值）\n",
    "    sum_mu1 = np.zeros((nMh, nMs, nRe, nB), dtype=np.float64)\n",
    "    sum_mu2 = np.zeros((nMh, nMs, nRe, nB), dtype=np.float64)\n",
    "    cnt_mu  = np.zeros((nMh, nMs, nRe, nB), dtype=np.int64)\n",
    "\n",
    "    with h5py.File(bank_path, \"r\") as f:\n",
    "        N = len(f[\"/base/logMh\"])\n",
    "        d_logMh = f[\"/base/logMh\"]\n",
    "        d_logMs = f[\"/base/logM_star\"]\n",
    "        d_logRe = f[\"/base/logRe\"]\n",
    "        d_beta  = f[\"/base/beta\"]\n",
    "        d_mu1   = f[\"/lens/mu1\"]\n",
    "        d_mu2   = f[\"/lens/mu2\"]\n",
    "        d_ok    = f[\"/lens/ok_mask\"]\n",
    "        d_bmax  = _get_opt_dset(f, \"/lens/betamax\") or _get_opt_dset(f, \"/lens/ycaustic_kpc\")\n",
    "\n",
    "        for s in tqdm(range(0, N, chunk), desc=\"Aggregate to grids\"):\n",
    "            e = min(N, s + chunk)\n",
    "            ok = d_ok[s:e].astype(bool)\n",
    "            logMh = d_logMh[s:e]\n",
    "            logMs = d_logMs[s:e]\n",
    "            logRe = d_logRe[s:e]\n",
    "            beta  = d_beta[s:e]\n",
    "            mu1   = d_mu1[s:e]\n",
    "            mu2   = d_mu2[s:e]\n",
    "            bmax  = d_bmax[s:e] if d_bmax is not None else None\n",
    "\n",
    "            mask = ok & np.isfinite(logMh) & np.isfinite(logMs) & np.isfinite(logRe)\n",
    "            if bmax is not None:\n",
    "                mask &= np.isfinite(bmax) & (bmax > 0)\n",
    "            mask &= np.isfinite(beta) & np.isfinite(mu1) & np.isfinite(mu2)\n",
    "\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            logMh = logMh[mask]\n",
    "            logMs = logMs[mask]\n",
    "            logRe = logRe[mask]\n",
    "            beta  = beta[mask]\n",
    "            mu1   = mu1[mask]\n",
    "            mu2   = mu2[mask]\n",
    "            bmax  = bmax[mask]\n",
    "\n",
    "            # 归一化 beta（基于样本自身的 betamax）\n",
    "            beta_n = np.clip(beta / bmax, 0.0, 1.0)\n",
    "\n",
    "            # 落在网格范围内\n",
    "            in_range = (\n",
    "                (logMh >= lo_Mh) & (logMh <= hi_Mh) &\n",
    "                (logMs >= lo_Ms) & (logMs <= hi_Ms) &\n",
    "                (logRe >= lo_Re) & (logRe <= hi_Re) &\n",
    "                (beta_n >= lo_Bn) & (beta_n <= hi_Bn)\n",
    "            )\n",
    "            if not np.any(in_range):\n",
    "                continue\n",
    "\n",
    "            logMh  = logMh[in_range]\n",
    "            logMs  = logMs[in_range]\n",
    "            logRe  = logRe[in_range]\n",
    "            bmax   = bmax[in_range]\n",
    "            beta_n = beta_n[in_range]\n",
    "            mu1    = mu1[in_range]\n",
    "            mu2    = mu2[in_range]\n",
    "\n",
    "            # 计算网格下标（digitize 返回的是 bin 号，从 1..len(edges)-1，这里用轴中心近似）\n",
    "            idx_Mh = np.searchsorted(axes_logMh,   logMh, side=\"left\")\n",
    "            idx_Ms = np.searchsorted(axes_logMstar,logMs, side=\"left\")\n",
    "            idx_Re = np.searchsorted(axes_logRe,   logRe, side=\"left\")\n",
    "            idx_Bn = np.searchsorted(axes_beta_n,  beta_n, side=\"left\")\n",
    "\n",
    "            # clamp 到 [0, n-1]\n",
    "            idx_Mh = np.clip(idx_Mh, 0, nMh-1)\n",
    "            idx_Ms = np.clip(idx_Ms, 0, nMs-1)\n",
    "            idx_Re = np.clip(idx_Re, 0, nRe-1)\n",
    "            idx_Bn = np.clip(idx_Bn, 0, nB-1)\n",
    "\n",
    "            # 累加 betamax 3D\n",
    "            np.add.at(sum_bmax, (idx_Mh, idx_Ms, idx_Re), bmax)\n",
    "            np.add.at(cnt_bmax, (idx_Mh, idx_Ms, idx_Re), 1)\n",
    "\n",
    "            # 累加 mu 4D（asinh 平滑）\n",
    "            y1 = np.arcsinh(mu1)\n",
    "            y2 = np.arcsinh(mu2)\n",
    "            np.add.at(sum_mu1, (idx_Mh, idx_Ms, idx_Re, idx_Bn), y1)\n",
    "            np.add.at(sum_mu2, (idx_Mh, idx_Ms, idx_Re, idx_Bn), y2)\n",
    "            np.add.at(cnt_mu,  (idx_Mh, idx_Ms, idx_Re, idx_Bn), 1)\n",
    "\n",
    "    # 计算均值；低于 min_count 的单元标 NaN\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        grid_bmax = sum_bmax / np.maximum(cnt_bmax, 1)\n",
    "        grid_mu1  = sum_mu1  / np.maximum(cnt_mu,   1)\n",
    "        grid_mu2  = sum_mu2  / np.maximum(cnt_mu,   1)\n",
    "\n",
    "    grid_bmax[cnt_bmax < min_count] = np.nan\n",
    "    grid_mu1[cnt_mu   < min_count]  = np.nan\n",
    "    grid_mu2[cnt_mu   < min_count]  = np.nan\n",
    "\n",
    "    # 最近邻填充 NaN（可选，保证 RGI 可用）\n",
    "    def fill_nan_nn(arr: np.ndarray) -> np.ndarray:\n",
    "        if distance_transform_edt is None:\n",
    "            # 简单退化：用全局均值填（不理想，但可用）\n",
    "            m = np.nanmean(arr)\n",
    "            out = arr.copy()\n",
    "            out[np.isnan(out)] = m\n",
    "            return out\n",
    "        mask = ~np.isnan(arr)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(arr, dtype=np.float32)\n",
    "        # 距离变换拿到最近有效索引\n",
    "        dist, (inds,) = distance_transform_edt(~mask, return_indices=True)\n",
    "        # inds 的形状与 arr 相同，代表沿着展平方向最近有效的索引\n",
    "        filled = arr.copy()\n",
    "        filled[~mask] = arr[tuple(inds[~mask])]\n",
    "        return filled\n",
    "\n",
    "    grid_bmax_filled = fill_nan_nn(grid_bmax).astype(np.float32, copy=False)\n",
    "    grid_mu1_filled  = fill_nan_nn(grid_mu1).astype(np.float32, copy=False)\n",
    "    grid_mu2_filled  = fill_nan_nn(grid_mu2).astype(np.float32, copy=False)\n",
    "\n",
    "    return dict(\n",
    "        axes=dict(\n",
    "            logMh=axes_logMh,\n",
    "            logM_star=axes_logMstar,\n",
    "            logRe=axes_logRe,\n",
    "            beta_norm=axes_beta_n,\n",
    "        ),\n",
    "        grids=dict(\n",
    "            betamax=grid_bmax_filled,  # 3D\n",
    "            mu1=grid_mu1_filled,       # 4D（asinh-mean）\n",
    "            mu2=grid_mu2_filled,       # 4D（asinh-mean）\n",
    "        ),\n",
    "        meta=dict(\n",
    "            bins=bins,\n",
    "            qclip=qclip,\n",
    "            min_count=min_count,\n",
    "            note=\"mu grids store mean(asinh(mu)); de-transform with sinh() at evaluation\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 保存/加载 插值表\n",
    "# -----------------------------\n",
    "def save_interpolator_h5(out_path: str, bundle: Dict):\n",
    "    axes = bundle[\"axes\"]\n",
    "    grids = bundle[\"grids\"]\n",
    "    meta = bundle[\"meta\"]\n",
    "\n",
    "    with h5py.File(out_path, \"w\") as f:\n",
    "        g_axes = f.create_group(\"axes\")\n",
    "        for k, arr in axes.items():\n",
    "            g_axes.create_dataset(k, data=np.asarray(arr, dtype=np.float32), compression=\"gzip\", shuffle=True)\n",
    "\n",
    "        g_grid = f.create_group(\"grids\")\n",
    "        g_grid.create_dataset(\"betamax\", data=grids[\"betamax\"], compression=\"gzip\", shuffle=True)\n",
    "        g_grid.create_dataset(\"mu1\", data=grids[\"mu1\"], compression=\"gzip\", shuffle=True)\n",
    "        g_grid.create_dataset(\"mu2\", data=grids[\"mu2\"], compression=\"gzip\", shuffle=True)\n",
    "\n",
    "        f.attrs[\"meta_json\"] = json.dumps(meta, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def load_interpolator_h5(path: str):\n",
    "    f = h5py.File(path, \"r\")\n",
    "    ax = f[\"axes\"]\n",
    "    gd = f[\"grids\"]\n",
    "\n",
    "    axes = dict(\n",
    "        logMh=np.array(ax[\"logMh\"][:], dtype=np.float32),\n",
    "        logM_star=np.array(ax[\"logM_star\"][:], dtype=np.float32),\n",
    "        logRe=np.array(ax[\"logRe\"][:], dtype=np.float32),\n",
    "        beta_norm=np.array(ax[\"beta_norm\"][:], dtype=np.float32),\n",
    "    )\n",
    "    grids = dict(\n",
    "        betamax=np.array(gd[\"betamax\"][:], dtype=np.float32),  # 3D\n",
    "        mu1=np.array(gd[\"mu1\"][:], dtype=np.float32),          # 4D (asinh-mean)\n",
    "        mu2=np.array(gd[\"mu2\"][:], dtype=np.float32),\n",
    "    )\n",
    "    meta = json.loads(f.attrs.get(\"meta_json\", \"{}\"))\n",
    "\n",
    "    # 构建 RGI\n",
    "    if RegularGridInterpolator is None:\n",
    "        raise RuntimeError(\"需要 SciPy 的 RegularGridInterpolator\")\n",
    "\n",
    "    rgi_bmax = RegularGridInterpolator(\n",
    "        (axes[\"logMh\"], axes[\"logM_star\"], axes[\"logRe\"]),\n",
    "        grids[\"betamax\"], method=\"linear\", bounds_error=False, fill_value=None,\n",
    "    )\n",
    "    rgi_mu1 = RegularGridInterpolator(\n",
    "        (axes[\"logMh\"], axes[\"logM_star\"], axes[\"logRe\"], axes[\"beta_norm\"]),\n",
    "        grids[\"mu1\"], method=\"linear\", bounds_error=False, fill_value=None,\n",
    "    )\n",
    "    rgi_mu2 = RegularGridInterpolator(\n",
    "        (axes[\"logMh\"], axes[\"logM_star\"], axes[\"logRe\"], axes[\"beta_norm\"]),\n",
    "        grids[\"mu2\"], method=\"linear\", bounds_error=False, fill_value=None,\n",
    "    )\n",
    "\n",
    "    return dict(file=f, axes=axes, grids=grids, meta=meta,\n",
    "                rgi_betamax=rgi_bmax, rgi_mu1=rgi_mu1, rgi_mu2=rgi_mu2)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 批量评估插值器\n",
    "# -----------------------------\n",
    "def interp_predict(bundle, logMh, logMstar, logRe, beta) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"返回：betamax_pred, mu1_pred, mu2_pred\"\"\"\n",
    "    X3 = np.stack([logMh, logMstar, logRe], axis=-1)\n",
    "    bmax = bundle[\"rgi_betamax\"](X3).astype(np.float32)\n",
    "    # beta_norm（根据预测的 bmax 归一化）\n",
    "    beta_n = np.clip(beta / np.maximum(bmax, 1e-8), bundle[\"axes\"][\"beta_norm\"][0], bundle[\"axes\"][\"beta_norm\"][-1])\n",
    "\n",
    "    X4 = np.stack([logMh, logMstar, logRe, beta_n], axis=-1)\n",
    "    y1 = bundle[\"rgi_mu1\"](X4).astype(np.float32)\n",
    "    y2 = bundle[\"rgi_mu2\"](X4).astype(np.float32)\n",
    "    mu1 = np.sinh(y1)\n",
    "    mu2 = np.sinh(y2)\n",
    "    return bmax, mu1, mu2\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 基准对比（bank 或 solver）\n",
    "# -----------------------------\n",
    "def bench_compare(interp_path: str, bank_path: str, n: int, mode: str = \"bank\", seed: int = 2024):\n",
    "    \"\"\"\n",
    "    mode=\"bank\": 与 bank 中的 (betamax, mu1, mu2) 直接对比\n",
    "    mode=\"solver\": 调用你的求解器重新解（需要 lens_model/lens_solver 存在）\n",
    "    \"\"\"\n",
    "    bundle = load_interpolator_h5(interp_path)\n",
    "    with h5py.File(bank_path, \"r\") as f:\n",
    "        N = len(f[\"/base/logMh\"])\n",
    "        idx = _random_indices(N, n, seed=seed)\n",
    "        logMh = f[\"/base/logMh\"][idx].astype(np.float32)\n",
    "        logMs = f[\"/base/logM_star\"][idx].astype(np.float32)\n",
    "        logRe = f[\"/base/logRe\"][idx].astype(np.float32)\n",
    "        beta  = f[\"/base/beta\"][idx].astype(np.float32)\n",
    "        ok    = f[\"/lens/ok_mask\"][idx].astype(bool)\n",
    "\n",
    "        # 仅对 ok 样本做比较\n",
    "        sel = ok & np.isfinite(logMh) & np.isfinite(logMs) & np.isfinite(logRe) & np.isfinite(beta)\n",
    "        logMh, logMs, logRe, beta = logMh[sel], logMs[sel], logRe[sel], beta[sel]\n",
    "\n",
    "        if mode == \"bank\":\n",
    "            bmax_true = (_get_opt_dset(f, \"/lens/betamax\") or _get_opt_dset(f, \"/lens/ycaustic_kpc\"))[idx][sel].astype(np.float32)\n",
    "            mu1_true  = f[\"/lens/mu1\"][idx][sel].astype(np.float32)\n",
    "            mu2_true  = f[\"/lens/mu2\"][idx][sel].astype(np.float32)\n",
    "\n",
    "        elif mode == \"solver\":\n",
    "            # 需要可用的求解器\n",
    "            from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import LensModel\n",
    "            from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import solve_lens\n",
    "\n",
    "            bmax_true = np.empty_like(logMh)\n",
    "            mu1_true  = np.empty_like(logMh)\n",
    "            mu2_true  = np.empty_like(logMh)\n",
    "            for i in tqdm(range(len(logMh)), desc=\"True solve\"):\n",
    "                try:\n",
    "                    model = LensModel(logMs[i], logMh[i], logRe[i], 0.3, 2.0)\n",
    "                    xA, xB = solve_lens(model, float(beta[i]))\n",
    "                    muA = float(model.mu_from_rt(xA))\n",
    "                    muB = float(model.mu_from_rt(xB))\n",
    "                    bmx = float(model.solve_ycaustic())\n",
    "                except Exception:\n",
    "                    muA = muB = bmx = np.nan\n",
    "                mu1_true[i] = muA\n",
    "                mu2_true[i] = muB\n",
    "                bmax_true[i] = bmx\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'bank' or 'solver'\")\n",
    "\n",
    "    # 插值预测\n",
    "    bmax_pred, mu1_pred, mu2_pred = interp_predict(bundle, logMh, logMs, logRe, beta)\n",
    "\n",
    "    def summarize(name, pred, true):\n",
    "        mask = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 1e-12)\n",
    "        if not np.any(mask):\n",
    "            print(f\"[{name}] no valid points\")\n",
    "            return\n",
    "        rel = np.abs((pred[mask] - true[mask]) / true[mask])\n",
    "        print(f\"[{name}] N={mask.sum()} | mean={rel.mean():.3e}, median={np.median(rel):.3e}, 95%={np.quantile(rel,0.95):.3e}, max={rel.max():.3e}\")\n",
    "\n",
    "    summarize(\"betamax\", bmax_pred, bmax_true)\n",
    "    summarize(\"mu1\", mu1_pred, mu1_true)\n",
    "    summarize(\"mu2\", mu2_pred, mu2_true)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Build & evaluate interpolator from mock bank\")\n",
    "    sub = ap.add_subparsers(dest=\"cmd\", required=True)\n",
    "\n",
    "    # build\n",
    "    ap_b = sub.add_parser(\"build\", help=\"从 bank 构建插值器 HDF5\")\n",
    "    ap_b.add_argument(\"--bank\", required=True, help=\"bank.h5 路径\")\n",
    "    ap_b.add_argument(\"--out\",  required=True, help=\"输出 interp.h5 路径\")\n",
    "    ap_b.add_argument(\"--bins\", nargs=4, type=int, default=(64,64,64,64), help=\"nMh nMstar nRe nBeta\")\n",
    "    ap_b.add_argument(\"--qclip\", nargs=2, type=float, default=None, help=\"（可选）统一对数特征分位裁剪，如 0.001 0.999；不含 beta_norm\")\n",
    "    ap_b.add_argument(\"--min-count\", type=int, default=20, help=\"单元最小样本数（不足将用最近邻填充）\")\n",
    "    ap_b.add_argument(\"--sample-for-quantile\", type=int, default=1_000_000, help=\"估计分位点的随机抽样大小\")\n",
    "\n",
    "    # bench\n",
    "    ap_c = sub.add_parser(\"bench\", help=\"对比插值 vs bank/solver 真值\")\n",
    "    ap_c.add_argument(\"--interp\", required=True, help=\"interp.h5 路径\")\n",
    "    ap_c.add_argument(\"--bank\",   required=True, help=\"bank.h5 路径\")\n",
    "    ap_c.add_argument(\"--n\", type=int, default=5000, help=\"抽样数量\")\n",
    "    ap_c.add_argument(\"--mode\", choices=[\"bank\",\"solver\"], default=\"bank\")\n",
    "\n",
    "    # eval\n",
    "    ap_e = sub.add_parser(\"eval\", help=\"用插值器评估 CSV 输入\")\n",
    "    ap_e.add_argument(\"--interp\", required=True)\n",
    "    ap_e.add_argument(\"--csv\", required=True, help=\"包含列 logMh,logM_star,logRe,beta\")\n",
    "    ap_e.add_argument(\"--out\", required=True, help=\"输出 CSV，含 betamax,mu1,mu2 预测\")\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.cmd == \"build\":\n",
    "        if args.qclip is None:\n",
    "            q = estimate_quantiles_from_bank(args.bank, sample_size=args.sample_for_quantile)\n",
    "        else:\n",
    "            # 对数特征统一裁剪；beta_norm 固定 [0,1]\n",
    "            q_est = estimate_quantiles_from_bank(args.bank, sample_size=min(args.sample_for_quantile, 500_000))\n",
    "            q = dict(\n",
    "                logMh=(q_est[\"logMh\"][0], q_est[\"logMh\"][1]) if args.qclip is None else tuple(args.qclip),\n",
    "                logM_star=(q_est[\"logM_star\"][0], q_est[\"logM_star\"][1]) if args.qclip is None else tuple(args.qclip),\n",
    "                logRe=(q_est[\"logRe\"][0], q_est[\"logRe\"][1]) if args.qclip is None else tuple(args.qclip),\n",
    "                beta_norm=(0.0, 1.0)\n",
    "            )\n",
    "        bundle = build_grids_from_bank(\n",
    "            args.bank, bins=tuple(args.bins), qclip=q, min_count=args.min_count\n",
    "        )\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(args.out)) or \".\", exist_ok=True)\n",
    "        save_interpolator_h5(args.out, bundle)\n",
    "        print(f\"[OK] saved interpolator to {args.out}\")\n",
    "\n",
    "    elif args.cmd == \"bench\":\n",
    "        bench_compare(args.interp, args.bank, n=args.n, mode=args.mode)\n",
    "\n",
    "    elif args.cmd == \"eval\":\n",
    "        import pandas as pd\n",
    "        bundle = load_interpolator_h5(args.interp)\n",
    "        df = pd.read_csv(args.csv)\n",
    "        for col in [\"logMh\",\"logM_star\",\"logRe\",\"beta\"]:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"CSV 缺少列：{col}\")\n",
    "        bmax, mu1, mu2 = interp_predict(\n",
    "            bundle,\n",
    "            df[\"logMh\"].to_numpy(np.float32),\n",
    "            df[\"logM_star\"].to_numpy(np.float32),\n",
    "            df[\"logRe\"].to_numpy(np.float32),\n",
    "            df[\"beta\"].to_numpy(np.float32),\n",
    "        )\n",
    "        df[\"betamax_pred\"] = bmax\n",
    "        df[\"mu1_pred\"] = mu1\n",
    "        df[\"mu2_pred\"] = mu2\n",
    "        df.to_csv(args.out, index=False)\n",
    "        print(f\"[OK] wrote predictions to {args.out}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa190a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd605fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# # ========= 1. 读取 bank.h5 =========\n",
    "# bank_path = \"./bank.h5\"  # 改成你的文件路径\n",
    "# with h5py.File(bank_path, \"r\") as f:\n",
    "#     logMh   = f[\"/base/logMh\"][:]\n",
    "#     logMstar= f[\"/base/logM_star\"][:]\n",
    "#     logRe   = f[\"/base/logRe\"][:]\n",
    "#     beta    = f[\"/base/beta\"][:]\n",
    "#     betamax = f[\"/lens/betamax\"][:]\n",
    "#     mu1     = f[\"/lens/mu1\"][:]\n",
    "#     mu2     = f[\"/lens/mu2\"][:]\n",
    "#     ok_mask = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "\n",
    "# # 只保留成功求解的样本\n",
    "# mask = ok_mask & np.isfinite(betamax)\n",
    "# logMh, logMstar, logRe, beta = logMh[mask], logMstar[mask], logRe[mask], beta[mask]\n",
    "# betamax, mu1, mu2 = betamax[mask], mu1[mask], mu2[mask]\n",
    "\n",
    "# print(f\"[INFO] 有效样本 {mask.sum()} / {len(mask)}\")\n",
    "\n",
    "# # ========= 2. 构建插值器 =========\n",
    "# # 先构建网格\n",
    "# bins = 32  # 每个维度分32个格点，可调\n",
    "# edges_logMh   = np.linspace(logMh.min(), logMh.max(), bins)\n",
    "# edges_logMstar= np.linspace(logMstar.min(), logMstar.max(), bins)\n",
    "# edges_logRe   = np.linspace(logRe.min(), logRe.max(), bins)\n",
    "# edges_beta    = np.linspace(beta.min(), beta.max(), bins)\n",
    "\n",
    "# # 生成多维网格 & 做统计\n",
    "# grid_shape = (bins, bins, bins, bins)\n",
    "# betamax_grid = np.full(grid_shape, np.nan, dtype=np.float32)\n",
    "\n",
    "# # 逐网格取平均值\n",
    "# # 注意：大样本时可以改成 np.histogramdd 做快速聚合\n",
    "# for i, mh_bin in enumerate(zip(edges_logMh[:-1], edges_logMh[1:])):\n",
    "#     for j, ms_bin in enumerate(zip(edges_logMstar[:-1], edges_logMstar[1:])):\n",
    "#         for k, re_bin in enumerate(zip(edges_logRe[:-1], edges_logRe[1:])):\n",
    "#             for l, b_bin in enumerate(zip(edges_beta[:-1], edges_beta[1:])):\n",
    "#                 sel = (\n",
    "#                     (logMh   >= mh_bin[0]) & (logMh   < mh_bin[1]) &\n",
    "#                     (logMstar>= ms_bin[0]) & (logMstar< ms_bin[1]) &\n",
    "#                     (logRe   >= re_bin[0]) & (logRe   < re_bin[1]) &\n",
    "#                     (beta    >= b_bin[0]) & (beta    < b_bin[1])\n",
    "#                 )\n",
    "#                 if sel.any():\n",
    "#                     betamax_grid[i, j, k, l] = betamax[sel].mean()\n",
    "\n",
    "# # 构建插值器\n",
    "# interp = RegularGridInterpolator(\n",
    "#     (edges_logMh[:-1], edges_logMstar[:-1], edges_logRe[:-1], edges_beta[:-1]),\n",
    "#     betamax_grid,\n",
    "#     method=\"linear\",\n",
    "#     bounds_error=False,\n",
    "#     fill_value=np.nan,\n",
    "# )\n",
    "\n",
    "# # ========= 3. 随机抽样对比 =========\n",
    "# idx = np.random.choice(len(betamax), size=5000, replace=False)\n",
    "# points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "# pred = interp(points)\n",
    "\n",
    "# # ========= 4. 可视化对比 =========\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.scatter(betamax[idx], pred, s=2, alpha=0.3)\n",
    "# lim = [betamax[idx].min(), betamax[idx].max()]\n",
    "# plt.plot(lim, lim, 'r--', lw=1)\n",
    "# plt.xlabel(\"True betamax\")\n",
    "# plt.ylabel(\"Interpolated betamax\")\n",
    "# plt.title(\"插值结果 vs 真值\")\n",
    "# plt.show()\n",
    "\n",
    "# rel_err = np.abs((pred - betamax[idx]) / betamax[idx])\n",
    "# print(f\"[误差统计] mean={rel_err.mean():.3e}, 95%={np.quantile(rel_err, 0.95):.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b785d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import h5py\n",
    "# from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# # === 1. 构建插值器（和前面类似） ===\n",
    "# bank_path = \"./bank.h5\"   # 改成你的 bank.h5 路径\n",
    "# with h5py.File(bank_path, \"r\") as f:\n",
    "#     logMh   = f[\"/base/logMh\"][:]\n",
    "#     logMstar= f[\"/base/logM_star\"][:]\n",
    "#     logRe   = f[\"/base/logRe\"][:]\n",
    "#     beta    = f[\"/base/beta\"][:]\n",
    "#     betamax = f[\"/lens/betamax\"][:]\n",
    "#     ok_mask = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "\n",
    "# mask = ok_mask & np.isfinite(betamax)\n",
    "# logMh, logMstar, logRe, beta, betamax = logMh[mask], logMstar[mask], logRe[mask], beta[mask], betamax[mask]\n",
    "\n",
    "# bins = 32\n",
    "# edges_logMh    = np.linspace(logMh.min(), logMh.max(), bins)\n",
    "# edges_logMstar = np.linspace(logMstar.min(), logMstar.max(), bins)\n",
    "# edges_logRe    = np.linspace(logRe.min(), logRe.max(), bins)\n",
    "# edges_beta     = np.linspace(beta.min(), beta.max(), bins)\n",
    "\n",
    "# betamax_grid = np.full((bins-1, bins-1, bins-1, bins-1), np.nan, dtype=np.float32)\n",
    "# for i in range(bins-1):\n",
    "#     for j in range(bins-1):\n",
    "#         for k in range(bins-1):\n",
    "#             for l in range(bins-1):\n",
    "#                 sel = (\n",
    "#                     (logMh   >= edges_logMh[i])   & (logMh   < edges_logMh[i+1]) &\n",
    "#                     (logMstar>= edges_logMstar[j])& (logMstar< edges_logMstar[j+1]) &\n",
    "#                     (logRe   >= edges_logRe[k])   & (logRe   < edges_logRe[k+1]) &\n",
    "#                     (beta    >= edges_beta[l])    & (beta    < edges_beta[l+1])\n",
    "#                 )\n",
    "#                 if sel.any():\n",
    "#                     betamax_grid[i,j,k,l] = betamax[sel].mean()\n",
    "\n",
    "# interp = RegularGridInterpolator(\n",
    "#     (edges_logMh[:-1], edges_logMstar[:-1], edges_logRe[:-1], edges_beta[:-1]),\n",
    "#     betamax_grid,\n",
    "#     method=\"linear\", bounds_error=False, fill_value=np.nan\n",
    "# )\n",
    "\n",
    "# # === 2. 用包里的透镜求解器计算真值 ===\n",
    "# from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.lens_model import LensModel\n",
    "# from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.lens_solver import solve_lens\n",
    "\n",
    "# def true_solver(logMh, logMstar, logRe, beta, zl=0.3, zs=2.0):\n",
    "#     model = LensModel(logM_star=logMstar, logM_halo=logMh, logRe=logRe, zl=zl, zs=zs)\n",
    "#     try:\n",
    "#         xA, xB = solve_lens(model, beta)\n",
    "#         mu1 = model.mu_from_rt(xA)\n",
    "#         mu2 = model.mu_from_rt(xB)\n",
    "#         betamax = model.solve_ycaustic()\n",
    "#         return mu1, mu2, betamax\n",
    "#     except Exception:\n",
    "#         return np.nan, np.nan, np.nan\n",
    "\n",
    "# # === 3. 随机抽几组样本，对比真值 vs 插值 ===\n",
    "# np.random.seed(42)\n",
    "# test_idx = np.random.choice(len(logMh), size=200, replace=False)\n",
    "# params = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "\n",
    "# true_vals, interp_vals = [], []\n",
    "# for p in params:\n",
    "#     mu1, mu2, bm_true = true_solver(*p)\n",
    "#     bm_pred = interp([p])[0]\n",
    "#     true_vals.append(bm_true)\n",
    "#     interp_vals.append(bm_pred)\n",
    "\n",
    "# true_vals, interp_vals = np.array(true_vals), np.array(interp_vals)\n",
    "\n",
    "# # === 4. 可视化比较 ===\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.scatter(true_vals, interp_vals, s=20, alpha=0.6)\n",
    "# lim = [np.nanmin(true_vals), np.nanmax(true_vals)]\n",
    "# plt.plot(lim, lim, \"r--\", lw=1)\n",
    "# plt.xlabel(\"True betamax (solver)\")\n",
    "# plt.ylabel(\"Interpolated betamax\")\n",
    "# plt.title(\"插值 vs 真值\")\n",
    "# plt.show()\n",
    "\n",
    "# rel_err = np.nanmean(np.abs((interp_vals - true_vals)/true_vals))\n",
    "# print(f\"[平均相对误差] {rel_err:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff1f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42cafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb795344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089b746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738547d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. 读取 bank 数据 ===\n",
    "bank_path = \"./bank.h5\"   # 修改为你的路径\n",
    "with h5py.File(bank_path, \"r\") as f:\n",
    "    logMh    = f[\"/base/logMh\"][:]\n",
    "    logMstar = f[\"/base/logM_star\"][:]\n",
    "    logRe    = f[\"/base/logRe\"][:]\n",
    "    beta     = f[\"/base/beta\"][:]\n",
    "    betamax  = f[\"/lens/betamax\"][:]\n",
    "    ok_mask  = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "\n",
    "mask = ok_mask & np.isfinite(betamax)\n",
    "logMh, logMstar, logRe, beta, betamax = logMh[mask], logMstar[mask], logRe[mask], beta[mask], betamax[mask]\n",
    "\n",
    "# === 2. 仅取附近点用于快速插值器构建 ===\n",
    "np.random.seed(0)\n",
    "N_train = 20000  # 训练点数量，越小越快\n",
    "idx = np.random.choice(len(logMh), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "train_values = betamax[idx]\n",
    "\n",
    "print(f\"[info] 构建局部插值器，训练样本数={N_train}\")\n",
    "interp = LinearNDInterpolator(train_points, train_values, fill_value=np.nan)\n",
    "\n",
    "# === 3. 随机选一小批测试点，直接用真值对比 ===\n",
    "N_test = 500\n",
    "test_idx = np.random.choice(len(logMh), size=N_test, replace=False)\n",
    "test_points = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_vals = betamax[test_idx]\n",
    "pred_vals = interp(test_points)\n",
    "\n",
    "# === 4. 可视化比较 ===\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(true_vals, pred_vals, alpha=0.5, s=15)\n",
    "lims = [np.nanmin(true_vals), np.nanmax(true_vals)]\n",
    "plt.plot(lims, lims, 'r--')\n",
    "plt.xlabel(\"True betamax\")\n",
    "plt.ylabel(\"Interpolated betamax\")\n",
    "plt.title(f\"fast test (N_train={N_train}, N_test={N_test})\")\n",
    "plt.show()\n",
    "\n",
    "rel_err = np.nanmean(np.abs((pred_vals - true_vals)/true_vals))\n",
    "print(f\"[average relative error] {rel_err:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选放大率<1000 的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31920379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. 读取 bank 数据 ===\n",
    "bank_path = \"./bank.h5\"\n",
    "with h5py.File(bank_path, \"r\") as f:\n",
    "    logMh    = f[\"/base/logMh\"][:]\n",
    "    logMstar = f[\"/base/logM_star\"][:]\n",
    "    logRe    = f[\"/base/logRe\"][:]\n",
    "    beta     = f[\"/base/beta\"][:]\n",
    "    betamax  = f[\"/lens/betamax\"][:]\n",
    "    mu1      = f[\"/lens/mu1\"][:]  # 读取放大率\n",
    "    mu2      = f[\"/lens/mu2\"][:]\n",
    "    ok_mask  = f[\"/lens/ok_mask\"][:].astype(bool)\n",
    "\n",
    "# === 基础筛选（只保证有解 & betamax 有效），不筛掉极端放大率 ===\n",
    "mask_base = ok_mask & np.isfinite(betamax)\n",
    "logMh, logMstar, logRe, beta, betamax, mu1, mu2 = (\n",
    "    logMh[mask_base],\n",
    "    logMstar[mask_base],\n",
    "    logRe[mask_base],\n",
    "    beta[mask_base],\n",
    "    betamax[mask_base],\n",
    "    mu1[mask_base],\n",
    "    mu2[mask_base],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54450cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === 2. 构建插值器（不去掉极端 μ 点）===\n",
    "np.random.seed(0)\n",
    "N_train = 20000\n",
    "idx = np.random.choice(len(logMh), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "train_values = betamax[idx]\n",
    "\n",
    "print(f\"[info] 构建局部插值器，训练样本数={N_train}\")\n",
    "interp = LinearNDInterpolator(train_points, train_values, fill_value=np.nan)\n",
    "\n",
    "# === 3. 随机选测试集 + 仅用于误差计算时筛掉 μ>1e4 ===\n",
    "N_test = 500\n",
    "test_idx = np.random.choice(len(logMh), size=N_test, replace=False)\n",
    "\n",
    "test_points = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_vals   = betamax[test_idx]\n",
    "pred_vals   = interp(test_points)\n",
    "mu1_test    = mu1[test_idx]\n",
    "mu2_test    = mu2[test_idx]\n",
    "\n",
    "# 筛掉放大率 > 1e4 的测试点\n",
    "mask_valid = (np.abs(mu1_test) < 1e4) & (np.abs(mu2_test) < 1e4)\n",
    "test_points = test_points[mask_valid]\n",
    "true_vals   = true_vals[mask_valid]\n",
    "pred_vals   = pred_vals[mask_valid]\n",
    "\n",
    "print(f\"[info] 有效测试点 {mask_valid.sum()}/{len(mask_valid)} ({mask_valid.mean()*100:.2f}%)\")\n",
    "\n",
    "# === 4. 可视化比较 ===\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(true_vals, pred_vals, alpha=0.5, s=15)\n",
    "lims = [np.nanmin(true_vals), np.nanmax(true_vals)]\n",
    "plt.plot(lims, lims, 'r--')\n",
    "plt.xlabel(\"True betamax\")\n",
    "plt.ylabel(\"Interpolated betamax\")\n",
    "plt.title(f\"Test (|μ|<1e4), N_train={N_train}, N_test={len(true_vals)})\")\n",
    "plt.show()\n",
    "\n",
    "rel_err = np.nanmean(np.abs((pred_vals - true_vals)/true_vals))\n",
    "print(f\"[average relative error] {rel_err:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469f32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "389c7d3a",
   "metadata": {},
   "source": [
    "# 之前把计算量都放进插值器了，下面的做出更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e132aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) 为 betamax / mu1 / mu2 构建插值器（同一输入 -> 三个输出）===\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "# 已有：train_points (N_train x 4)，idx 是训练索引\n",
    "interp_betamax = LinearNDInterpolator(train_points, betamax[idx], fill_value=np.nan)\n",
    "interp_mu1     = LinearNDInterpolator(train_points, mu1[idx],     fill_value=np.nan)\n",
    "interp_mu2     = LinearNDInterpolator(train_points, mu2[idx],     fill_value=np.nan)\n",
    "\n",
    "# 已有：test_points / true_vals(=betamax真值) / pred_vals(=betamax插值预测)\n",
    "# 以及 mu1_test / mu2_test，且 test_points/true_vals/pred_vals 已经用 mask_valid 过滤过\n",
    "true_betamax = true_vals\n",
    "pred_betamax = pred_vals\n",
    "true_mu1     = mu1_test[mask_valid]\n",
    "true_mu2     = mu2_test[mask_valid]\n",
    "\n",
    "# 用同一批 test_points 做 mu1/mu2 的插值预测\n",
    "pred_mu1 = interp_mu1(test_points)\n",
    "pred_mu2 = interp_mu2(test_points)\n",
    "\n",
    "# === 6) 逐变量计算误差（安全处理 0/NaN）===\n",
    "def safe_rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    if m.sum() == 0:\n",
    "        return np.array([]), m\n",
    "    return np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m]))), m\n",
    "\n",
    "err_beta, m_beta = safe_rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = safe_rel_err(pred_mu1,     true_mu1)\n",
    "err_mu2,  m_mu2  = safe_rel_err(pred_mu2,     true_mu2)\n",
    "\n",
    "print(f\"[betamax] mean rel.err = {np.nanmean(err_beta) if err_beta.size else np.nan:.3e}, N={m_beta.sum()}\")\n",
    "print(f\"[mu1]     mean rel.err = {np.nanmean(err_mu1)  if err_mu1.size  else np.nan:.3e}, N={m_mu1.sum()}\")\n",
    "print(f\"[mu2]     mean rel.err = {np.nanmean(err_mu2)  if err_mu2.size  else np.nan:.3e}, N={m_mu2.sum()}\")\n",
    "\n",
    "# === 7) 多子图：真值 vs 插值 ===\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "\n",
    "pairs = [\n",
    "    (\"betamax\", pred_betamax[m_beta], true_betamax[m_beta], err_beta, axes[0]),\n",
    "    (\"mu1\",     pred_mu1[m_mu1],      true_mu1[m_mu1],      err_mu1,  axes[1]),\n",
    "    (\"mu2\",     pred_mu2[m_mu2],      true_mu2[m_mu2],      err_mu2,  axes[2]),\n",
    "]\n",
    "\n",
    "for name, pred_ok, true_ok, err_ok, ax in pairs:\n",
    "    ax.scatter(true_ok, pred_ok, s=10, alpha=0.35)\n",
    "    # 对角线范围覆盖真值/预测\n",
    "    vmin = np.nanmin([true_ok.min(), pred_ok.min()])\n",
    "    vmax = np.nanmax([true_ok.max(), pred_ok.max()])\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    ax.set_xlim(vmin, vmax)\n",
    "    ax.set_ylim(vmin, vmax)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean rel.err={np.nanmean(err_ok):.2e} | N={len(err_ok)}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303dba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "\n",
    "pairs = [\n",
    "    (\"betamax\", pred_betamax[m_beta], true_betamax[m_beta], err_beta, axes[0]),\n",
    "    (\"mu1\",     pred_mu1[m_mu1],      true_mu1[m_mu1],      err_mu1,  axes[1]),\n",
    "    (\"mu2\",     pred_mu2[m_mu2],      true_mu2[m_mu2],      err_mu2,  axes[2]),\n",
    "]\n",
    "\n",
    "for name, pred_ok, true_ok, err_ok, ax in pairs:\n",
    "    ax.scatter(true_ok, pred_ok, s=10, alpha=0.35)\n",
    "    # 对角线范围覆盖真值/预测\n",
    "    vmin = np.nanmin([true_ok.min(), pred_ok.min()])\n",
    "    vmax = np.nanmax([true_ok.max(), pred_ok.max()])\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(vmin, vmax)\n",
    "        ax.set_ylim(vmin, vmax)\n",
    "    else:\n",
    "        ax.set_xlim(0, 1000)\n",
    "        ax.set_ylim(0, 1000)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean rel.err={np.nanmean(err_ok):.2e} | N={len(err_ok)}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d8f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d275bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) 为 betamax / mu1 / mu2 构建插值器（同一输入 -> 三个输出）===\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import numpy as np\n",
    "# === 2. 构建插值器（不去掉极端 μ 点）===\n",
    "np.random.seed(0)\n",
    "N_train = 200000\n",
    "idx = np.random.choice(len(logMh), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "train_values = betamax[idx]\n",
    "# ——新增：μ 的稳定变换与逆变换（保号，压缩动态范围）\n",
    "def _mu_t(x):        # transform\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def _mu_inv(y):      # inverse\n",
    "    return np.sign(y) * np.expm1(np.abs(y))\n",
    "\n",
    "# 已有：train_points (N_train x 4)，idx 是训练索引\n",
    "interp_betamax = LinearNDInterpolator(train_points, betamax[idx],      fill_value=np.nan)\n",
    "interp_mu1     = LinearNDInterpolator(train_points, _mu_t(mu1[idx]),   fill_value=np.nan)\n",
    "interp_mu2     = LinearNDInterpolator(train_points, _mu_t(mu2[idx]),   fill_value=np.nan)\n",
    "\n",
    "# 已有：test_points / true_vals(=betamax真值) / pred_vals(=betamax插值预测)\n",
    "# 以及 mu1_test / mu2_test，且 test_points/true_vals/pred_vals 已经用 mask_valid 过滤过\n",
    "true_betamax = true_vals\n",
    "pred_betamax = pred_vals\n",
    "true_mu1     = mu1_test[mask_valid]\n",
    "true_mu2     = mu2_test[mask_valid]\n",
    "\n",
    "N_test = 5000\n",
    "test_idx = np.random.choice(len(logMh), size=N_test, replace=False)\n",
    "\n",
    "test_points = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "\n",
    "train_points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "train_values = betamax[idx]\n",
    "\n",
    "\n",
    "# 用同一批 test_points 做 mu1/mu2 的插值预测（注意先反变换回 μ）\n",
    "pred_mu1 = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2 = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "# === 6) 逐变量计算误差（安全处理 0/NaN）===\n",
    "def safe_rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    if m.sum() == 0:\n",
    "        return np.array([]), m\n",
    "    return np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m]))), m\n",
    "\n",
    "err_beta, m_beta = safe_rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = safe_rel_err(pred_mu1,     true_mu1)\n",
    "err_mu2,  m_mu2  = safe_rel_err(pred_mu2,     true_mu2)\n",
    "\n",
    "print(f\"[betamax] mean rel.err = {np.nanmean(err_beta) if err_beta.size else np.nan:.3e}, N={m_beta.sum()}\")\n",
    "print(f\"[mu1]     mean rel.err = {np.nanmean(err_mu1)  if err_mu1.size  else np.nan:.3e}, N={m_mu1.sum()}\")\n",
    "print(f\"[mu2]     mean rel.err = {np.nanmean(err_mu2)  if err_mu2.size  else np.nan:.3e}, N={m_mu2.sum()}\")\n",
    "\n",
    "# === 7) 多子图：真值 vs 插值 ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "\n",
    "pairs = [\n",
    "    (\"betamax\", pred_betamax[m_beta], true_betamax[m_beta], err_beta, axes[0]),\n",
    "    (\"mu1\",     pred_mu1[m_mu1],      true_mu1[m_mu1],      err_mu1,  axes[1]),\n",
    "    (\"mu2\",     pred_mu2[m_mu2],      true_mu2[m_mu2],      err_mu2,  axes[2]),\n",
    "]\n",
    "\n",
    "for name, pred_ok, true_ok, err_ok, ax in pairs:\n",
    "    ax.scatter(true_ok, pred_ok, s=10, alpha=0.35)\n",
    "    vmin = np.nanmin([true_ok.min(), pred_ok.min()])\n",
    "    vmax = np.nanmax([true_ok.max(), pred_ok.max()])\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    ax.set_xlim(vmin, vmax)\n",
    "    ax.set_ylim(vmin, vmax)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean rel.err={np.nanmean(err_ok):.2e} | N={len(err_ok)}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 统一抽样同一批测试点\n",
    "N_test = 5000\n",
    "test_idx = np.random.choice(len(logMh), size=N_test, replace=False)\n",
    "test_points  = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "\n",
    "# 对应的真值（注意：与 test_idx 一一对应）\n",
    "true_betamax = betamax[test_idx]\n",
    "true_mu1     = mu1[test_idx]\n",
    "true_mu2     = mu2[test_idx]\n",
    "\n",
    "# 预测值（与 test_points 一一对应）\n",
    "pred_betamax = interp_betamax(test_points)\n",
    "pred_mu1     = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2     = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "# 可选：过滤极端放大率（在“真值”上判断，并同步到预测）\n",
    "mask_mag = (np.abs(true_mu1) < 1e4) & (np.abs(true_mu2) < 1e4)\n",
    "true_betamax, pred_betamax = true_betamax[mask_mag], pred_betamax[mask_mag]\n",
    "true_mu1,     pred_mu1     = true_mu1[mask_mag],     pred_mu1[mask_mag]\n",
    "true_mu2,     pred_mu2     = true_mu2[mask_mag],     pred_mu2[mask_mag]\n",
    "\n",
    "# 统一的安全相对误差函数\n",
    "def safe_rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    err = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return err, m\n",
    "\n",
    "err_beta, m_beta = safe_rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = safe_rel_err(pred_mu1,     true_mu1)\n",
    "err_mu2,  m_mu2  = safe_rel_err(pred_mu2,     true_mu2)\n",
    "\n",
    "print(f\"[betamax] mean rel.err = {np.nanmean(err_beta):.3e}, N={m_beta.sum()}\")\n",
    "print(f\"[mu1]     mean rel.err = {np.nanmean(err_mu1):.3e}, N={m_mu1.sum()}\")\n",
    "print(f\"[mu2]     mean rel.err = {np.nanmean(err_mu2):.3e}, N={m_mu2.sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"[betamax] mean rel.err = {np.nanmean(err_beta) if err_beta.size else np.nan:.3e}, N={m_beta.sum()}\")\n",
    "print(f\"[mu1]     mean rel.err = {np.nanmean(err_mu1)  if err_mu1.size  else np.nan:.3e}, N={m_mu1.sum()}\")\n",
    "print(f\"[mu2]     mean rel.err = {np.nanmean(err_mu2)  if err_mu2.size  else np.nan:.3e}, N={m_mu2.sum()}\")\n",
    "\n",
    "# === 7) 多子图：真值 vs 插值 ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "\n",
    "pairs = [\n",
    "    (\"betamax\", pred_betamax[m_beta], true_betamax[m_beta], err_beta, axes[0]),\n",
    "    (\"mu1\",     pred_mu1[m_mu1],      true_mu1[m_mu1],      err_mu1,  axes[1]),\n",
    "    (\"mu2\",     pred_mu2[m_mu2],      true_mu2[m_mu2],      err_mu2,  axes[2]),\n",
    "]\n",
    "\n",
    "for name, pred_ok, true_ok, err_ok, ax in pairs:\n",
    "    ax.scatter(true_ok, pred_ok, s=10, alpha=0.35)\n",
    "    vmin = np.nanmin([true_ok.min(), pred_ok.min()])\n",
    "    vmax = np.nanmax([true_ok.max(), pred_ok.max()])\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(vmin, vmax)\n",
    "        ax.set_ylim(vmin, vmax)\n",
    "    else:\n",
    "        ax.set_xlim(0, 30)\n",
    "        ax.set_ylim(0, 30)\n",
    "\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean rel.err={np.nanmean(err_ok):.2e} | N={len(err_ok)}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 仅在 |μ|≤30 子域里训练 & 评估；目标误差 < 1e-3 =====\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "MU_CAP = 30.0\n",
    "TARGET_RELERR = 1e-3  # 0.1%\n",
    "\n",
    "def _mu_t(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "def _mu_inv(y):\n",
    "    return np.sign(y) * np.expm1(np.abs(y))\n",
    "\n",
    "# ---------- 1) 训练集：仅保留 |μ|≤30 ----------\n",
    "train_mask = (\n",
    "    np.isfinite(betamax) &\n",
    "    np.isfinite(mu1) & np.isfinite(mu2) &\n",
    "    (np.abs(mu1) <= MU_CAP) & (np.abs(mu2) <= MU_CAP)\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "N_train = min(200_000, train_mask.sum())\n",
    "train_idx = rng.choice(np.flatnonzero(train_mask), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[train_idx], logMstar[train_idx], logRe[train_idx], beta[train_idx]])\n",
    "\n",
    "interp_betamax = LinearNDInterpolator(train_points, betamax[train_idx],                fill_value=np.nan)\n",
    "interp_mu1     = LinearNDInterpolator(train_points, _mu_t(mu1[train_idx]),             fill_value=np.nan)\n",
    "interp_mu2     = LinearNDInterpolator(train_points, _mu_t(mu2[train_idx]),             fill_value=np.nan)\n",
    "\n",
    "# ---------- 2) 测试集：同样限制在 |μ|≤30 ----------\n",
    "N_test = 5000\n",
    "cand_idx = rng.choice(len(logMh), size=5*N_test, replace=False)      # 先抽一批候选\n",
    "cand_mask = (\n",
    "    np.isfinite(betamax[cand_idx]) &\n",
    "    np.isfinite(mu1[cand_idx]) & np.isfinite(mu2[cand_idx]) &\n",
    "    (np.abs(mu1[cand_idx]) <= MU_CAP) & (np.abs(mu2[cand_idx]) <= MU_CAP)\n",
    ")\n",
    "test_idx = cand_idx[cand_mask][:N_test]\n",
    "\n",
    "test_points   = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_betamax  = betamax[test_idx]\n",
    "true_mu1      = mu1[test_idx]\n",
    "true_mu2      = mu2[test_idx]\n",
    "\n",
    "pred_betamax  = interp_betamax(test_points)\n",
    "pred_mu1      = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2      = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "# ---------- 3) 误差评估 ----------\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "err_beta, m_beta = rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = rel_err(pred_mu1,     true_mu1)\n",
    "err_mu2,  m_mu2  = rel_err(pred_mu2,     true_mu2)\n",
    "\n",
    "def summarize(name, e):\n",
    "    if e.size == 0:\n",
    "        print(f\"[{name}] no valid points\")\n",
    "        return\n",
    "    print(f\"[{name}] mean={e.mean():.3e}, p95={np.percentile(e,95):.3e}, max={e.max():.3e} \"\n",
    "          f\"{'✅' if (e.mean()<TARGET_RELERR and np.percentile(e,95)<TARGET_RELERR) else '❌'}\")\n",
    "\n",
    "summarize(\"betamax\", err_beta)\n",
    "summarize(\"mu1    \", err_mu1)\n",
    "summarize(\"mu2    \", err_mu2)\n",
    "\n",
    "# ---------- 4) 可视化（坐标限定在 0–MU_CAP） ----------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "pairs = [\n",
    "    (\"betamax\", pred_betamax[m_beta], true_betamax[m_beta], err_beta, axes[0], None), # betamax不截轴\n",
    "    (\"mu1\",     pred_mu1[m_mu1],      true_mu1[m_mu1],      err_mu1,  axes[1], MU_CAP),\n",
    "    (\"mu2\",     pred_mu2[m_mu2],      true_mu2[m_mu2],      err_mu2,  axes[2], MU_CAP),\n",
    "]\n",
    "for name, p, t, e, ax, cap in pairs:\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    vmin = 0 if cap is not None else float(np.nanmin([t.min(), p.min()]))\n",
    "    vmax = cap if cap is not None else float(np.nanmax([t.max(), p.max()]))\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    ax.set_xlim(vmin, vmax); ax.set_ylim(vmin, vmax)\n",
    "    ax.set_xlabel(f\"True {name}\"); ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean={np.nanmean(e):.2e}, p95={np.percentile(e,95):.2e}, max={np.max(e):.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mu1, bins=100, range=(-30,30), alpha=0.5, label=\"mu1\", density=True)\n",
    "plt.hist(mu2, bins=100, range=(-30,30), alpha=0.5, label=\"mu2\", density=True)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694bca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8dacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 仅在 |μ|≤30 子域里训练 & 评估；目标误差 < 1e-3 =====\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "MU_CAP = 30.0\n",
    "TARGET_RELERR = 1e-3  # 0.1%\n",
    "\n",
    "def _mu_t(x):  # transform\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "def _mu_inv(y):  # inverse\n",
    "    return np.sign(y) * np.expm1(np.abs(y))\n",
    "\n",
    "# ---------- 1) 训练集：仅保留 |μ|≤30 ----------\n",
    "train_mask = (\n",
    "    np.isfinite(betamax) &\n",
    "    np.isfinite(mu1) & np.isfinite(mu2) &\n",
    "    (np.abs(mu1) <= MU_CAP) & (np.abs(mu2) <= MU_CAP)\n",
    ")\n",
    "rng = np.random.default_rng(0)\n",
    "N_train = min(200_000, int(train_mask.sum()))\n",
    "train_idx = rng.choice(np.flatnonzero(train_mask), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[train_idx], logMstar[train_idx], logRe[train_idx], beta[train_idx]])\n",
    "interp_betamax = LinearNDInterpolator(train_points, betamax[train_idx],            fill_value=np.nan)\n",
    "interp_mu1     = LinearNDInterpolator(train_points, _mu_t(mu1[train_idx]),         fill_value=np.nan)\n",
    "interp_mu2     = LinearNDInterpolator(train_points, _mu_t(mu2[train_idx]),         fill_value=np.nan)\n",
    "\n",
    "# ---------- 2) 测试集：先抽候选，再用 |mu1|≤MU_CAP 过滤 ----------\n",
    "N_test = 5000\n",
    "cand_idx = rng.choice(len(logMh), size=5*N_test, replace=False)\n",
    "cand_mask_mu1 = (\n",
    "    np.isfinite(betamax[cand_idx]) &\n",
    "    np.isfinite(mu1[cand_idx]) & (np.abs(mu1[cand_idx]) <= MU_CAP)\n",
    ")\n",
    "test_idx = cand_idx[cand_mask_mu1][:N_test]\n",
    "\n",
    "test_points  = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_betamax = betamax[test_idx]\n",
    "true_mu1     = mu1[test_idx]\n",
    "true_mu2     = mu2[test_idx]\n",
    "\n",
    "pred_betamax = interp_betamax(test_points)\n",
    "pred_mu1     = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2     = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "# ---------- 3) 误差评估 ----------\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "# betamax / mu1 正常评估\n",
    "err_beta, m_beta = rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = rel_err(pred_mu1,     true_mu1)\n",
    "\n",
    "# mu2：用 mu1 的“有效掩码”来筛选同一批点，然后在这个子集上计算 mu2 误差\n",
    "# （并额外确保 mu2 的真值/预测是有限的）\n",
    "mask_common_for_mu2 = m_mu1.copy()\n",
    "mask_common_for_mu2[mask_common_for_mu2] &= (\n",
    "    np.isfinite(pred_mu2[mask_common_for_mu2]) &\n",
    "    np.isfinite(true_mu2[mask_common_for_mu2]) &\n",
    "    (np.abs(true_mu2[mask_common_for_mu2]) > 0)\n",
    ")\n",
    "err_mu2 = np.abs(\n",
    "    (pred_mu2[mask_common_for_mu2] - true_mu2[mask_common_for_mu2]) /\n",
    "    np.maximum(1e-12, np.abs(true_mu2[mask_common_for_mu2]))\n",
    ")\n",
    "\n",
    "def summarize(name, e):\n",
    "    if e.size == 0:\n",
    "        print(f\"[{name}] no valid points\")\n",
    "        return\n",
    "    print(f\"[{name}] mean={e.mean():.3e}, p95={np.percentile(e,95):.3e}, max={e.max():.3e} \"\n",
    "          f\"{'✅' if (e.mean()<TARGET_RELERR and np.percentile(e,95)<TARGET_RELERR) else '❌'}\")\n",
    "\n",
    "summarize(\"betamax\", err_beta)\n",
    "summarize(\"mu1    \", err_mu1)\n",
    "summarize(\"mu2    \", err_mu2)\n",
    "\n",
    "# ---------- 4) 可视化 ----------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "\n",
    "# betamax：按自身有效掩码\n",
    "tb, pb = true_betamax[m_beta], pred_betamax[m_beta]\n",
    "axes[0].scatter(tb, pb, s=10, alpha=0.35)\n",
    "vmin, vmax = float(np.nanmin([tb.min(), pb.min()])), float(np.nanmax([tb.max(), pb.max()]))\n",
    "axes[0].plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "axes[0].set_xlim(vmin, vmax); axes[0].set_ylim(vmin, vmax)\n",
    "axes[0].set_xlabel(\"True betamax\"); axes[0].set_ylabel(\"Interpolated betamax\")\n",
    "axes[0].set_title(f\"betamax | mean={err_beta.mean():.2e}, p95={np.percentile(err_beta,95):.2e}, max={np.max(err_beta):.2e}\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# mu1：按 mu1 掩码，并限制到 0..MU_CAP 作图\n",
    "t1, p1 = true_mu1[m_mu1], pred_mu1[m_mu1]\n",
    "axes[1].scatter(t1, p1, s=10, alpha=0.35)\n",
    "axes[1].plot([0, MU_CAP], [0, MU_CAP], 'r--', lw=1)\n",
    "axes[1].set_xlim(0, MU_CAP); axes[1].set_ylim(0, MU_CAP)\n",
    "axes[1].set_xlabel(\"True mu1\"); axes[1].set_ylabel(\"Interpolated mu1\")\n",
    "axes[1].set_title(f\"mu1 | mean={err_mu1.mean():.2e}, p95={np.percentile(err_mu1,95):.2e}, max={np.max(err_mu1):.2e}\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# mu2：用“mu1 的有效点集合”筛选，不做坐标限幅（按数据自适应）\n",
    "t2 = true_mu2[mask_common_for_mu2]\n",
    "p2 = pred_mu2[mask_common_for_mu2]\n",
    "axes[2].scatter(t2, p2, s=10, alpha=0.35)\n",
    "vmin2, vmax2 = float(np.nanmin([t2.min(), p2.min()])), float(np.nanmax([t2.max(), p2.max()]))\n",
    "axes[2].plot([vmin2, vmax2], [vmin2, vmax2], 'r--', lw=1)\n",
    "axes[2].set_xlim(vmin2, vmax2); axes[2].set_ylim(vmin2, vmax2)\n",
    "axes[2].set_xlabel(\"True mu2\"); axes[2].set_ylabel(\"Interpolated mu2\")\n",
    "axes[2].set_title(f\"mu2 | mean={err_mu2.mean():.2e}, p95={np.percentile(err_mu2,95):.2e}, max={np.max(err_mu2):.2e}\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c232e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== 配置 =====\n",
    "ERR_THR = 0.05      # 相对误差阈值（可改）\n",
    "SUBSAMPLE = 4000    # 每张图最多取多少点以加速绘制\n",
    "\n",
    "def make_pair(title, pts, err, err_thr=ERR_THR, subsample=SUBSAMPLE):\n",
    "    \"\"\"根据参数点 pts (N,4) 与误差 err (N,) 画 pairplot。\"\"\"\n",
    "    ok = np.isfinite(err)\n",
    "    pts = pts[ok]\n",
    "    err = err[ok]\n",
    "    lab = np.where(err > err_thr, f\"err>{err_thr:.2g}\", f\"err≤{err_thr:.2g}\")\n",
    "\n",
    "    if len(pts) > subsample:\n",
    "        idx = np.random.default_rng(0).choice(len(pts), size=subsample, replace=False)\n",
    "        pts, lab = pts[idx], lab[idx]\n",
    "\n",
    "    df = pd.DataFrame(pts, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "    df[\"label\"] = lab\n",
    "\n",
    "    g = sns.pairplot(\n",
    "        df,\n",
    "        vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"],\n",
    "        hue=\"label\",\n",
    "        diag_kind=\"kde\",\n",
    "        plot_kws={\"alpha\":0.5, \"s\":10},\n",
    "        palette={\"err≤\"+f\"{err_thr:.2g}\":\"#7db8ff\", f\"err>{err_thr:.2g}\":\"#ff5a5a\"},\n",
    "        corner=True\n",
    "    )\n",
    "    g.fig.suptitle(title + f\" | N={len(df)}  (>{err_thr:.2g} : {(lab==f'err>{err_thr:.2g}').mean()*100:.1f}%)\",\n",
    "                   y=1.02, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# ---- 各量的 pairplot ----\n",
    "\n",
    "# 1) betamax：用 m_beta 对应的 test_points 子集与误差 err_beta\n",
    "points_beta = test_points[m_beta]\n",
    "make_pair(\"betamax  high-error locations\",\n",
    "          points_beta, err_beta)\n",
    "\n",
    "# 2) mu1：用 m_mu1 子集与误差 err_mu1\n",
    "points_mu1 = test_points[m_mu1]\n",
    "make_pair(\"mu1  high-error locations\",\n",
    "          points_mu1, err_mu1)\n",
    "\n",
    "# 3) mu2：用与 mu1 对齐的集合（mask_common_for_mu2），误差为上面计算的 err_mu2\n",
    "points_mu2 = test_points[mask_common_for_mu2]\n",
    "make_pair(\"mu2  high-error locations (aligned with mu1 set)\",\n",
    "          points_mu2, err_mu2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dff34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7dedf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "# ========== 预处理和函数 ==========\n",
    "MU_CAP = 30.0\n",
    "BETAMAX_CAP = 20.0\n",
    "TARGET_RELERR = 1e-3\n",
    "\n",
    "def _mu_t(x):  # transform\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "def _mu_inv(y):  # inverse\n",
    "    return np.sign(y) * np.expm1(np.abs(y))\n",
    "\n",
    "# ---------- 1) 训练集：保留 |mu1|≤30 & |mu2|≤30 & betamax≤20 ----------\n",
    "train_mask = (\n",
    "    np.isfinite(betamax) & np.isfinite(mu1) & np.isfinite(mu2) &\n",
    "    (np.abs(mu1) <= MU_CAP) & (np.abs(mu2) <= MU_CAP) &\n",
    "    (betamax <= BETAMAX_CAP)\n",
    ")\n",
    "rng = np.random.default_rng(0)\n",
    "# N_train = min(200_000, int(train_mask.sum()))\n",
    "N_train = 500_000\n",
    "train_idx = rng.choice(np.flatnonzero(train_mask), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[train_idx], logMstar[train_idx], logRe[train_idx], beta[train_idx]])\n",
    "interp_betamax = LinearNDInterpolator(train_points, betamax[train_idx], fill_value=np.nan)\n",
    "interp_mu1     = LinearNDInterpolator(train_points, _mu_t(mu1[train_idx]), fill_value=np.nan)\n",
    "interp_mu2     = LinearNDInterpolator(train_points, _mu_t(mu2[train_idx]), fill_value=np.nan)\n",
    "\n",
    "# ---------- 2) 测试集：用同样条件筛选 ----------\n",
    "cand_idx = rng.choice(len(logMh), size=25000, replace=False)\n",
    "cand_mask = (\n",
    "    np.isfinite(betamax[cand_idx]) & np.isfinite(mu1[cand_idx]) & np.isfinite(mu2[cand_idx]) &\n",
    "    (np.abs(mu1[cand_idx]) <= MU_CAP) & (np.abs(mu2[cand_idx]) <= MU_CAP) &\n",
    "    (betamax[cand_idx] <= BETAMAX_CAP)\n",
    ")\n",
    "test_idx = cand_idx[cand_mask]\n",
    "test_points  = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_betamax = betamax[test_idx]\n",
    "true_mu1     = mu1[test_idx]\n",
    "true_mu2     = mu2[test_idx]\n",
    "\n",
    "# ---------- 3) 预测 ----------\n",
    "pred_betamax = interp_betamax(test_points)\n",
    "pred_mu1     = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2     = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "# ---------- 4) 统一误差计算 ----------\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "err_beta, m_beta = rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = rel_err(pred_mu1, true_mu1)\n",
    "err_mu2,  m_mu2  = rel_err(pred_mu2, true_mu2)\n",
    "\n",
    "# ---------- 5) 打印汇总 ----------\n",
    "def summarize(name, e):\n",
    "    if e.size == 0:\n",
    "        print(f\"[{name}] no valid points\")\n",
    "        return\n",
    "    print(f\"[{name}] mean={e.mean():.3e}, p95={np.percentile(e,95):.3e}, max={e.max():.3e} \"\n",
    "          f\"{'✅' if (e.mean()<TARGET_RELERR and np.percentile(e,95)<TARGET_RELERR) else '❌'}\")\n",
    "\n",
    "summarize(\"betamax\", err_beta)\n",
    "summarize(\"mu1    \", err_mu1)\n",
    "summarize(\"mu2    \", err_mu2)\n",
    "\n",
    "# ---------- 6) 可视化 ----------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "pairs = [\n",
    "    (\"betamax\", pred_betamax[m_beta], true_betamax[m_beta], err_beta, axes[0], (0, BETAMAX_CAP)),\n",
    "    (\"mu1\",     pred_mu1[m_mu1],      true_mu1[m_mu1],      err_mu1,  axes[1], (0, MU_CAP)),\n",
    "    (\"mu2\",     pred_mu2[m_mu2],      true_mu2[m_mu2],      err_mu2,  axes[2], (0, MU_CAP)),\n",
    "]\n",
    "for name, p, t, e, ax, lims in pairs:\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    vmin, vmax = lims\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    ax.set_xlim(vmin, vmax); ax.set_ylim(vmin, vmax)\n",
    "    ax.set_xlabel(f\"True {name}\"); ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={np.max(e):.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631fca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4180eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 保存\n",
    "with open(\"lens_interpolators.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"betamax\": interp_betamax,\n",
    "        \"mu1\": interp_mu1,\n",
    "        \"mu2\": interp_mu2,\n",
    "    }, f)\n",
    "\n",
    "print(\"[info] Interpolators saved to lens_interpolators.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"lens_interpolators.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# interp_betamax = data[\"betamax\"]\n",
    "# interp_mu1     = data[\"mu1\"]\n",
    "# interp_mu2     = data[\"mu2\"]\n",
    "\n",
    "# print(\"[info] Interpolators loaded (no rebuild needed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995e0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22532a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288921cf",
   "metadata": {},
   "source": [
    "# 对比mu2不同函数的拟合效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "MU_CAP = 30.0\n",
    "BETAMAX_CAP = 20.0\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "# ========== 构建训练/测试集 ==========\n",
    "train_mask = (\n",
    "    np.isfinite(betamax) & np.isfinite(mu1) & np.isfinite(mu2) &\n",
    "    (np.abs(mu1) <= MU_CAP) & (np.abs(mu2) <= MU_CAP) &\n",
    "    (betamax <= BETAMAX_CAP)\n",
    ")\n",
    "N_train = 200_000\n",
    "train_idx = rng.choice(np.flatnonzero(train_mask), size=N_train, replace=False)\n",
    "train_points = np.column_stack([logMh[train_idx], logMstar[train_idx], logRe[train_idx], beta[train_idx]])\n",
    "\n",
    "cand_idx = rng.choice(len(logMh), size=25000, replace=False)\n",
    "cand_mask = train_mask[cand_idx]\n",
    "test_idx = cand_idx[cand_mask]\n",
    "test_points = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_mu2 = mu2[test_idx]\n",
    "\n",
    "# ========== 三种变换 ==========\n",
    "def mu2_log1p_transform(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "def mu2_log1p_inverse(y):\n",
    "    return np.sign(y) * np.expm1(np.abs(y))\n",
    "\n",
    "def mu2_piecewise_transform(x, mu0=5.0):\n",
    "    x = np.asarray(x)\n",
    "    y = np.empty_like(x)\n",
    "    mask = np.abs(x) <= mu0\n",
    "    y[mask] = x[mask]\n",
    "    y[~mask] = np.sign(x[~mask]) * (mu0 + np.log(np.abs(x[~mask]) - mu0 + 1.0))\n",
    "    return y\n",
    "def mu2_piecewise_inverse(y, mu0=5.0):\n",
    "    y = np.asarray(y)\n",
    "    x = np.empty_like(y)\n",
    "    mask = np.abs(y) <= mu0\n",
    "    x[mask] = y[mask]\n",
    "    x[~mask] = np.sign(y[~mask]) * (np.exp(np.abs(y[~mask]) - mu0) + mu0 - 1.0)\n",
    "    return x\n",
    "\n",
    "def mu2_inv_transform(x):\n",
    "    x = np.asarray(x)\n",
    "    mask = np.abs(x) > 1.0\n",
    "    y = np.empty_like(x)\n",
    "    y[mask] = 1.0 / x[mask]\n",
    "    y[~mask] = x[~mask]\n",
    "    return y, mask\n",
    "\n",
    "def mu2_inv_inverse(y, mask):\n",
    "    x = np.empty_like(y)\n",
    "    x[mask] = 1.0 / y[mask]\n",
    "    x[~mask] = y[~mask]\n",
    "    return x\n",
    "\n",
    "# ========== 插值并计算误差 ==========\n",
    "methods = {}\n",
    "\n",
    "# --- 方法1: log1p\n",
    "interp1 = LinearNDInterpolator(train_points, mu2_log1p_transform(mu2[train_idx]), fill_value=np.nan)\n",
    "pred1 = mu2_log1p_inverse(interp1(test_points))\n",
    "err1, m1 = rel_err(pred1, true_mu2)\n",
    "methods[\"log1p\"] = (pred1[m1], true_mu2[m1], err1)\n",
    "\n",
    "# --- 方法2: piecewise\n",
    "interp2 = LinearNDInterpolator(train_points, mu2_piecewise_transform(mu2[train_idx]), fill_value=np.nan)\n",
    "pred2 = mu2_piecewise_inverse(interp2(test_points))\n",
    "err2, m2 = rel_err(pred2, true_mu2)\n",
    "methods[\"piecewise\"] = (pred2[m2], true_mu2[m2], err2)\n",
    "\n",
    "# --- 方法3: 1/mu\n",
    "tvals, train_mask_inv = mu2_inv_transform(mu2[train_idx])\n",
    "interp3 = LinearNDInterpolator(train_points, tvals, fill_value=np.nan)\n",
    "pred_t = interp3(test_points)\n",
    "# 预测阶段需要重新计算 mask：用 true_mu2 的分布来决定哪些点需要反变换\n",
    "mask_pred_inv = np.abs(true_mu2) > 1.0\n",
    "pred3 = mu2_inv_inverse(pred_t, mask_pred_inv)\n",
    "err3, m3 = rel_err(pred3, true_mu2)\n",
    "methods[\"1/mu\"] = (pred3[m3], true_mu2[m3], err3)\n",
    "\n",
    "# ========== 可视化 ==========\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    ax.plot([0, MU_CAP], [0, MU_CAP], 'r--', lw=1)\n",
    "    ax.set_xlim(0, MU_CAP)\n",
    "    ax.set_ylim(0, MU_CAP)\n",
    "    ax.set_xlabel(\"True mu2\")\n",
    "    ax.set_ylabel(\"Predicted mu2\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa4f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "# === 1) 配置 ===\n",
    "CENTER_IDX = 12345  # 随机选一个训练点作为中心\n",
    "N_SCAN = 50        # β 扫描点数\n",
    "BETA_SPAN = 0.1    # 在中心点 β 附近 ±BETA_SPAN 范围扫描\n",
    "\n",
    "# === 2) 选择中心点 ===\n",
    "center_point = np.array([logMh[CENTER_IDX], logMstar[CENTER_IDX], logRe[CENTER_IDX], beta[CENTER_IDX]])\n",
    "center_beta = center_point[-1]\n",
    "\n",
    "# 构造 β 扫描点\n",
    "beta_scan = np.linspace(center_beta - BETA_SPAN, center_beta + BETA_SPAN, N_SCAN)\n",
    "scan_points = np.tile(center_point, (N_SCAN, 1))\n",
    "scan_points[:, -1] = beta_scan  # 只改变 β\n",
    "\n",
    "# === 3) 真值 & 插值 ===\n",
    "# 真值直接取 mu2 数组 or 调用 solve_lens 重新求解\n",
    "true_mu2_scan = np.array([mu2[np.argmin(np.abs(beta - b))] for b in beta_scan])  # 近似取最近点\n",
    "pred_mu2_scan = _mu_inv(interp_mu2(scan_points))  # 插值器预测\n",
    "\n",
    "# === 4) 数值梯度 (有限差分) ===\n",
    "# 中心差分计算 ∂mu2/∂β\n",
    "grad_true = np.gradient(true_mu2_scan, beta_scan)\n",
    "grad_pred = np.gradient(pred_mu2_scan, beta_scan)\n",
    "\n",
    "# === 5) 可视化 ===\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# 第一行：mu2 曲线\n",
    "axes[0].plot(beta_scan, true_mu2_scan, 'k-', label='True μ₂')\n",
    "axes[0].plot(beta_scan, pred_mu2_scan, 'r--', label='Interpolated μ₂')\n",
    "axes[0].set_ylabel(\"μ₂\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 第二行：梯度曲线\n",
    "axes[1].plot(beta_scan, grad_true, 'k-', label='True ∂μ₂/∂β')\n",
    "axes[1].plot(beta_scan, grad_pred, 'r--', label='Interpolated ∂μ₂/∂β')\n",
    "axes[1].set_xlabel(\"β\")\n",
    "axes[1].set_ylabel(\"Gradient\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"μ₂ vs β near β₀={center_beta:.3f} | logMh={center_point[0]:.3f}, logM*={center_point[1]:.3f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4620a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1) 配置 ===\n",
    "CENTER_IDX = 12345  # 随机选一个训练点作为中心\n",
    "N_SCAN = 50        # β 扫描点数\n",
    "BETA_SPAN = 0.1    # 在中心点 β 附近 ±BETA_SPAN 范围扫描\n",
    "\n",
    "# === 2) 选择中心点 ===\n",
    "center_point = np.array([logMh[CENTER_IDX], logMstar[CENTER_IDX], logRe[CENTER_IDX], beta[CENTER_IDX]])\n",
    "center_beta = center_point[-1]\n",
    "\n",
    "# 构造 β 扫描点\n",
    "beta_scan = np.linspace(center_beta - BETA_SPAN, center_beta + BETA_SPAN, N_SCAN)\n",
    "scan_points = np.tile(center_point, (N_SCAN, 1))\n",
    "scan_points[:, -1] = beta_scan  # 只改变 β\n",
    "\n",
    "# === 3) 真值 & 插值 ===\n",
    "# 真值近似取最近 β 的点\n",
    "true_mu1_scan = np.array([mu1[np.argmin(np.abs(beta - b))] for b in beta_scan])  # 近似真值\n",
    "pred_mu1_scan = _mu_inv(interp_mu1(scan_points))  # 插值器预测\n",
    "\n",
    "# === 4) 数值梯度 (有限差分) ===\n",
    "grad_true = np.gradient(true_mu1_scan, beta_scan)\n",
    "grad_pred = np.gradient(pred_mu1_scan, beta_scan)\n",
    "\n",
    "# === 5) 可视化 ===\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# 第一行：mu1 曲线\n",
    "axes[0].plot(beta_scan, true_mu1_scan, 'k-', label='True μ₁')\n",
    "axes[0].plot(beta_scan, pred_mu1_scan, 'r--', label='Interpolated μ₁')\n",
    "axes[0].set_ylabel(\"μ₁\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 第二行：梯度曲线\n",
    "axes[1].plot(beta_scan, grad_true, 'k-', label='True ∂μ₁/∂β')\n",
    "axes[1].plot(beta_scan, grad_pred, 'r--', label='Interpolated ∂μ₁/∂β')\n",
    "axes[1].set_xlabel(\"β\")\n",
    "axes[1].set_ylabel(\"Gradient\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"μ₁ vs β near β₀={center_beta:.3f} | logMh={center_point[0]:.3f}, logM*={center_point[1]:.3f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c94c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24eba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b005f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sl_ mock_generator.lens_model import LensModel\n",
    "# from mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "def scan_mu_vs_logMh(\n",
    "    logMh_min=12.0, logMh_max=13.5, n_scan=50,\n",
    "    logMstar=11.4, logRe=0.7, beta_unit=0.3,\n",
    "    zl=0.3, zs=2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    在均匀 logMh 网格上求解透镜方程并绘制 mu1, mu2, betamax.\n",
    "    \"\"\"\n",
    "    logMh_grid = np.linspace(logMh_min, logMh_max, n_scan)\n",
    "    mu1_list, mu2_list, betamax_list = [], [], []\n",
    "\n",
    "    for logMh in logMh_grid:\n",
    "        model = LensModel(logM_star=logMstar, logM_halo=logMh, logRe=logRe, zl=zl, zs=zs)\n",
    "        try:\n",
    "            xA, xB = solve_single_lens(model, beta_unit)\n",
    "            mu1_val = model.mu_from_rt(xA)\n",
    "            mu2_val = model.mu_from_rt(xB)\n",
    "            betamax_val = model.solve_ycaustic()\n",
    "        except Exception:\n",
    "            mu1_val = np.nan\n",
    "            mu2_val = np.nan\n",
    "            betamax_val = np.nan\n",
    "        mu1_list.append(mu1_val)\n",
    "        mu2_list.append(mu2_val)\n",
    "        betamax_list.append(betamax_val)\n",
    "\n",
    "    mu1_arr = np.array(mu1_list)\n",
    "    mu2_arr = np.array(mu2_list)\n",
    "    betamax_arr = np.array(betamax_list)\n",
    "\n",
    "    # === 画图 ===\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(6, 10), sharex=True, dpi=150)\n",
    "\n",
    "    axes[0].plot(logMh_grid, mu1_arr, 'o-', label=r'$\\mu_1$')\n",
    "    axes[0].set_ylabel(r'$\\mu_1$')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].plot(logMh_grid, mu2_arr, 'o-', label=r'$\\mu_2$', color='tab:red')\n",
    "    axes[1].set_ylabel(r'$\\mu_2$')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    axes[2].plot(logMh_grid, betamax_arr, 'o-', label=r'$\\beta_{\\max}$', color='tab:green')\n",
    "    axes[2].set_xlabel(r'$\\log M_h$')\n",
    "    axes[2].set_ylabel(r'$\\beta_{\\max}$')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"μ₁, μ₂, βmax vs logMh | logM*={logMstar:.2f}, logRe={logRe:.2f}, β={beta_unit:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return logMh_grid, mu1_arr, mu2_arr, betamax_arr\n",
    "\n",
    "# 直接调用测试\n",
    "logMh_grid, mu1_arr, mu2_arr, betamax_arr = scan_mu_vs_logMh(\n",
    "    logMh_min=12.0, logMh_max=13.5, n_scan=600,\n",
    "    logMstar=11.4, logRe=0.7, beta_unit=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34d639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f6c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === 1) 选择一个中心点 ===\n",
    "logMh0 = 12.8\n",
    "logMstar0 = 11.4\n",
    "logRe0 = 0.7\n",
    "beta0 = 0.3\n",
    "\n",
    "# 扫描范围（可以根据你的网格点范围调整）\n",
    "logMh_grid = np.linspace(12.0, 13.5, 20)\n",
    "logMstar_grid = np.linspace(11.2, 11.6, 20)\n",
    "logRe_grid = np.linspace(0.5, 0.9, 20)\n",
    "beta_grid = np.linspace(0.01, 0.5, 20)\n",
    "\n",
    "# === 2) 定义一个包装函数直接调用你的 lens solver ===\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "\n",
    "def compute_mu(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=1.0, zs=4.0)\n",
    "    props = lens_properties(model, beta)\n",
    "    return props['magnificationA'], props['magnificationB']\n",
    "\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# === 先生成全四维网格 ===\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 先生成全四维网格 ===\n",
    "mu1_grid = np.empty((len(logMh_grid), len(logMstar_grid), len(logRe_grid), len(beta_grid)))\n",
    "mu2_grid = np.empty_like(mu1_grid)\n",
    "\n",
    "for i, mh in enumerate(tqdm(logMh_grid, desc=\"Building 4D grid (logMh)\")):\n",
    "    for j, ms in enumerate(logMstar_grid):\n",
    "        for k, re in enumerate(logRe_grid):\n",
    "            for l, b in enumerate(beta_grid):\n",
    "                mu1, mu2 = compute_mu(mh, ms, re, b)\n",
    "                mu1_grid[i, j, k, l] = mu1\n",
    "                mu2_grid[i, j, k, l] = mu2\n",
    "\n",
    "\n",
    "# === 构造插值器 ===\n",
    "mu1_interp = RegularGridInterpolator(\n",
    "    (logMh_grid, logMstar_grid, logRe_grid, beta_grid),\n",
    "    mu1_grid,\n",
    "    bounds_error=False,\n",
    "    fill_value=None\n",
    ")\n",
    "\n",
    "mu2_interp = RegularGridInterpolator(\n",
    "    (logMh_grid, logMstar_grid, logRe_grid, beta_grid),\n",
    "    mu2_grid,\n",
    "    bounds_error=False,\n",
    "    fill_value=None\n",
    ")\n",
    "\n",
    "\n",
    "# === 3) 逐维扫描 ===\n",
    "def scan_and_plot(param_name, scan_values):\n",
    "    mu1_list, mu2_list = [], []\n",
    "    for val in scan_values:\n",
    "        kwargs = dict(logMh=logMh0, logMstar=logMstar0, logRe=logRe0, beta=beta0)\n",
    "        kwargs[param_name] = val\n",
    "        mu1, mu2 = compute_mu(**kwargs)\n",
    "        mu1_list.append(mu1)\n",
    "        mu2_list.append(mu2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(scan_values, mu1_list, \"o-\", label=\"μ₁\")\n",
    "    plt.plot(scan_values, mu2_list, \"o-\", label=\"μ₂\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Magnification\")\n",
    "    plt.title(f\"μ₁, μ₂ vs {param_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# === 4) 分别测试四个方向 ===\n",
    "scan_and_plot(\"logMh\", logMh_grid)\n",
    "scan_and_plot(\"logMstar\", logMstar_grid)\n",
    "scan_and_plot(\"logRe\", logRe_grid)\n",
    "scan_and_plot(\"beta\", beta_grid)\n",
    "\n",
    "# === 5) 随机采样点，对比插值器和真实解 ===\n",
    "# 这里假设你已经有一个插值器 mu1_interp, mu2_interp\n",
    "n_test = 50\n",
    "rng = np.random.default_rng(42)\n",
    "test_points = np.column_stack([\n",
    "    rng.uniform(logMh_grid.min(), logMh_grid.max(), n_test),\n",
    "    rng.uniform(logMstar_grid.min(), logMstar_grid.max(), n_test),\n",
    "    rng.uniform(logRe_grid.min(), logRe_grid.max(), n_test),\n",
    "    rng.uniform(beta_grid.min(), beta_grid.max(), n_test),\n",
    "])\n",
    "\n",
    "residuals_mu1 = []\n",
    "residuals_mu2 = []\n",
    "\n",
    "for pt in test_points:\n",
    "    mu1_true, mu2_true = compute_mu(*pt)\n",
    "    mu1_interp_val = mu1_interp(pt)\n",
    "    mu2_interp_val = mu2_interp(pt)\n",
    "    residuals_mu1.append(mu1_interp_val - mu1_true)\n",
    "    residuals_mu2.append(mu2_interp_val - mu2_true)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(residuals_mu1, bins=20, alpha=0.5, label=\"μ₁ residuals\")\n",
    "plt.hist(residuals_mu2, bins=20, alpha=0.5, label=\"μ₂ residuals\")\n",
    "plt.axvline(0, color=\"k\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Interp - True\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"插值器误差分布\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a70489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Scan curves (μ1, μ2 vs one parameter at a time) ---\n",
    "def scan_curves(param_name, scan_values):\n",
    "    mu1_list, mu2_list = [], []\n",
    "    for val in scan_values:\n",
    "        kwargs = dict(logMh=logMh0, logMstar=logMstar0, logRe=logRe0, beta=beta0)\n",
    "        kwargs[param_name] = val\n",
    "        mu1, mu2 = compute_mu(**kwargs)\n",
    "        mu1_list.append(mu1)\n",
    "        mu2_list.append(mu2)\n",
    "    return np.asarray(scan_values), np.asarray(mu1_list), np.asarray(mu2_list)\n",
    "\n",
    "x_mh,  y1_mh,  y2_mh  = scan_curves(\"logMh\",    logMh_grid)\n",
    "x_ms,  y1_ms,  y2_ms  = scan_curves(\"logMstar\", logMstar_grid)\n",
    "x_re,  y1_re,  y2_re  = scan_curves(\"logRe\",    logRe_grid)\n",
    "x_b,   y1_b,   y2_b   = scan_curves(\"beta\",     beta_grid)\n",
    "\n",
    "# --- 2) Off-grid residuals (Interp - True) ---\n",
    "rng = np.random.default_rng(42)\n",
    "n_test = 50\n",
    "test_points = np.column_stack([\n",
    "    rng.uniform(logMh_grid.min(),     logMh_grid.max(),     n_test),\n",
    "    rng.uniform(logMstar_grid.min(),  logMstar_grid.max(),  n_test),\n",
    "    rng.uniform(logRe_grid.min(),     logRe_grid.max(),     n_test),\n",
    "    rng.uniform(beta_grid.min(),      beta_grid.max(),      n_test),\n",
    "])\n",
    "\n",
    "residuals_mu1, residuals_mu2 = [], []\n",
    "for pt in test_points:\n",
    "    mu1_true, mu2_true = compute_mu(*pt)\n",
    "    mu1_pred = float(mu1_interp(pt))\n",
    "    mu2_pred = float(mu2_interp(pt))\n",
    "    residuals_mu1.append(mu1_pred - mu1_true)\n",
    "    residuals_mu2.append(mu2_pred - mu2_true)\n",
    "\n",
    "residuals_mu1 = np.asarray(residuals_mu1)\n",
    "residuals_mu2 = np.asarray(residuals_mu2)\n",
    "\n",
    "# --- 3) One figure: 2x3 grid of subplots ---\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "gs = fig.add_gridspec(2, 3, wspace=0.28, hspace=0.32)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "# Top row: scans of μ1/μ2 vs parameters\n",
    "def plot_scan(ax, x, y1, y2, xlabel):\n",
    "    ax.plot(x, y1, \"o-\", label=\"μ₁\")\n",
    "    ax.plot(x, y2, \"o-\", label=\"μ₂\")\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"Magnification\")\n",
    "    ax.set_title(f\"μ₁ & μ₂ vs {xlabel}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plot_scan(ax1, x_mh, y1_mh, y2_mh, \"log M_h\")\n",
    "plot_scan(ax2, x_ms, y1_ms, y2_ms, \"log M_*\")\n",
    "plot_scan(ax3, x_re, y1_re, y2_re, \"log R_e\")\n",
    "\n",
    "# Bottom-left: μ vs β\n",
    "plot_scan(ax4, x_b, y1_b, y2_b, \"β (unit)\")\n",
    "\n",
    "# Bottom-middle: residual histograms\n",
    "ax5.hist(residuals_mu1, bins=100, alpha=0.6, label=\"μ₁ residuals\")\n",
    "ax5.hist(residuals_mu2, bins=100, alpha=0.6, label=\"μ₂ residuals\")\n",
    "ax5.axvline(0.0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "ax5.set_xlabel(\"Residual (Interp − True)\")\n",
    "ax5.set_ylabel(\"Count\")\n",
    "ax5.set_title(\"Interpolation Residuals\")\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-right: residual scatter vs. true (optional diagnostic)\n",
    "ax6.scatter(np.abs(residuals_mu1), np.abs(residuals_mu2), s=28, alpha=0.8)\n",
    "ax6.set_xlabel(\"|μ₁ residual|\")\n",
    "ax6.set_ylabel(\"|μ₂ residual|\")\n",
    "ax6.set_title(\"Residual Magnitudes\")\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Magnification Scans and Interpolator Diagnostics\", fontsize=16, y=0.98)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02800bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from mock_generator.lens_model import LensModel\n",
    "# from mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "def scan_mu_vs_logMh(\n",
    "    logMh_min=12.0, logMh_max=13.5, n_scan=50,\n",
    "    logMstar=11.4, logRe=0.7, beta_unit=0.3,\n",
    "    zl=0.3, zs=2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    在均匀 logMh 网格上求解透镜方程并绘制 mu1, mu2, betamax.\n",
    "    \"\"\"\n",
    "    logMh_grid = np.linspace(logMh_min, logMh_max, n_scan)\n",
    "    mu1_list, mu2_list, betamax_list = [], [], []\n",
    "\n",
    "    for logMh in logMh_grid:\n",
    "        model = LensModel(logM_star=logMstar, logM_halo=logMh, logRe=logRe, zl=zl, zs=zs)\n",
    "        try:\n",
    "            xA, xB = solve_single_lens(model, beta_unit)\n",
    "            mu1_val = model.mu_from_rt(xA)\n",
    "            mu2_val = model.mu_from_rt(xB)\n",
    "            betamax_val = model.solve_ycaustic()\n",
    "        except Exception:\n",
    "            mu1_val = np.nan\n",
    "            mu2_val = np.nan\n",
    "            betamax_val = np.nan\n",
    "        mu1_list.append(mu1_val)\n",
    "        mu2_list.append(mu2_val)\n",
    "        betamax_list.append(betamax_val)\n",
    "\n",
    "    mu1_arr = np.array(mu1_list)\n",
    "    mu2_arr = np.array(mu2_list)\n",
    "    betamax_arr = np.array(betamax_list)\n",
    "\n",
    "    # === 画图 ===\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(6, 10), sharex=True, dpi=150)\n",
    "\n",
    "    axes[0].plot(logMh_grid, mu1_arr, 'o-', label=r'$\\mu_1$')\n",
    "    axes[0].set_ylabel(r'$\\mu_1$')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].plot(logMh_grid, mu2_arr, 'o-', label=r'$\\mu_2$', color='tab:red')\n",
    "    axes[1].set_ylabel(r'$\\mu_2$')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    axes[2].plot(logMh_grid, betamax_arr, 'o-', label=r'$\\beta_{\\max}$', color='tab:green')\n",
    "    axes[2].set_xlabel(r'$\\log M_h$')\n",
    "    axes[2].set_ylabel(r'$\\beta_{\\max}$')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"μ₁, μ₂, βmax vs logMh | logM*={logMstar:.2f}, logRe={logRe:.2f}, β={beta_unit:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return logMh_grid, mu1_arr, mu2_arr, betamax_arr\n",
    "\n",
    "# 直接调用测试\n",
    "logMh_grid, mu1_arr, mu2_arr, betamax_arr = scan_mu_vs_logMh(\n",
    "    logMh_min=12.0, logMh_max=13.5, n_scan=60,\n",
    "    logMstar=11.4, logRe=0.7, beta_unit=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61bd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fe2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test3.h5', 'r') as f:\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n",
    "\n",
    "# 构建插值器\n",
    "train_points = np.column_stack([logMh, logMstar, logRe, beta])\n",
    "print(np.shape(train_points))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a417a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(logMh, logMstar, s=1, alpha=0.3, c=betamax, cmap='viridis')\n",
    "# plt.colorbar(label='betamax')\n",
    "# plt.xlabel('logMh')\n",
    "# plt.ylabel('logMstar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3df5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interp_betamax = LinearNDInterp`olator(train_points, betamax, fill_value=np.nan)\n",
    "# interp_mu1 = LinearNDInterpolator(train_points, mu1, fill_value=np.nan)\n",
    "# interp_mu2 = LinearNDInterpolator(train_points, mu2, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 检查 train_points 是否包含 inf 或过大的值\n",
    "print(\"Checking for inf or NaN values in train_points:\")\n",
    "print(np.any(np.isinf(train_points)))  # 检查是否有 inf\n",
    "print(np.any(np.isnan(train_points)))  # 检查是否有 NaN\n",
    "\n",
    "# 检查 betamax, mu1, mu2 是否包含 inf 或过大的值\n",
    "print(\"Checking for inf or NaN values in betamax, mu1, mu2:\")\n",
    "print(np.any(np.isinf(betamax)))\n",
    "print(np.any(np.isnan(betamax)))\n",
    "print(np.any(np.isinf(mu1)))\n",
    "print(np.any(np.isnan(mu1)))\n",
    "print(np.any(np.isinf(mu2)))\n",
    "print(np.any(np.isnan(mu2)))\n",
    "\n",
    "# 检查是否有极端值\n",
    "print(\"Checking for extreme values (too large or too small):\")\n",
    "print(np.any(np.abs(train_points) > 1e10))  # 检查是否有过大的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换 inf 或过大的值\n",
    "train_points[np.isinf(train_points)] = np.nan  # 将 inf 替换为 NaN\n",
    "train_points[np.abs(train_points) > 1e10] = np.nan  # 将过大的值替换为 NaN\n",
    "\n",
    "# 对 betamax, mu1, mu2 也做相同处理\n",
    "betamax[np.isinf(betamax)] = np.nan\n",
    "mu1[np.isinf(mu1)] = np.nan\n",
    "mu2[np.isinf(mu2)] = np.nan\n",
    "\n",
    "# 重新填充 NaN 值\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_points_imputed = imputer.fit_transform(train_points)\n",
    "betamax_imputed = imputer.fit_transform(betamax.reshape(-1, 1))\n",
    "mu1_imputed = imputer.fit_transform(mu1.reshape(-1, 1))\n",
    "mu2_imputed = imputer.fit_transform(mu2.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新拟合 KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# 生成随机测试点\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 10000\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh), np.max(logMh), N_test),\n",
    "    rng.uniform(np.min(logMstar), np.max(logMstar), N_test),\n",
    "    rng.uniform(np.min(logRe), np.max(logRe), N_test),\n",
    "    rng.uniform(np.min(beta), np.max(beta), N_test),\n",
    "])\n",
    "knn_betamax = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_betamax.fit(train_points_imputed, betamax_imputed)\n",
    "\n",
    "knn_mu1 = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_mu1.fit(train_points_imputed, mu1_imputed)\n",
    "\n",
    "knn_mu2 = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_mu2.fit(train_points_imputed, mu2_imputed)\n",
    "\n",
    "# 计算插值结果\n",
    "interp_betamax_values = knn_betamax.predict(random_points)\n",
    "interp_mu1_values = knn_mu1.predict(random_points)\n",
    "interp_mu2_values = knn_mu2.predict(random_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb01d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c2e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476b2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相对误差分布\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    print(np.max(e))\n",
    "    ax.hist(e, bins=500, alpha=0.7)\n",
    "    ax.set_xlabel(\"Relative Error\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"{name} Relative Error Distribution | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test.h5', 'r') as f:\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n",
    "print (len(betamax))\n",
    "# 替换 inf 或过大的值\n",
    "train_points = np.column_stack([logMh, logMstar, logRe, beta])\n",
    "train_points[np.isinf(train_points)] = np.nan  # 将 inf 替换为 NaN\n",
    "train_points[np.abs(train_points) > 1e10] = np.nan  # 将过大的值替换为 NaN\n",
    "\n",
    "# 对 betamax, mu1, mu2 也做相同处理\n",
    "betamax[np.isinf(betamax)] = np.nan\n",
    "mu1[np.isinf(mu1)] = np.nan\n",
    "mu2[np.isinf(mu2)] = np.nan\n",
    "\n",
    "# 重新填充 NaN 值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_points_imputed = imputer.fit_transform(train_points)\n",
    "betamax_imputed = imputer.fit_transform(betamax.reshape(-1, 1))\n",
    "mu1_imputed = imputer.fit_transform(mu1.reshape(-1, 1))\n",
    "mu2_imputed = imputer.fit_transform(mu2.reshape(-1, 1))\n",
    "\n",
    "# ------------------------- 插值器构建 ----------------------------\n",
    "# 构建 LinearNDInterpolator 插值器\n",
    "interp_betamax = LinearNDInterpolator(train_points_imputed, betamax_imputed)\n",
    "interp_mu1 = LinearNDInterpolator(train_points_imputed, mu1_imputed)\n",
    "interp_mu2 = LinearNDInterpolator(train_points_imputed, mu2_imputed)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成随机测试点\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 10000\n",
    "\n",
    "logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "beta_min, beta_max, beta_delta = np.min(beta), np.max(beta),np.ptp(beta)\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh)+0.1*logMh_delta, np.max(logMh)-0.1*logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar)+0.1*logMstar_delta, np.max(logMstar)-0.1*logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe)+0.1*logRe_delta, np.max(logRe)-0.1*logRe_delta, N_test),\n",
    "    rng.uniform(np.min(beta)+0.1*beta_delta, np.max(beta)-0.1*beta_delta, N_test),   \n",
    "])\n",
    "\n",
    "# ------------------------- 计算插值和数值解 ----------------------------\n",
    "# 用插值器计算每个随机点的插值结果\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "end_time = time.time()\n",
    "print(f\"Interpolation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 计算数值求解的真实值\n",
    "def compute_mu_betamax(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=0.3, zs=2)\n",
    "    props = lens_properties(model, beta)\n",
    "    betamax = model.solve_ycaustic()\n",
    "    mu1 = float(props['magnificationA'])\n",
    "    mu2 = float(props['magnificationB'])\n",
    "    return betamax, mu1, mu2\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "betamax_true = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "    betamax_true.append(betamax)\n",
    "\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "betamax_true = np.array(betamax_true)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"True value computation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ------------------------- 对比插值结果与真值 ----------------------------\n",
    "# 计算相对误差\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "methods = {\n",
    "    \"betamax\": (interp_betamax_values.flatten(), betamax_true, rel_err(interp_betamax_values.flatten(), betamax_true)[0]),\n",
    "    \"mu1\": (interp_mu1_values.flatten(), mu1_true, rel_err(interp_mu1_values.flatten(), mu1_true)[0]),\n",
    "    \"mu2\": (interp_mu2_values.flatten(), mu2_true, rel_err(interp_mu2_values.flatten(), mu2_true)[0]),\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b5362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883716bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    ax.plot([0, np.nanmax(t)], [0, np.nanmax(t)], 'r--', lw=1)\n",
    "    # ax.set_xlim(0, np.nanmax(t))\n",
    "    # ax.set_ylim(0, np.nanmax(t))\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(0, 80.0)\n",
    "        ax.set_ylim(0, 80.0)\n",
    "    else:\n",
    "        MU_CAP = 20.0\n",
    "        ax.set_xlim(0, MU_CAP)\n",
    "        ax.set_ylim(0, MU_CAP)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8570bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 计算 MAE, MSE, RMSE 和 R²\n",
    "def compute_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    mse = mean_squared_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# 计算 betamax、mu1 和 mu2 的精度\n",
    "mae_betamax, mse_betamax, rmse_betamax, r2_betamax = compute_metrics(betamax_true, interp_betamax_values)\n",
    "mae_mu1, mse_mu1, rmse_mu1, r2_mu1 = compute_metrics(mu1_true, interp_mu1_values)\n",
    "mae_mu2, mse_mu2, rmse_mu2, r2_mu2 = compute_metrics(mu2_true, interp_mu2_values)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"betamax: MAE={mae_betamax:.4f}, MSE={mse_betamax:.4f}, RMSE={rmse_betamax:.4f}, R²={r2_betamax:.4f}\")\n",
    "print(f\"mu1: MAE={mae_mu1:.4f}, MSE={mse_mu1:.4f}, RMSE={rmse_mu1:.4f}, R²={r2_mu1:.4f}\")\n",
    "print(f\"mu2: MAE={mae_mu2:.4f}, MSE={mse_mu2:.4f}, RMSE={rmse_mu2:.4f}, R²={r2_mu2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e519d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test.h5', 'r') as f:\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n",
    "print (len(betamax))\n",
    "# 替换 inf 或过大的值\n",
    "train_points = np.column_stack([logMh, logMstar, logRe, beta])\n",
    "train_points[np.isinf(train_points)] = np.nan  # 将 inf 替换为 NaN\n",
    "train_points[np.abs(train_points) > 1e10] = np.nan  # 将过大的值替换为 NaN\n",
    "\n",
    "# 对 betamax, mu1, mu2 也做相同处理\n",
    "betamax[np.isinf(betamax)] = np.nan\n",
    "mu1[np.isinf(mu1)] = np.nan\n",
    "mu2[np.isinf(mu2)] = np.nan\n",
    "\n",
    "# 重新填充 NaN 值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_points_imputed = imputer.fit_transform(train_points)\n",
    "betamax_imputed = imputer.fit_transform(betamax.reshape(-1, 1))\n",
    "mu1_imputed = imputer.fit_transform(mu1.reshape(-1, 1))\n",
    "mu2_imputed = imputer.fit_transform(mu2.reshape(-1, 1))\n",
    "\n",
    "# ------------------------- 插值器构建 ----------------------------\n",
    "# 构建 LinearNDInterpolator 插值器\n",
    "interp_betamax = LinearNDInterpolator(train_points_imputed, betamax_imputed)\n",
    "interp_mu1 = LinearNDInterpolator(train_points_imputed, mu1_imputed)\n",
    "interp_mu2 = LinearNDInterpolator(train_points_imputed, mu2_imputed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 生成随机测试点\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 10000\n",
    "\n",
    "logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "beta_min, beta_max, beta_delta = np.min(beta), np.max(beta),np.ptp(beta)\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh)+0.1*logMh_delta, np.max(logMh)-0.1*logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar)+0.1*logMstar_delta, np.max(logMstar)-0.1*logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe)+0.1*logRe_delta, np.max(logRe)-0.1*logRe_delta, N_test),\n",
    "    rng.uniform(np.min(beta)+0.1*beta_delta, np.max(beta)-0.1*beta_delta, N_test),   \n",
    "])\n",
    "\n",
    "# ------------------------- 计算插值和数值解 ----------------------------\n",
    "# 用插值器计算每个随机点的插值结果\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "end_time = time.time()\n",
    "print(f\"Interpolation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 计算数值求解的真实值\n",
    "def compute_mu_betamax(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=0.3, zs=2)\n",
    "    props = lens_properties(model, beta)\n",
    "    betamax = model.solve_ycaustic()\n",
    "    mu1 = float(props['magnificationA'])\n",
    "    mu2 = float(props['magnificationB'])\n",
    "    return betamax, mu1, mu2\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "betamax_true = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "    betamax_true.append(betamax)\n",
    "\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "betamax_true = np.array(betamax_true)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"True value computation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ------------------------- 对比插值结果与真值 ----------------------------\n",
    "# 计算相对误差\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "methods = {\n",
    "    \"betamax\": (interp_betamax_values.flatten(), betamax_true, rel_err(interp_betamax_values.flatten(), betamax_true)[0]),\n",
    "    \"mu1\": (interp_mu1_values.flatten(), mu1_true, rel_err(interp_mu1_values.flatten(), mu1_true)[0]),\n",
    "    \"mu2\": (interp_mu2_values.flatten(), mu2_true, rel_err(interp_mu2_values.flatten(), mu2_true)[0]),\n",
    "}   \n",
    "\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    ax.plot([0, np.nanmax(t)], [0, np.nanmax(t)], 'r--', lw=1)\n",
    "    # ax.set_xlim(0, np.nanmax(t))\n",
    "    # ax.set_ylim(0, np.nanmax(t))\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(0, 80.0)\n",
    "        ax.set_ylim(0, 80.0)\n",
    "    else:\n",
    "        MU_CAP = 20.0\n",
    "        ax.set_xlim(0, MU_CAP)\n",
    "        ax.set_ylim(0, MU_CAP)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719d22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "\n",
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test.h5', 'r') as f:\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n",
    "print(len(betamax))\n",
    "\n",
    "# 替换 inf 或过大的值\n",
    "train_points = np.column_stack([logMh, logMstar, logRe, beta])\n",
    "train_points[np.isinf(train_points)] = np.nan  # 将 inf 替换为 NaN\n",
    "train_points[np.abs(train_points) > 1e10] = np.nan  # 将过大的值替换为 NaN\n",
    "\n",
    "# 对 betamax, mu1, mu2 也做相同处理\n",
    "betamax[np.isinf(betamax)] = np.nan\n",
    "mu1[np.isinf(mu1)] = np.nan\n",
    "mu2[np.isinf(mu2)] = np.nan\n",
    "\n",
    "# 重新填充 NaN 值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_points_imputed = imputer.fit_transform(train_points)\n",
    "betamax_imputed = imputer.fit_transform(betamax.reshape(-1, 1))\n",
    "mu1_imputed = imputer.fit_transform(mu1.reshape(-1, 1))\n",
    "mu2_imputed = imputer.fit_transform(mu2.reshape(-1, 1))\n",
    "\n",
    "# ------------------------- 使用 KDTree 加速插值 ----------------------------\n",
    "# 使用 KDTree 加速最近邻查找\n",
    "tree = KDTree(train_points_imputed)\n",
    "\n",
    "# ------------------------- 生成随机测试点 ----------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 1000\n",
    "\n",
    "logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "beta_min, beta_max, beta_delta = np.min(beta), np.max(beta), np.ptp(beta)\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh) + 0.1 * logMh_delta, np.max(logMh) - 0.1 * logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar) + 0.1 * logMstar_delta, np.max(logMstar) - 0.1 * logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe) + 0.1 * logRe_delta, np.max(logRe) - 0.1 * logRe_delta, N_test),\n",
    "    rng.uniform(np.min(beta) + 0.1 * beta_delta, np.max(beta) - 0.1 * beta_delta, N_test),   \n",
    "])\n",
    "\n",
    "# ------------------------- 计算插值和数值解 ----------------------------\n",
    "# 用 KDTree 查找最接近的点\n",
    "\n",
    "# ------------------------- 修改邻居数量 ----------------------------\n",
    "k_neighbors = 50  # 增加邻居数量来平滑插值\n",
    "\n",
    "# 查询所有的随机点，查找最近的k个训练点\n",
    "dist, ind = tree.query(random_points, k=k_neighbors)\n",
    "\n",
    "# 根据找到的邻居索引获取插值结果\n",
    "interp_betamax_values = np.mean(betamax_imputed[ind], axis=1)  # 对邻居的结果取平均\n",
    "interp_mu1_values = np.mean(mu1_imputed[ind], axis=1)\n",
    "interp_mu2_values = np.mean(mu2_imputed[ind], axis=1)\n",
    "\n",
    "# 计算插值计算时间\n",
    "start_time = time.time()\n",
    "print(f\"Interpolation of {N_test} points with k={k_neighbors} neighbors took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# 查询所有的随机点，查找最近的训练点\n",
    "dist, ind = tree.query(random_points, k=1)\n",
    "\n",
    "# 根据找到的索引获取插值结果\n",
    "interp_betamax_values = betamax_imputed[ind.flatten()]\n",
    "interp_mu1_values = mu1_imputed[ind.flatten()]\n",
    "interp_mu2_values = mu2_imputed[ind.flatten()]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Interpolation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 计算数值求解的真实值\n",
    "def compute_mu_betamax(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=0.3, zs=2)\n",
    "    props = lens_properties(model, beta)\n",
    "    betamax = model.solve_ycaustic()\n",
    "    mu1 = float(props['magnificationA'])\n",
    "    mu2 = float(props['magnificationB'])\n",
    "    return betamax, mu1, mu2\n",
    "\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "betamax_true = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "    betamax_true.append(betamax)\n",
    "\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "betamax_true = np.array(betamax_true)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"True value computation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ------------------------- 对比插值结果与真值 ----------------------------\n",
    "# 计算相对误差\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "methods = {\n",
    "    \"betamax\": (interp_betamax_values.flatten(), betamax_true, rel_err(interp_betamax_values.flatten(), betamax_true)[0]),\n",
    "    \"mu1\": (interp_mu1_values.flatten(), mu1_true, rel_err(interp_mu1_values.flatten(), mu1_true)[0]),\n",
    "    \"mu2\": (interp_mu2_values.flatten(), mu2_true, rel_err(interp_mu2_values.flatten(), mu2_true)[0]),\n",
    "}\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    ax.plot([0, np.nanmax(t)], [0, np.nanmax(t)], 'r--', lw=1)\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(0, 80.0)\n",
    "        ax.set_ylim(0, 80.0)\n",
    "    else:\n",
    "        MU_CAP = 20.0\n",
    "        ax.set_xlim(0, MU_CAP)\n",
    "        ax.set_ylim(0, MU_CAP)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55402bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a23feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf\n",
    "\n",
    "# ------------------------- 使用 RBF 插值 ----------------------------\n",
    "\n",
    "# 使用径向基函数插值\n",
    "rbf_betamax = Rbf(*train_points_imputed.T, betamax_imputed.flatten(), function='multiquadric', epsilon=1)\n",
    "interp_betamax_values = rbf_betamax(*random_points.T)\n",
    "\n",
    "rbf_mu1 = Rbf(*train_points_imputed.T, mu1_imputed.flatten(), function='multiquadric', epsilon=1)\n",
    "interp_mu1_values = rbf_mu1(*random_points.T)\n",
    "\n",
    "rbf_mu2 = Rbf(*train_points_imputed.T, mu2_imputed.flatten(), function='multiquadric', epsilon=1)\n",
    "interp_mu2_values = rbf_mu2(*random_points.T)\n",
    "\n",
    "# ------------------------- 计算插值计算时间 ----------------------------\n",
    "start_time = time.time()\n",
    "print(f\"Interpolation of {N_test} points using RBF took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# ------------------------- 对比插值结果与真值 ----------------------------\n",
    "\n",
    "# 计算相对误差\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "methods = {\n",
    "    \"betamax\": (interp_betamax_values.flatten(), betamax_true, rel_err(interp_betamax_values.flatten(), betamax_true)[0]),\n",
    "    \"mu1\": (interp_mu1_values.flatten(), mu1_true, rel_err(interp_mu1_values.flatten(), mu1_true)[0]),\n",
    "    \"mu2\": (interp_mu2_values.flatten(), mu2_true, rel_err(interp_mu2_values.flatten(), mu2_true)[0]),\n",
    "}\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    ax.plot([0, np.nanmax(t)], [0, np.nanmax(t)], 'r--', lw=1)\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(0, 80.0)\n",
    "        ax.set_ylim(0, 80.0)\n",
    "    else:\n",
    "        MU_CAP = 20.0\n",
    "        ax.set_xlim(0, MU_CAP)\n",
    "        ax.set_ylim(0, MU_CAP)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd2d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915d453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6197ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data range:\")\n",
    "print(\"logMh:\", np.min(logMh), np.max(logMh))\n",
    "print(\"logMstar:\", np.min(logMstar), np.max(logMstar))\n",
    "print(\"logRe:\", np.min(logRe), np.max(logRe))\n",
    "print(\"beta:\", np.min(beta), np.max(beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802a6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99655818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test3.h5', 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['base'].keys())\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd56caf",
   "metadata": {},
   "source": [
    "# good regular interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test3.h5', 'r') as f:\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n",
    "print(len(betamax))\n",
    "\n",
    "# 替换 inf 或过大的值\n",
    "train_points = np.column_stack([logMh, logMstar, logRe, beta])\n",
    "train_points[np.isinf(train_points)] = np.nan  # 将 inf 替换为 NaN\n",
    "train_points[np.abs(train_points) > 1e10] = np.nan  # 将过大的值替换为 NaN\n",
    "\n",
    "# 对 betamax, mu1, mu2 也做相同处理\n",
    "betamax[np.isinf(betamax)] = np.nan\n",
    "mu1[np.isinf(mu1)] = np.nan\n",
    "mu2[np.isinf(mu2)] = np.nan\n",
    "\n",
    "# 重新填充 NaN 值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_points_imputed = imputer.fit_transform(train_points)\n",
    "betamax_imputed = imputer.fit_transform(betamax.reshape(-1, 1))\n",
    "mu1_imputed = imputer.fit_transform(mu1.reshape(-1, 1))\n",
    "mu2_imputed = imputer.fit_transform(mu2.reshape(-1, 1))\n",
    "\n",
    "# ------------------------- 使用 RegularGridInterpolator 插值 ----------------------------\n",
    "# 获取每个维度的唯一值\n",
    "logMh_unique = np.unique(logMh)\n",
    "logMstar_unique = np.unique(logMstar)\n",
    "logRe_unique = np.unique(logRe)\n",
    "beta_unique = np.unique(beta)\n",
    "\n",
    "# 将 betamax, mu1, mu2 重塑为与 grid 维度一致的 4D 数组\n",
    "betamax_imputed_4d = betamax_imputed.reshape(len(logMh_unique), len(logMstar_unique), len(logRe_unique), len(beta_unique))\n",
    "mu1_imputed_4d = mu1_imputed.reshape(len(logMh_unique), len(logMstar_unique), len(logRe_unique), len(beta_unique))\n",
    "mu2_imputed_4d = mu2_imputed.reshape(len(logMh_unique), len(logMstar_unique), len(logRe_unique), len(beta_unique))\n",
    "\n",
    "# 创建 RegularGridInterpolator 插值器\n",
    "interp_betamax = RegularGridInterpolator((logMh_unique, logMstar_unique, logRe_unique, beta_unique), betamax_imputed_4d)\n",
    "interp_mu1 = RegularGridInterpolator((logMh_unique, logMstar_unique, logRe_unique, beta_unique), mu1_imputed_4d)\n",
    "interp_mu2 = RegularGridInterpolator((logMh_unique, logMstar_unique, logRe_unique, beta_unique), mu2_imputed_4d)\n",
    "\n",
    "# ------------------------- 生成随机测试点 ----------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 1000\n",
    "\n",
    "logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "beta_min, beta_max, beta_delta = np.min(beta), np.max(beta), np.ptp(beta)\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh) + 0.1 * logMh_delta, np.max(logMh) - 0.1 * logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar) + 0.1 * logMstar_delta, np.max(logMstar) - 0.1 * logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe) + 0.1 * logRe_delta, np.max(logRe) - 0.1 * logRe_delta, N_test),\n",
    "    rng.uniform(np.min(beta) + 0.1 * beta_delta, np.max(beta) - 0.1 * beta_delta, N_test),\n",
    "])\n",
    "\n",
    "# ------------------------- 计算插值和数值解 ----------------------------\n",
    "# 用 RegularGridInterpolator 计算每个随机点的插值结果\n",
    "start_time = time.time()\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "end_time = time.time()\n",
    "print(f\"Interpolation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 计算数值求解的真实值\n",
    "def compute_mu_betamax(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=0.3, zs=2)\n",
    "    props = lens_properties(model, beta)\n",
    "    betamax = model.solve_ycaustic()\n",
    "    mu1 = float(props['magnificationA'])\n",
    "    mu2 = float(props['magnificationB'])\n",
    "    return betamax, mu1, mu2\n",
    "\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "betamax_true = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "    betamax_true.append(betamax)\n",
    "\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "betamax_true = np.array(betamax_true)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"True value computation of {N_test} points took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ------------------------- 对比插值结果与真值 ----------------------------\n",
    "# 计算相对误差\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "methods = {\n",
    "    \"betamax\": (interp_betamax_values.flatten(), betamax_true, rel_err(interp_betamax_values.flatten(), betamax_true)[0]),\n",
    "    \"mu1\": (interp_mu1_values.flatten(), mu1_true, rel_err(interp_mu1_values.flatten(), mu1_true)[0]),\n",
    "    \"mu2\": (interp_mu2_values.flatten(), mu2_true, rel_err(interp_mu2_values.flatten(), mu2_true)[0]),\n",
    "}\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "for ax, (name, (p, t, e)) in zip(axes, methods.items()):\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    ax.plot([0, np.nanmax(t)], [0, np.nanmax(t)], 'r--', lw=1)\n",
    "    if name == \"betamax\":\n",
    "        ax.set_xlim(0, 80.0)\n",
    "        ax.set_ylim(0, 80.0)\n",
    "    else:\n",
    "        MU_CAP = 20.0\n",
    "        ax.set_xlim(0, MU_CAP)\n",
    "        ax.set_ylim(0, MU_CAP)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\")\n",
    "    ax.set_title(f\"{name} | mean={e.mean():.2e}, p95={np.percentile(e,95):.2e}, max={e.max():.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设以下是插值结果和真实值（你可以用你的插值结果替代这些变量）\n",
    "pred_betamax = interp_betamax_values.flatten()\n",
    "true_betamax = betamax_true\n",
    "pred_mu1 = interp_mu1_values.flatten()\n",
    "true_mu1 = mu1_true\n",
    "pred_mu2 = interp_mu2_values.flatten()\n",
    "true_mu2 = mu2_true\n",
    "\n",
    "# 计算最大绝对误差\n",
    "max_error_betamax = np.max(np.abs(pred_betamax - true_betamax))\n",
    "max_error_mu1 = np.max(np.abs(pred_mu1 - true_mu1))\n",
    "max_error_mu2 = np.max(np.abs(pred_mu2 - true_mu2))\n",
    "\n",
    "print(f\"Max absolute error in betamax: {max_error_betamax}\")\n",
    "print(f\"Max absolute error in mu1: {max_error_mu1}\")\n",
    "print(f\"Max absolute error in mu2: {max_error_mu2}\")\n",
    "\n",
    "# 计算误差并选择误差大的点\n",
    "error_betamax = np.abs(pred_betamax - true_betamax)\n",
    "error_mu1 = np.abs(pred_mu1 - true_mu1)\n",
    "error_mu2 = np.abs(pred_mu2 - true_mu2)\n",
    "\n",
    "# 假设我们选择误差大于某个阈值的点\n",
    "threshold = 0.2  # 你可以选择不同的阈值\n",
    "big_error_indices_betamax = np.where(error_betamax > threshold)[0]\n",
    "big_error_indices_mu1 = np.where(error_mu1 > threshold)[0]\n",
    "big_error_indices_mu2 = np.where(error_mu2 > threshold)[0]\n",
    "\n",
    "# 画出大误差点的位置\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "\n",
    "# betamax\n",
    "axes[0].scatter(true_betamax, pred_betamax, s=10, alpha=0.35)\n",
    "axes[0].scatter(true_betamax[big_error_indices_betamax], pred_betamax[big_error_indices_betamax], color='red', label='Big error points')\n",
    "axes[0].plot([0, np.nanmax(true_betamax)], [0, np.nanmax(pred_betamax)], 'r--', lw=1)\n",
    "axes[0].set_xlabel(\"True betamax\")\n",
    "axes[0].set_ylabel(\"Predicted betamax\")\n",
    "axes[0].set_title(f\"betamax | Max Error = {max_error_betamax:.2e}\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# mu1\n",
    "axes[1].scatter(true_mu1, pred_mu1, s=10, alpha=0.35)\n",
    "axes[1].scatter(true_mu1[big_error_indices_mu1], pred_mu1[big_error_indices_mu1], color='red', label='Big error points')\n",
    "axes[1].plot([0, np.nanmax(true_mu1)], [0, np.nanmax(pred_mu1)], 'r--', lw=1)\n",
    "axes[1].set_xlabel(\"True mu1\")\n",
    "axes[1].set_ylabel(\"Predicted mu1\")\n",
    "axes[1].set_title(f\"mu1 | Max Error = {max_error_mu1:.2e}\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# mu2\n",
    "axes[2].scatter(true_mu2, pred_mu2, s=10, alpha=0.35)\n",
    "axes[2].scatter(true_mu2[big_error_indices_mu2], pred_mu2[big_error_indices_mu2], color='red', label='Big error points')\n",
    "axes[2].plot([0, np.nanmax(true_mu2)], [0, np.nanmax(pred_mu2)], 'r--', lw=1)\n",
    "axes[2].set_xlabel(\"True mu2\")\n",
    "axes[2].set_ylabel(\"Predicted mu2\")\n",
    "axes[2].set_title(f\"mu2 | Max Error = {max_error_mu2:.2e}\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设以下是插值结果和真实值（你可以用你的插值结果替代这些变量）\n",
    "pred_betamax = interp_betamax_values.flatten()\n",
    "true_betamax = betamax_true\n",
    "pred_mu1 = interp_mu1_values.flatten()\n",
    "true_mu1 = mu1_true\n",
    "pred_mu2 = interp_mu2_values.flatten()\n",
    "true_mu2 = mu2_true\n",
    "\n",
    "# 计算相对误差\n",
    "error_betamax = np.abs(pred_betamax - true_betamax)\n",
    "error_mu1 = np.abs(pred_mu1 - true_mu1)\n",
    "error_mu2 = np.abs(pred_mu2 - true_mu2)\n",
    "\n",
    "# 假设我们选择误差大于某个阈值的点\n",
    "threshold = 0.2  # 你可以选择不同的阈值\n",
    "\n",
    "# 将误差大的点和小的点分开\n",
    "big_error_indices_betamax = np.where(error_betamax > threshold)[0]\n",
    "small_error_indices_betamax = np.where(error_betamax <= threshold)[0]\n",
    "\n",
    "big_error_indices_mu1 = np.where(error_mu1 > threshold)[0]\n",
    "small_error_indices_mu1 = np.where(error_mu1 <= threshold)[0]\n",
    "\n",
    "big_error_indices_mu2 = np.where(error_mu2 > threshold)[0]\n",
    "small_error_indices_mu2 = np.where(error_mu2 <= threshold)[0]\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=150)\n",
    "\n",
    "# betamax\n",
    "axes[0].scatter(true_betamax[small_error_indices_betamax], pred_betamax[small_error_indices_betamax], s=10, alpha=0.35, label=\"Small error points\")\n",
    "axes[0].scatter(true_betamax[big_error_indices_betamax], pred_betamax[big_error_indices_betamax], s=10, color='red', alpha=0.5, label=\"Big error points\")\n",
    "axes[0].plot([0, np.nanmax(true_betamax)], [0, np.nanmax(pred_betamax)], 'r--', lw=1)\n",
    "axes[0].set_xlabel(\"True betamax\")\n",
    "axes[0].set_ylabel(\"Predicted betamax\")\n",
    "axes[0].set_title(f\"betamax | Max Error = {max_error_betamax:.2e}\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# mu1\n",
    "axes[1].scatter(true_mu1[small_error_indices_mu1], pred_mu1[small_error_indices_mu1], s=10, alpha=0.35, label=\"Small error points\")\n",
    "axes[1].scatter(true_mu1[big_error_indices_mu1], pred_mu1[big_error_indices_mu1], s=10, color='red', alpha=0.5, label=\"Big error points\")\n",
    "axes[1].plot([0, np.nanmax(true_mu1)], [0, np.nanmax(pred_mu1)], 'r--', lw=1)\n",
    "axes[1].set_xlabel(\"True mu1\")\n",
    "axes[1].set_ylabel(\"Predicted mu1\")\n",
    "axes[1].set_title(f\"mu1 | Max Error = {max_error_mu1:.2e}\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# mu2\n",
    "axes[2].scatter(true_mu2[small_error_indices_mu2], pred_mu2[small_error_indices_mu2], s=10, alpha=0.35, label=\"Small error points\")\n",
    "axes[2].scatter(true_mu2[big_error_indices_mu2], pred_mu2[big_error_indices_mu2], s=10, color='red', alpha=0.5, label=\"Big error points\")\n",
    "axes[2].plot([0, np.nanmax(true_mu2)], [0, np.nanmax(pred_mu2)], 'r--', lw=1)\n",
    "axes[2].set_xlabel(\"True mu2\")\n",
    "axes[2].set_ylabel(\"Predicted mu2\")\n",
    "axes[2].set_title(f\"mu2 | Max Error = {max_error_mu2:.2e}\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b916a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 假设这些是你的真实值和插值结果\n",
    "true_betamax = betamax_true\n",
    "interp_betamax_values = interp_betamax_values.flatten()\n",
    "true_mu1 = mu1_true\n",
    "interp_mu1_values = interp_mu1_values.flatten()\n",
    "true_mu2 = mu2_true\n",
    "interp_mu2_values = interp_mu2_values.flatten()\n",
    "\n",
    "# 计算误差\n",
    "error_betamax = np.abs(true_betamax - interp_betamax_values)\n",
    "error_mu1 = np.abs(true_mu1 - interp_mu1_values)\n",
    "error_mu2 = np.abs(true_mu2 - interp_mu2_values)\n",
    "\n",
    "# 定义误差阈值\n",
    "threshold = 0.0012  # 可以调整阈值\n",
    "\n",
    "# 分类误差大的和小的点\n",
    "big_error_betamax = error_betamax > threshold\n",
    "big_error_mu1 = error_mu1 > threshold\n",
    "big_error_mu2 = error_mu2 > threshold\n",
    "\n",
    "# 将数据放入 DataFrame 中并标记大误差\n",
    "data = pd.DataFrame({\n",
    "    'betamax': true_betamax,\n",
    "    'mu1': true_mu1,\n",
    "    'mu2': true_mu2,\n",
    "    'error_betamax': error_betamax,\n",
    "    'error_mu1': error_mu1,\n",
    "    'error_mu2': error_mu2,\n",
    "    'big_error_betamax': big_error_betamax,\n",
    "    'big_error_mu1': big_error_mu1,\n",
    "    'big_error_mu2': big_error_mu2\n",
    "})\n",
    "\n",
    "# 使用 seaborn 画 pairplot\n",
    "sns.pairplot(data, hue=\"big_error_betamax\", vars=['betamax', 'mu1', 'mu2'], palette={True: 'red', False: 'blue'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c54e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成随机测试点\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 1000\n",
    "\n",
    "logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "beta_min, beta_max, beta_delta = np.min(beta), np.max(beta), np.ptp(beta)\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh) + 0.1 * logMh_delta, np.max(logMh) - 0.1 * logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar) + 0.1 * logMstar_delta, np.max(logMstar) - 0.1 * logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe) + 0.1 * logRe_delta, np.max(logRe) - 0.1 * logRe_delta, N_test),\n",
    "    rng.uniform(np.min(beta) + 0.1 * beta_delta, np.max(beta) - 0.1 * beta_delta, N_test),\n",
    "])\n",
    "\n",
    "# 使用插值器计算插值结果\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "\n",
    "# 计算真实值\n",
    "betamax_true = []\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    betamax_true.append(betamax)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "\n",
    "# 转换为 numpy 数组\n",
    "betamax_true = np.array(betamax_true)\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "\n",
    "# 计算误差\n",
    "error_betamax = np.abs(interp_betamax_values - betamax_true)\n",
    "error_mu1 = np.abs(interp_mu1_values - mu1_true)\n",
    "error_mu2 = np.abs(interp_mu2_values - mu2_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置阈值，选择高误差和低误差的点\n",
    "threshold = 0.002  # 误差阈值\n",
    "\n",
    "big_error_indices_betamax = np.where(error_betamax > threshold)[0]\n",
    "small_error_indices_betamax = np.where(error_betamax <= threshold)[0]\n",
    "\n",
    "big_error_indices_mu1 = np.where(error_mu1 > threshold)[0]\n",
    "small_error_indices_mu1 = np.where(error_mu1 <= threshold)[0]\n",
    "\n",
    "big_error_indices_mu2 = np.where(error_mu2 > threshold)[0]\n",
    "small_error_indices_mu2 = np.where(error_mu2 <= threshold)[0]\n",
    "\n",
    "# 创建 DataFrame，包含测试集参数和插值结果\n",
    "lens_df = pd.DataFrame(random_points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "lens_df[\"betamax\"] = interp_betamax_values\n",
    "lens_df[\"mu1\"] = interp_mu1_values\n",
    "lens_df[\"mu2\"] = interp_mu2_values\n",
    "\n",
    "# 高误差和低误差的 DataFrame\n",
    "\n",
    "betamax_high_error = lens_df.iloc[big_error_indices_betamax]\n",
    "betamax_low_error = lens_df.iloc[small_error_indices_betamax]\n",
    "\n",
    "mu1_high_error = lens_df.iloc[big_error_indices_mu1]\n",
    "mu1_low_error = lens_df.iloc[small_error_indices_mu1]\n",
    "\n",
    "mu2_high_error = lens_df.iloc[big_error_indices_mu2]\n",
    "mu2_low_error = lens_df.iloc[small_error_indices_mu2]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成随机测试点\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 1000\n",
    "\n",
    "# logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "# logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "# logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "# # beta_min, beta_max, beta_delta = np.min(beta), np.max(beta), np.ptp(beta)\n",
    "# beta_min, beta_max, beta_delta = 0.2, 0.95, 0\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh) + 0.1 * logMh_delta, np.max(logMh) - 0.1 * logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar) + 0.1 * logMstar_delta, np.max(logMstar) - 0.1 * logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe) + 0.1 * logRe_delta, np.max(logRe) - 0.1 * logRe_delta, N_test),\n",
    "    # rng.uniform(np.min(beta) + 0.1 * beta_delta, np.max(beta) - 0.1 * beta_delta, N_test),\n",
    "    rng.uniform(beta_min, beta_max, N_test),\n",
    "])\n",
    "\n",
    "\n",
    "# random_points = np.column_stack([\n",
    "#     rng.uniform(np.max(logMh), np.max(logMh) +logMh_delta, N_test),\n",
    "#     rng.uniform(np.max(logMstar), np.max(logMstar) + logMstar_delta, N_test),\n",
    "#     rng.uniform(np.max(logRe), np.max(logRe) + logRe_delta, N_test),\n",
    "#     rng.uniform(beta_min, beta_max, N_test),\n",
    "# ])\n",
    "\n",
    "# 使用插值器计算插值结果\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "\n",
    "# 计算真实值\n",
    "betamax_true = []\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    betamax_true.append(betamax)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "\n",
    "# 转换为 numpy 数组\n",
    "betamax_true = np.array(betamax_true)\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "\n",
    "# 计算误差\n",
    "error_betamax = np.abs(interp_betamax_values - betamax_true)\n",
    "error_mu1 = np.abs(interp_mu1_values - mu1_true)\n",
    "error_mu2 = np.abs(interp_mu2_values - mu2_true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7521d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算误差\n",
    "error_betamax = np.abs(interp_betamax_values - betamax_true)\n",
    "error_mu1 = np.abs(interp_mu1_values - mu1_true)\n",
    "error_mu2 = np.abs(interp_mu2_values - mu2_true)\n",
    "\n",
    "# 设置误差阈值，选择高误差和低误差的点\n",
    "threshold = 0.0013  # 误差阈值\n",
    "\n",
    "# 根据误差大小划分为两类：高误差和低误差\n",
    "high_error_betamax = error_betamax > threshold\n",
    "low_error_betamax = error_betamax <= threshold\n",
    "\n",
    "high_error_mu1 = error_mu1 > threshold\n",
    "low_error_mu1 = error_mu1 <= threshold\n",
    "\n",
    "high_error_mu2 = error_mu2 > threshold\n",
    "low_error_mu2 = error_mu2 <= threshold\n",
    "\n",
    "# 创建新的 DataFrame，添加误差标签（高误差/低误差）\n",
    "lens_df = pd.DataFrame(random_points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "\n",
    "# 添加 betamax, mu1, mu2 插值结果\n",
    "lens_df[\"betamax\"] = interp_betamax_values\n",
    "lens_df[\"mu1\"] = interp_mu1_values\n",
    "lens_df[\"mu2\"] = interp_mu2_values\n",
    "\n",
    "# 添加误差类别列\n",
    "lens_df[\"betamax_error_type\"] = np.where(high_error_betamax, \"High Error\", \"Low Error\")\n",
    "lens_df[\"mu1_error_type\"] = np.where(high_error_mu1, \"High Error\", \"Low Error\")\n",
    "lens_df[\"mu2_error_type\"] = np.where(high_error_mu2, \"High Error\", \"Low Error\")\n",
    "\n",
    "# 转换为分类变量\n",
    "lens_df['betamax_error_type'] = lens_df['betamax_error_type'].astype('category')\n",
    "lens_df['mu1_error_type'] = lens_df['mu1_error_type'].astype('category')\n",
    "lens_df['mu2_error_type'] = lens_df['mu2_error_type'].astype('category')\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Betamax\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"betamax\"],  # 明确指定要绘制的变量\n",
    "             hue=\"betamax_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Betamax Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Mu1\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu1\"],  # 明确指定要绘制的变量\n",
    "             hue=\"mu1_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Mu1 Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Mu2\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu2\"],  # 明确指定要绘制的变量\n",
    "             hue=\"mu2_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Mu2 Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81083775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c5357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01090e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置阈值，选择高误差和低误差的点\n",
    "threshold = 0.002  # 误差阈值\n",
    "\n",
    "big_error_indices_betamax = np.where(error_betamax > threshold)[0]\n",
    "small_error_indices_betamax = np.where(error_betamax <= threshold)[0]\n",
    "\n",
    "big_error_indices_mu1 = np.where(error_mu1 > threshold)[0]\n",
    "small_error_indices_mu1 = np.where(error_mu1 <= threshold)[0]\n",
    "\n",
    "big_error_indices_mu2 = np.where(error_mu2 > threshold)[0]\n",
    "small_error_indices_mu2 = np.where(error_mu2 <= threshold)[0]\n",
    "\n",
    "# 创建 DataFrame，包含测试集参数和插值结果\n",
    "lens_df = pd.DataFrame(random_points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "lens_df[\"betamax\"] = interp_betamax_values\n",
    "lens_df[\"mu1\"] = interp_mu1_values\n",
    "lens_df[\"mu2\"] = interp_mu2_values\n",
    "\n",
    "# 高误差和低误差的 DataFrame\n",
    "betamax_high_error = lens_df.iloc[big_error_indices_betamax]\n",
    "betamax_low_error = lens_df.iloc[small_error_indices_betamax]\n",
    "\n",
    "mu1_high_error = lens_df.iloc[big_error_indices_mu1]\n",
    "mu1_low_error = lens_df.iloc[small_error_indices_mu1]\n",
    "\n",
    "mu2_high_error = lens_df.iloc[big_error_indices_mu2]\n",
    "mu2_low_error = lens_df.iloc[small_error_indices_mu2]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成随机测试点\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 10000\n",
    "\n",
    "logMh_min, logMh_max, logMh_delta = np.min(logMh), np.max(logMh), np.ptp(logMh)\n",
    "logMstar_min, logMstar_max, logMstar_delta = np.min(logMstar), np.max(logMstar), np.ptp(logMstar)\n",
    "logRe_min, logRe_max, logRe_delta = np.min(logRe), np.max(logRe), np.ptp(logRe)\n",
    "# beta_min, beta_max, beta_delta = np.min(beta), np.max(beta), np.ptp(beta)\n",
    "beta_min, beta_max, beta_delta = 0.0, 0.1, 0\n",
    "\n",
    "random_points = np.column_stack([\n",
    "    rng.uniform(np.min(logMh) + 0.1 * logMh_delta, np.max(logMh) - 0.1 * logMh_delta, N_test),\n",
    "    rng.uniform(np.min(logMstar) + 0.1 * logMstar_delta, np.max(logMstar) - 0.1 * logMstar_delta, N_test),\n",
    "    rng.uniform(np.min(logRe) + 0.1 * logRe_delta, np.max(logRe) - 0.1 * logRe_delta, N_test),\n",
    "    # rng.uniform(np.min(beta) + 0.1 * beta_delta, np.max(beta) - 0.1 * beta_delta, N_test),\n",
    "    rng.uniform(beta_min, beta_max, N_test),\n",
    "])\n",
    "\n",
    "# 使用插值器计算插值结果\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "\n",
    "# 计算真实值\n",
    "betamax_true = []\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    betamax_true.append(betamax)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "\n",
    "# 转换为 numpy 数组\n",
    "betamax_true = np.array(betamax_true)\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "\n",
    "# 计算误差\n",
    "error_betamax = np.abs(interp_betamax_values - betamax_true)\n",
    "error_mu1 = np.abs(interp_mu1_values - mu1_true)\n",
    "error_mu2 = np.abs(interp_mu2_values - mu2_true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c8103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算误差\n",
    "error_betamax = np.abs(interp_betamax_values - betamax_true)\n",
    "error_mu1 = np.abs(interp_mu1_values - mu1_true)\n",
    "error_mu2 = np.abs(interp_mu2_values - mu2_true)\n",
    "\n",
    "# 设置误差阈值，选择高误差和低误差的点\n",
    "threshold = 0.002  # 误差阈值\n",
    "\n",
    "# 根据误差大小划分为两类：高误差和低误差\n",
    "high_error_betamax = error_betamax > threshold\n",
    "low_error_betamax = error_betamax <= threshold\n",
    "\n",
    "high_error_mu1 = error_mu1 > threshold\n",
    "low_error_mu1 = error_mu1 <= threshold\n",
    "\n",
    "high_error_mu2 = error_mu2 > threshold\n",
    "low_error_mu2 = error_mu2 <= threshold\n",
    "\n",
    "# 创建新的 DataFrame，添加误差标签（高误差/低误差）\n",
    "lens_df = pd.DataFrame(random_points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "\n",
    "# 添加 betamax, mu1, mu2 插值结果\n",
    "lens_df[\"betamax\"] = interp_betamax_values\n",
    "lens_df[\"mu1\"] = interp_mu1_values\n",
    "lens_df[\"mu2\"] = interp_mu2_values\n",
    "\n",
    "# 添加误差类别列\n",
    "lens_df[\"betamax_error_type\"] = np.where(high_error_betamax, \"High Error\", \"Low Error\")\n",
    "lens_df[\"mu1_error_type\"] = np.where(high_error_mu1, \"High Error\", \"Low Error\")\n",
    "lens_df[\"mu2_error_type\"] = np.where(high_error_mu2, \"High Error\", \"Low Error\")\n",
    "\n",
    "# 转换为分类变量\n",
    "lens_df['betamax_error_type'] = lens_df['betamax_error_type'].astype('category')\n",
    "lens_df['mu1_error_type'] = lens_df['mu1_error_type'].astype('category')\n",
    "lens_df['mu2_error_type'] = lens_df['mu2_error_type'].astype('category')\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Betamax\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"betamax\"],  # 明确指定要绘制的变量\n",
    "             hue=\"betamax_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Betamax Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Mu1\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu1\"],  # 明确指定要绘制的变量\n",
    "             hue=\"mu1_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Mu1 Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Mu2\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu2\"],  # 明确指定要绘制的变量\n",
    "             hue=\"mu2_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Mu2 Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f75d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351b89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6d2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b02fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1688806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511babb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算误差\n",
    "error_betamax = np.abs(interp_betamax_values - betamax_true)\n",
    "error_mu1 = np.abs(interp_mu1_values - mu1_true)\n",
    "error_mu2 = np.abs(interp_mu2_values - mu2_true)\n",
    "\n",
    "# 设置误差阈值，选择高误差和低误差的点\n",
    "threshold = 0.0018  # 误差阈值\n",
    "\n",
    "# 根据误差大小划分为两类：高误差和低误差\n",
    "high_error_betamax = error_betamax > threshold\n",
    "low_error_betamax = error_betamax <= threshold\n",
    "\n",
    "high_error_mu1 = error_mu1 > threshold\n",
    "low_error_mu1 = error_mu1 <= threshold\n",
    "\n",
    "high_error_mu2 = error_mu2 > threshold\n",
    "low_error_mu2 = error_mu2 <= threshold\n",
    "\n",
    "# 创建新的 DataFrame，添加误差标签（高误差/低误差）\n",
    "lens_df = pd.DataFrame(random_points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "\n",
    "# 添加 betamax, mu1, mu2 插值结果\n",
    "lens_df[\"betamax\"] = interp_betamax_values\n",
    "lens_df[\"mu1\"] = interp_mu1_values\n",
    "lens_df[\"mu2\"] = interp_mu2_values\n",
    "\n",
    "# 添加误差类别列\n",
    "lens_df[\"betamax_error_type\"] = np.where(high_error_betamax, \"High Error\", \"Low Error\")\n",
    "lens_df[\"mu1_error_type\"] = np.where(high_error_mu1, \"High Error\", \"Low Error\")\n",
    "lens_df[\"mu2_error_type\"] = np.where(high_error_mu2, \"High Error\", \"Low Error\")\n",
    "\n",
    "# 转换为分类变量\n",
    "lens_df['betamax_error_type'] = lens_df['betamax_error_type'].astype('category')\n",
    "lens_df['mu1_error_type'] = lens_df['mu1_error_type'].astype('category')\n",
    "lens_df['mu2_error_type'] = lens_df['mu2_error_type'].astype('category')\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Betamax\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"betamax\"],  # 明确指定要绘制的变量\n",
    "             hue=\"betamax_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Betamax Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Mu1\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu1\"],  # 明确指定要绘制的变量\n",
    "             hue=\"mu1_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Mu1 Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()\n",
    "\n",
    "# 绘制高误差和低误差的 pairplot：Mu2\n",
    "sns.pairplot(lens_df,\n",
    "             vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu2\"],  # 明确指定要绘制的变量\n",
    "             hue=\"mu2_error_type\",  # 用误差类型着色\n",
    "             palette={\"High Error\": \"red\", \"Low Error\": \"blue\"},  # 高误差为红色，低误差为蓝色\n",
    "             corner=True,  # 只显示下三角部分\n",
    "             plot_kws={\"alpha\": 0.7, \"s\": 10},  # 点的透明度和大小\n",
    "             diag_kind=\"kde\",  # 对角线使用核密度估计图\n",
    "             height=2.5,  # 设置每个子图的大小\n",
    "             markers=\"o\")  # 数据点使用圆形标记\n",
    "plt.suptitle(\"Mu2 Pairplot (High vs Low Error)\", y=1.02)  # 设置标题并调整位置\n",
    "plt.tight_layout()  # 调整布局，使标题不与图形重叠\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a0ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0b5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_df['betamax_error_type'] = lens_df['betamax_error_type'].fillna('Unknown')  # Or drop rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lens_df[[\"betamax_error_type\", \"logMh\", \"logMstar\", \"logRe\", \"beta\", \"betamax\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the 'mu1_error_type' column\n",
    "print(lens_df['mu1_error_type'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d7b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72f060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 项目上上级目录（Github）\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator import lens_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1715539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m sl_inference_1Dinfer_hdf5_fix_A_runonlinux.grid_generator \n",
    "# --axes logMh:12:13:0.05,logMstar:11.5:12:0.1,logRe:0.5:1.5:0.1,beta:0:1:0.1 \n",
    "# --out mock_test2.h5 --n-proc 8 --overwrite\n",
    "\n",
    "# python -m sl_inference_1Dinfer_hdf5_fix_A_runonlinux.grid_generator \n",
    "# --axes logMh:12.3:12.6:0.02,logMstar:11.5:11.6:0.02,logRe:0.5:0.7:0.02,beta:0:1:0.05 \n",
    "# --out mock_test3.h5 --n-proc 8 --overwrite\n",
    "\n",
    "print(1)\n",
    "# 生成随机测试点\n",
    "logMh_range = (12.3, 12.6)\n",
    "logMstar_range = (11.5, 11.6)\n",
    "logRe_range = (0.5, 0.7)\n",
    "beta_range = (0.01, 1.0)\n",
    "\n",
    "num_points = 100000\n",
    "random_logMh = np.random.uniform(logMh_range[0], logMh_range[1], num_points)\n",
    "random_logMstar = np.random.uniform(logMstar_range[0], logMstar_range[1], num_points)\n",
    "random_logRe = np.random.uniform(logRe_range[0], logRe_range[1], num_points)\n",
    "random_beta = np.random.uniform(beta_range[0], beta_range[1], num_points)\n",
    "\n",
    "# 将它们合并为测试点\n",
    "random_points = np.column_stack([random_logMh, random_logMstar, random_logRe, random_beta])\n",
    "\n",
    "# 数值求解函数（根据你的模型）\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_solver import solve_single_lens\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "\n",
    "def compute_mu_betamax(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=0.3, zs=2)\n",
    "    props = lens_properties(model, beta)\n",
    "    betamax = model.solve_ycaustic()\n",
    "    return betamax, props['magnificationA'], props['magnificationB']\n",
    "\n",
    "\n",
    "# 用插值器计算每个随机点的结果\n",
    "import time\n",
    "begin_time = time.time()\n",
    "interp_betamax_values = interp_betamax(random_points)\n",
    "interp_mu1_values = interp_mu1(random_points)\n",
    "interp_mu2_values = interp_mu2(random_points)\n",
    "end_time = time.time()  \n",
    "print(f\"[info] Interpolation time for {num_points} points: {end_time - begin_time:.2f} seconds\")\n",
    "\n",
    "# 数值求解并计算结果\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "betamax_true = []\n",
    "\n",
    "for pt in tqdm(random_points, desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "    betamax_true.append(betamax)\n",
    "\n",
    "betamax_true = np.array(betamax_true)\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "\n",
    "# 计算插值结果和数值结果的残差\n",
    "# residuals_mu1 = interp_mu1(random_points) - mu1_true\n",
    "# residuals_mu2 = interp_mu2(random_points) - mu2_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(mu1_true, interp_mu1_values, s=10, alpha=0.5)\n",
    "plt.plot([0, 50], [0, 50], 'r--')\n",
    "plt.xlabel(\"True μ₁\")\n",
    "plt.ylabel(\"Interpolated μ₁\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.title(\"μ₁: True vs Interpolated\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(mu2_true, interp_mu2_values, s=10, alpha=0.5)\n",
    "plt.plot([0, 30], [0, 30], 'r--')\n",
    "plt.xlabel(\"True μ₂\")\n",
    "plt.ylabel(\"Interpolated μ₂\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.title(\"μ₂: True vs Interpolated\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(betamax_true, interp_betamax_values, s=10, alpha=0.5)\n",
    "plt.plot([0, 20], [0, 20], 'r--')\n",
    "plt.xlabel(\"True βmax\")\n",
    "plt.ylabel(\"Interpolated βmax\")\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 20)\n",
    "plt.title(\"βmax: True vs Interpolated\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(betamax), np.shape(betamax_interp), np.shape(betamax_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dceda27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_model import LensModel\n",
    "from sl_inference_1Dinfer_hdf5_fix_A_runonlinux.mock_generator.lens_properties import lens_properties\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取 HDF5 数据\n",
    "with h5py.File('../../mock_test.h5', 'r') as f:\n",
    "    logMh = f['base/logMh'][:]\n",
    "    logMstar = f['base/logMstar'][:]\n",
    "    logRe = f['base/logRe'][:]\n",
    "    beta = f['base/beta'][:]\n",
    "    mu1 = f['lens/mu1'][:]\n",
    "    mu2 = f['lens/mu2'][:]\n",
    "    betamax = f['lens/betamax'][:]\n",
    "\n",
    "# 随机抽取 30 个样本\n",
    "n_samples = 30\n",
    "rng = np.random.default_rng(42)  # 设置随机数种子\n",
    "indices = rng.choice(len(logMh), size=n_samples, replace=False)  # 随机选择30个索引\n",
    "\n",
    "# 提取这些样本的参数\n",
    "logMh_sample = logMh[indices]\n",
    "logMstar_sample = logMstar[indices]\n",
    "logRe_sample = logRe[indices]\n",
    "beta_sample = beta[indices]\n",
    "\n",
    "mu1_sample = mu1[indices]\n",
    "mu2_sample = mu2[indices]\n",
    "betamax_sample = betamax[indices]\n",
    "\n",
    "# 数值求解函数\n",
    "def compute_mu_betamax(logMh, logMstar, logRe, beta):\n",
    "    model = LensModel(logMstar, logMh, logRe, zl=0.3, zs=2)\n",
    "    props = lens_properties(model, beta)\n",
    "    betamax = model.solve_ycaustic()  # 数值求解betamax\n",
    "    mu1 = float(props['magnificationA'])\n",
    "    mu2 = float(props['magnificationB'])\n",
    "    return betamax, mu1, mu2\n",
    "\n",
    "# 数值求解并计算结果\n",
    "mu1_true = []\n",
    "mu2_true = []\n",
    "betamax_true = []\n",
    "\n",
    "for pt in tqdm(zip(logMh_sample, logMstar_sample, logRe_sample, beta_sample), desc=\"Computing true values\"):\n",
    "    betamax, mu1_val, mu2_val = compute_mu_betamax(*pt)\n",
    "    mu1_true.append(mu1_val)\n",
    "    mu2_true.append(mu2_val)\n",
    "    betamax_true.append(betamax)\n",
    "\n",
    "mu1_true = np.array(mu1_true)\n",
    "mu2_true = np.array(mu2_true)\n",
    "betamax_true = np.array(betamax_true)\n",
    "\n",
    "# 打印对比结果\n",
    "for i in range(n_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"True mu1: {mu1_true[i]}, HDF5 mu1: {mu1_sample[i]}\")\n",
    "    print(f\"True mu2: {mu2_true[i]}, HDF5 mu2: {mu2_sample[i]}\")\n",
    "    print(f\"True betamax: {betamax_true[i]}, HDF5 betamax: {betamax_sample[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 可选：计算误差\n",
    "mu1_error = mu1_sample - mu1_true\n",
    "mu2_error = mu2_sample - mu2_true\n",
    "betamax_error = betamax_sample - betamax_true\n",
    "\n",
    "print(f\"Mean error in mu1: {np.mean(mu1_error)}\")\n",
    "print(f\"Mean error in mu2: {np.mean(mu2_error)}\")\n",
    "print(f\"Mean error in betamax: {np.mean(betamax_error)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a4f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39054e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a909f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346404fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a580ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ea10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab8355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da618ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"lens_interpolators.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "interp_betamax = data[\"betamax\"]\n",
    "interp_mu1     = data[\"mu1\"]\n",
    "interp_mu2     = data[\"mu2\"]\n",
    "\n",
    "print(\"[info] Interpolators loaded (no rebuild needed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba6ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b806dc22",
   "metadata": {},
   "source": [
    "# High Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c65967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 用生成数据的分布 10–90% 区间来构建测试集 ----------\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 5000  # 你要的测试集规模\n",
    "\n",
    "# 计算四个参数的10/90分位\n",
    "b_lo,  b_hi  = np.percentile(beta,     [10, 90])\n",
    "ms_lo, ms_hi = np.percentile(logMstar, [10, 90])\n",
    "mh_lo, mh_hi = np.percentile(logMh,    [10, 90])\n",
    "re_lo, re_hi = np.percentile(logRe,    [10, 90])\n",
    "\n",
    "print(\"[p10, p90] beta   =\", (b_lo,  b_hi))\n",
    "print(\"[p10, p90] logM*  =\", (ms_lo, ms_hi))\n",
    "print(\"[p10, p90] logMh  =\", (mh_lo, mh_hi))\n",
    "print(\"[p10, p90] logRe  =\", (re_lo, re_hi))\n",
    "\n",
    "# 四维同时落在10–90%区间内\n",
    "core_mask = (\n",
    "    (beta     >= b_lo)  & (beta     <= b_hi)  &\n",
    "    (logMstar >= ms_lo) & (logMstar <= ms_hi) &\n",
    "    (logMh    >= mh_lo) & (logMh    <= mh_hi) &\n",
    "    (logRe    >= re_lo) & (logRe    <= re_hi)\n",
    ")\n",
    "\n",
    "core_idx = np.flatnonzero(core_mask)\n",
    "if core_idx.size == 0:\n",
    "    raise RuntimeError(\"80%核心区间筛完后没有可用样本，请检查数据分布或放宽阈值。\")\n",
    "\n",
    "# 若可用样本少于N_test，就全用；否则随机抽 N_test\n",
    "pick = core_idx if core_idx.size <= N_test else rng.choice(core_idx, size=N_test, replace=False)\n",
    "\n",
    "# 构建测试点与真值\n",
    "test_points  = np.column_stack([logMh[pick], logMstar[pick], logRe[pick], beta[pick]])\n",
    "true_betamax = betamax[pick]\n",
    "true_mu1     = mu1[pick]\n",
    "true_mu2     = mu2[pick]\n",
    "\n",
    "# 用你已训练好的插值器预测\n",
    "pred_betamax = interp_betamax(test_points)\n",
    "pred_mu1     = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2     = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "print(f\"[info] 测试集规模: {pick.size}（从核心区间候选 {core_idx.size} 中抽取）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5dafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_range = (\n",
    "    (true_betamax >= 0.0) & (true_betamax <= 20.0) &\n",
    "    (np.abs(true_mu1) <= 30.0)\n",
    ")\n",
    "test_points  = test_points[mask_range]\n",
    "true_betamax = true_betamax[mask_range]\n",
    "true_mu1     = true_mu1[mask_range]\n",
    "true_mu2     = true_mu2[mask_range]\n",
    "pred_betamax = pred_betamax[mask_range]\n",
    "pred_mu1     = pred_mu1[mask_range]\n",
    "pred_mu2     = pred_mu2[mask_range]\n",
    "print(f\"[info] 叠加物理范围后测试集规模: {mask_range.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    e = np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m])))\n",
    "    return e, m\n",
    "\n",
    "# 逐量计算误差\n",
    "err_beta, m_beta = rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = rel_err(pred_mu1,     true_mu1)\n",
    "err_mu2,  m_mu2  = rel_err(pred_mu2,     true_mu2)\n",
    "\n",
    "print(f\"[betamax] mean={np.mean(err_beta):.3e}, p95={np.percentile(err_beta,95):.3e}, max={np.max(err_beta):.3e}\")\n",
    "print(f\"[mu1]     mean={np.mean(err_mu1):.3e}, p95={np.percentile(err_mu1,95):.3e}, max={np.max(err_mu1):.3e}\")\n",
    "print(f\"[mu2]     mean={np.mean(err_mu2):.3e}, p95={np.percentile(err_mu2,95):.3e}, max={np.max(err_mu2):.3e}\")\n",
    "\n",
    "# 多子图画对比\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "\n",
    "pairs = [\n",
    "    (\"betamax\", true_betamax[m_beta], pred_betamax[m_beta], axes[0]),\n",
    "    (\"mu1\",     true_mu1[m_mu1],      pred_mu1[m_mu1],      axes[1]),\n",
    "    (\"mu2\",     true_mu2[m_mu2],      pred_mu2[m_mu2],      axes[2]),\n",
    "]\n",
    "\n",
    "for name, t, p, ax in pairs:\n",
    "    ax.scatter(t, p, s=10, alpha=0.35)\n",
    "    vmin, vmax = float(np.nanmin([t.min(), p.min()])), float(np.nanmax([t.max(), p.max()]))\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    ax.set_xlim(vmin, vmax)\n",
    "    ax.set_ylim(vmin, vmax)\n",
    "    if  name == \"mu2\":\n",
    "        ax.set_xlim(0, 5)\n",
    "        ax.set_ylim(0, 5)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\")\n",
    "    ax.set_title(f\"{name} 对比\\nmean={np.mean(np.abs((p - t)/t)):.2e}, p95={np.percentile(np.abs((p - t)/t),95):.2e}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6302597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- 用生成数据的分布 10–90% 区间来构建测试集 ----------\n",
    "rng = np.random.default_rng(42)\n",
    "N_test = 5000  # 你要的测试集规模\n",
    "\n",
    "# 计算四个参数的10/90分位\n",
    "b_lo,  b_hi  = np.percentile(beta,     [10, 90])\n",
    "ms_lo, ms_hi = np.percentile(logMstar, [10, 90])\n",
    "mh_lo, mh_hi = np.percentile(logMh,    [10, 90])\n",
    "re_lo, re_hi = np.percentile(logRe,    [10, 90])\n",
    "\n",
    "print(\"[p10, p90] beta   =\", (b_lo,  b_hi))\n",
    "print(\"[p10, p90] logM*  =\", (ms_lo, ms_hi))\n",
    "print(\"[p10, p90] logMh  =\", (mh_lo, mh_hi))\n",
    "print(\"[p10, p90] logRe  =\", (re_lo, re_hi))\n",
    "\n",
    "# 四维同时落在10–90%区间内\n",
    "core_mask = (\n",
    "    (beta     >= b_lo)  & (beta     <= b_hi)  &\n",
    "    (logMstar >= ms_lo) & (logMstar <= ms_hi) &\n",
    "    (logMh    >= mh_lo) & (logMh    <= mh_hi) &\n",
    "    (logRe    >= re_lo) & (logRe    <= re_hi)\n",
    ")\n",
    "\n",
    "core_idx = np.flatnonzero(core_mask)\n",
    "if core_idx.size == 0:\n",
    "    raise RuntimeError(\"80%核心区间筛完后没有可用样本，请检查数据分布或放宽阈值。\")\n",
    "\n",
    "# 若可用样本少于N_test，就全用；否则随机抽 N_test\n",
    "test_idx = core_idx if core_idx.size <= N_test else rng.choice(core_idx, size=N_test, replace=False)\n",
    "\n",
    "# 构建测试点与真值\n",
    "test_points  = np.column_stack([logMh[test_idx], logMstar[test_idx], logRe[test_idx], beta[test_idx]])\n",
    "true_betamax = betamax[test_idx]\n",
    "true_mu1     = mu1[test_idx]\n",
    "true_mu2     = mu2[test_idx]\n",
    "\n",
    "# 用你已训练好的插值器预测\n",
    "pred_betamax = interp_betamax(test_points)\n",
    "pred_mu1     = _mu_inv(interp_mu1(test_points))\n",
    "pred_mu2     = _mu_inv(interp_mu2(test_points))\n",
    "\n",
    "print(f\"[info] 测试集规模: {test_idx.size}（从核心区间候选 {core_idx.size} 中抽取）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== 1) 计算 mu2 的相对误差 =====\n",
    "err_mu2, m_mu2 = rel_err(pred_mu2, true_mu2)\n",
    "\n",
    "# 阈值：相对误差 > 0.05 定义为高误差点\n",
    "THRESH = 0.05\n",
    "mask_high = err_mu2 > THRESH\n",
    "mask_low  = err_mu2 <= THRESH\n",
    "\n",
    "print(f\"总样本数={len(err_mu2)}, 高误差数={mask_high.sum()} ({mask_high.sum()/len(err_mu2)*100:.1f}%)\")\n",
    "\n",
    "# ===== 2) 构建 DataFrame 方便分析 =====\n",
    "df = pd.DataFrame({\n",
    "    \"logMh\":    logMh[test_idx][m_mu2],\n",
    "    \"logMstar\": logMstar[test_idx][m_mu2],\n",
    "    \"logRe\":    logRe[test_idx][m_mu2],\n",
    "    \"beta\":     beta[test_idx][m_mu2],\n",
    "    \"mu2_true\": true_mu2[m_mu2],\n",
    "    \"mu2_pred\": pred_mu2[m_mu2],\n",
    "    \"rel_err\":  err_mu2,\n",
    "    \"err_label\": np.where(mask_high, \"high_err\", \"low_err\"),\n",
    "})\n",
    "\n",
    "# ===== 3) 高误差和低误差的统计对比 =====\n",
    "print(\"\\n===== 高误差 vs 低误差：均值比较 =====\")\n",
    "for col in [\"logMh\", \"logMstar\", \"logRe\", \"beta\", \"mu2_true\"]:\n",
    "    print(f\"{col:>8s}: high={df.loc[mask_high,col].mean():.3f}, \"\n",
    "          f\"low={df.loc[mask_low,col].mean():.3f}\")\n",
    "\n",
    "# ===== 4) 可视化：pairplot 查看高误差点位置 =====\n",
    "sns.pairplot(df, vars=[\"logMh\",\"logMstar\",\"logRe\",\"beta\"], hue=\"err_label\",\n",
    "             palette={\"low_err\":\"blue\", \"high_err\":\"red\"},\n",
    "             plot_kws=dict(alpha=0.4, s=10))\n",
    "plt.suptitle(f\"mu₂ 高误差点分布 (阈值>{THRESH}, 样本占比 {mask_high.sum()/len(err_mu2)*100:.1f}%)\",\n",
    "             y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ===== 5) 可视化：单变量分布对比 =====\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "cols = [\"logMh\", \"logMstar\", \"logRe\", \"beta\"]\n",
    "\n",
    "for ax, col in zip(axes.flatten(), cols):\n",
    "    sns.kdeplot(df.loc[mask_low, col], label=\"low_err\", ax=ax, color=\"blue\")\n",
    "    sns.kdeplot(df.loc[mask_high, col], label=\"high_err\", ax=ax, color=\"red\")\n",
    "    ax.set_title(f\"{col} 分布对比\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557d05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4690c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89afab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3191f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9492b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以 mu1 的有效掩码为基础，进一步要求 mu2 有效\n",
    "mask_common_for_mu2 = m_mu1.copy()  # 和 N_test 对齐的布尔数组\n",
    "mask_common_for_mu2[mask_common_for_mu2] &= (\n",
    "    np.isfinite(pred_mu2[mask_common_for_mu2]) &\n",
    "    np.isfinite(true_mu2[mask_common_for_mu2]) &\n",
    "    (np.abs(true_mu2[mask_common_for_mu2]) > 0)\n",
    ")\n",
    "\n",
    "# —— 用“整数索引”对齐所有参数（避免重复布尔索引导致的长度不一致）\n",
    "idx_common = np.where(mask_common_for_mu2)[0]              # 长度 = 有效点数\n",
    "param_idx  = test_idx[idx_common]                          # 映射回原始 bank 的行号\n",
    "\n",
    "# 这些是参数（与 mu1 的有效点完全对齐）\n",
    "logMh_sel    = logMh[param_idx]\n",
    "logMstar_sel = logMstar[param_idx]\n",
    "logRe_sel    = logRe[param_idx]\n",
    "beta_sel     = beta[param_idx]\n",
    "\n",
    "# 这些是 mu2 的真值/预测/相对误差（已经是“有效子集”，长度与 idx_common 相同）\n",
    "true_mu2_sel = true_mu2[idx_common]\n",
    "pred_mu2_sel = pred_mu2[idx_common]\n",
    "err_mu2_sel  = np.abs((pred_mu2_sel - true_mu2_sel) / np.maximum(1e-12, np.abs(true_mu2_sel)))\n",
    "\n",
    "# 误差标签\n",
    "THRESH = 0.05\n",
    "labels = np.where(err_mu2_sel > THRESH, \"high_err\", \"low_err\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"logMh\": logMh_sel,\n",
    "    \"logMstar\": logMstar_sel,\n",
    "    \"logRe\": logRe_sel,\n",
    "    \"beta\": beta_sel,\n",
    "    \"mu2_true\": true_mu2_sel,\n",
    "    \"mu2_pred\": pred_mu2_sel,\n",
    "    \"rel_err\": err_mu2_sel,\n",
    "    \"label\": labels,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(\n",
    "    df.sample(min(len(df), 4000), random_state=0),\n",
    "    vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"],\n",
    "    hue=\"label\",\n",
    "    palette={\"low_err\":\"steelblue\",\"high_err\":\"crimson\"},\n",
    "    plot_kws={\"alpha\":0.4, \"s\":8},\n",
    "    diag_kind=\"kde\"\n",
    ")\n",
    "plt.suptitle(f\"mu2 high-error points (> {THRESH:.2f}), fraction={ (labels=='high_err').mean()*100:.1f}% \", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a8368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd3b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f88faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c25185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1543b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb74905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c2012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d119dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796f161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74755863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2637bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e68d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84c21aa7",
   "metadata": {},
   "source": [
    "# Add DetJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) 为 betamax / mu1 / mu2 / detJ 构建插值器（同一输入 -> 多输出）===\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "N_train = 50000\n",
    "idx = np.random.choice(len(logMh), size=N_train, replace=False)\n",
    "\n",
    "train_points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "train_values = betamax[idx]\n",
    "# ——稳定变换与逆变换（保号，压缩动态范围）\n",
    "def _sv_t(x):   # signed log1p\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "def _sv_inv(y): # inverse\n",
    "    return np.sign(y) * np.expm1(np.abs(y))\n",
    "\n",
    "# 训练索引与输入点（已在你代码中定义过 N_train/idx/train_points）\n",
    "# train_points = np.column_stack([logMh[idx], logMstar[idx], logRe[idx], beta[idx]])\n",
    "\n",
    "# 构建各物理量的插值器\n",
    "interp = {\n",
    "    \"betamax\": LinearNDInterpolator(train_points, betamax[idx],     fill_value=np.nan),\n",
    "    \"mu1\":     LinearNDInterpolator(train_points, _sv_t(mu1[idx]),  fill_value=np.nan),\n",
    "    \"mu2\":     LinearNDInterpolator(train_points, _sv_t(mu2[idx]),  fill_value=np.nan),\n",
    "}\n",
    "# detJ 可能不存在，存在则加入\n",
    "try:\n",
    "    detJ  # 如果未读取会 NameError\n",
    "    interp[\"detJ\"] = LinearNDInterpolator(train_points, _sv_t(detJ[idx]), fill_value=np.nan)\n",
    "    has_detJ = True\n",
    "except Exception:\n",
    "    has_detJ = False\n",
    "\n",
    "# 准备真值与预测（测试点 test_points 已由你代码产生，并经过 mask_valid 过滤）\n",
    "true_vals_map = {\n",
    "    \"betamax\": true_vals,                    # (|μ|<1e4 后的)\n",
    "    \"mu1\":     mu1_test[mask_valid],\n",
    "    \"mu2\":     mu2_test[mask_valid],\n",
    "}\n",
    "if has_detJ:\n",
    "    true_vals_map[\"detJ\"] = detJ[test_idx][mask_valid]\n",
    "\n",
    "pred_vals_map = {\n",
    "    \"betamax\": pred_vals,                    # 已有\n",
    "    \"mu1\":     _sv_inv(interp[\"mu1\"](test_points)),\n",
    "    \"mu2\":     _sv_inv(interp[\"mu2\"](test_points)),\n",
    "}\n",
    "if has_detJ:\n",
    "    pred_vals_map[\"detJ\"] = _sv_inv(interp[\"detJ\"](test_points))\n",
    "\n",
    "# === 6) 逐变量计算误差（安全处理 0/NaN）===\n",
    "def safe_rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    if m.sum() == 0:\n",
    "        return np.array([]), m\n",
    "    return np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m]))), m\n",
    "\n",
    "errs = {}\n",
    "masks = {}\n",
    "for k in true_vals_map:\n",
    "    e, m = safe_rel_err(pred_vals_map[k], true_vals_map[k])\n",
    "    errs[k], masks[k] = e, m\n",
    "    mean_e = np.nanmean(e) if e.size else np.nan\n",
    "    print(f\"[{k}] mean rel.err = {mean_e:.3e}, N={m.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 7) 子图：真值 vs 插值，并高亮大误差点 ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "quantities_order = [\"betamax\", \"mu1\", \"mu2\"] + ([\"detJ\"] if has_detJ else [])\n",
    "nq = len(quantities_order)\n",
    "fig, axes = plt.subplots(1, nq, figsize=(5*nq, 5), dpi=150)\n",
    "\n",
    "if nq == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "REL_THR = 0.30  # 误差阈值（相对误差 > 10% 高亮），可调\n",
    "\n",
    "for ax, name in zip(axes, quantities_order):\n",
    "    tv_full = true_vals_map[name]\n",
    "    pv_full = pred_vals_map[name]\n",
    "    m = masks[name]\n",
    "    tv = tv_full[m]\n",
    "    pv = pv_full[m]\n",
    "    e  = errs[name]\n",
    "\n",
    "    # 散点 + 对角线\n",
    "    ax.scatter(tv, pv, s=10, alpha=0.35, label=\"samples\")\n",
    "    vmin = np.nanmin([tv.min(), pv.min()])\n",
    "    vmax = np.nanmax([tv.max(), pv.max()])\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "\n",
    "    # 高亮误差超过阈值的点\n",
    "    out = e > REL_THR\n",
    "    if np.any(out):\n",
    "        ax.scatter(tv[out], pv[out],\n",
    "                   s=45, facecolors=\"none\", edgecolors=\"crimson\",\n",
    "                   linewidths=1.0, label=f\"rel.err>{REL_THR:.0%} ({out.sum()})\")\n",
    "\n",
    "    ax.set_xlim(vmin, vmax)\n",
    "    ax.set_ylim(vmin, vmax)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Interpolated {name}\")\n",
    "    ax.set_title(f\"{name} | mean rel.err={np.nanmean(e):.2e} | N={len(e)}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(loc=\"best\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. 计算相对误差（沿用已有 pred_* 和 true_*）===\n",
    "def safe_rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (np.abs(true) > 0)\n",
    "    return np.abs((pred[m] - true[m]) / np.maximum(1e-12, np.abs(true[m]))), m\n",
    "\n",
    "err_beta, m_beta = safe_rel_err(pred_betamax, true_betamax)\n",
    "err_mu1,  m_mu1  = safe_rel_err(pred_mu1,     true_mu1)\n",
    "err_mu2,  m_mu2  = safe_rel_err(pred_mu2,     true_mu2)\n",
    "\n",
    "# === 2. 找出误差最大的若干点 ===\n",
    "top_k = 5  # 取前 5 个误差最大的点\n",
    "worst_beta_idx = np.argsort(err_beta)[::-1][:top_k]\n",
    "worst_mu1_idx  = np.argsort(err_mu1)[::-1][:top_k]\n",
    "worst_mu2_idx  = np.argsort(err_mu2)[::-1][:top_k]\n",
    "\n",
    "# === 3. 取出这些测试点的输入参数 (logMh, logMstar, logRe, beta) ===\n",
    "points_beta_worst = test_points[m_beta][worst_beta_idx]\n",
    "points_mu1_worst  = test_points[m_mu1][worst_mu1_idx]\n",
    "points_mu2_worst  = test_points[m_mu2][worst_mu2_idx]\n",
    "\n",
    "# === 4. 构造 DataFrame ===\n",
    "df_train = pd.DataFrame(train_points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "df_train[\"type\"] = \"train\"\n",
    "\n",
    "def make_df(points, label):\n",
    "    df = pd.DataFrame(points, columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"])\n",
    "    df[\"type\"] = label\n",
    "    return df\n",
    "\n",
    "df_all = pd.concat([\n",
    "    df_train.sample(5000, random_state=0),  # 抽样，避免太大\n",
    "    make_df(points_beta_worst, \"worst_betamax\"),\n",
    "    make_df(points_mu1_worst,  \"worst_mu1\"),\n",
    "    make_df(points_mu2_worst,  \"worst_mu2\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "# === 5. 绘制 pairplot ===\n",
    "palette = {\n",
    "    \"train\": \"lightgray\",\n",
    "    \"worst_betamax\": \"red\",\n",
    "    \"worst_mu1\": \"blue\",\n",
    "    \"worst_mu2\": \"green\",\n",
    "}\n",
    "\n",
    "sns.pairplot(\n",
    "    df_all,\n",
    "    vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"],  # 明确是 β，而不是 betamax\n",
    "    hue=\"type\",\n",
    "    palette=palette,\n",
    "    plot_kws={\"alpha\": 0.5, \"s\": 8},\n",
    "    diag_kind=\"kde\"\n",
    ")\n",
    "plt.suptitle(f\"Top-{top_k} worst betamax/mu1/mu2 points in parameter space (β not betamax)\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假设 logMh, logMstar, logRe, beta, betamax, mu1, mu2 已经存在\n",
    "df = pd.DataFrame({\n",
    "    \"logMh\": logMh,\n",
    "    \"logMstar\": logMstar,\n",
    "    \"logRe\": logRe,\n",
    "    \"beta\": beta,\n",
    "    \"betamax\": betamax,\n",
    "    \"log_mu12\": np.log1p(np.abs(mu1) + np.abs(mu2)),  # 正则化 μ1+μ2，避免爆炸\n",
    "})\n",
    "\n",
    "df_sample = df.sample(3000, random_state=42)\n",
    "\n",
    "vars_to_plot = [\"logMh\", \"logMstar\", \"logRe\", \"beta\"]\n",
    "color_vars   = [\"betamax\", \"log_mu12\"]\n",
    "\n",
    "for cvar in color_vars:\n",
    "    g = sns.PairGrid(df_sample, vars=vars_to_plot, diag_sharey=False)\n",
    "    g.map_diag(sns.histplot, kde=True, color=\"gray\")\n",
    "    g.map_lower(sns.scatterplot, color=\"lightgray\", s=5, alpha=0.3)\n",
    "\n",
    "    # 遍历 axes 数组 (i=row, j=col)\n",
    "    for i in range(len(vars_to_plot)):\n",
    "        for j in range(len(vars_to_plot)):\n",
    "            if i < j:  # 上三角\n",
    "                ax = g.axes[i, j]\n",
    "                x = df_sample[vars_to_plot[j]]\n",
    "                y = df_sample[vars_to_plot[i]]\n",
    "                c = df_sample[cvar]\n",
    "                sc = ax.scatter(x, y, c=c, cmap=\"viridis\", s=8, alpha=0.6)\n",
    "\n",
    "    # 颜色条放在整个 figure 的右边\n",
    "    g.fig.colorbar(sc, ax=g.axes, label=cvar, shrink=0.8)\n",
    "    g.fig.suptitle(f\"Pairplot colored by {cvar}\", y=1.02)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59858d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_idx = np.random.randint(len(logMh))\n",
    "p0 = np.array([logMh[center_idx], logMstar[center_idx], logRe[center_idx], beta[center_idx]])\n",
    "\n",
    "# 取邻域样本\n",
    "d = np.linalg.norm(np.column_stack([logMh, logMstar, logRe, beta]) - p0, axis=1)\n",
    "mask_local = d < 0.05  # 0.05 ~ \"邻域半径\"\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(beta[mask_local], betamax[mask_local], c=logMh[mask_local], cmap=\"viridis\", s=10)\n",
    "plt.xlabel(\"beta\"); plt.ylabel(\"betamax\")\n",
    "plt.title(\"局部邻域：颜色=logMh\")\n",
    "plt.colorbar(label=\"logMh\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# === 1. 子样本选择 ===\n",
    "N_sub = 20000  # 控制在几万，保证速度\n",
    "rng = np.random.default_rng(0)\n",
    "idx_sub = rng.choice(len(logMh), size=N_sub, replace=False)\n",
    "\n",
    "X_sub = np.column_stack([logMh[idx_sub], logMstar[idx_sub], logRe[idx_sub], beta[idx_sub]])\n",
    "y_sub = betamax[idx_sub]  # 也可以改成 mu1[idx_sub] 或 mu2[idx_sub]\n",
    "\n",
    "# === 2. 最近邻搜索 ===\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm=\"auto\").fit(X_sub)\n",
    "distances, indices = nbrs.kneighbors(X_sub)\n",
    "\n",
    "# === 3. 最近邻差分梯度范数 ===\n",
    "dy = np.abs(y_sub - y_sub[indices[:, 1]])\n",
    "grad_norm = dy / (distances[:, 1] + 1e-12)\n",
    "\n",
    "# === 4. 可视化分布 ===\n",
    "plt.figure(figsize=(6, 4), dpi=120)\n",
    "plt.hist(np.log10(grad_norm), bins=100, color=\"steelblue\", alpha=0.7)\n",
    "plt.xlabel(r\"$\\log_{10}(|\\Delta y| / \\|\\Delta x\\|)$\", fontsize=12)\n",
    "plt.ylabel(\"Count\")\n",
    "# plt.title(\"局部梯度范数分布 (子样本)\", fontsize=13)\n",
    "plt.title(f\"Local gradient norm distribution (N={N_sub})\", fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"[info] sample number={N_sub}, local gradient log10 mean={np.mean(np.log10(grad_norm)):.3f}, \"\n",
    "      f\"95% percentile={np.percentile(np.log10(grad_norm),95):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b378e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# === 1. 子样本选择 ===\n",
    "N_sub = 20000  # 控制在几万，保证速度\n",
    "rng = np.random.default_rng(0)\n",
    "idx_sub = rng.choice(len(logMh), size=N_sub, replace=False)\n",
    "\n",
    "X_sub = np.column_stack([logMh[idx_sub], logMstar[idx_sub], logRe[idx_sub], beta[idx_sub]])\n",
    "y_sub = mu1[idx_sub]  # 也可以改成 mu1[idx_sub] 或 mu2[idx_sub]\n",
    "\n",
    "# === 2. 最近邻搜索 ===\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm=\"auto\").fit(X_sub)\n",
    "distances, indices = nbrs.kneighbors(X_sub)\n",
    "\n",
    "# === 3. 最近邻差分梯度范数 ===\n",
    "dy = np.abs(y_sub - y_sub[indices[:, 1]])\n",
    "grad_norm = dy / (distances[:, 1] + 1e-12)\n",
    "\n",
    "# === 4. 可视化分布 ===\n",
    "plt.figure(figsize=(6, 4), dpi=120)\n",
    "plt.hist(np.log10(grad_norm), bins=100, color=\"steelblue\", alpha=0.7)\n",
    "plt.xlabel(r\"$\\log_{10}(|\\Delta y| / \\|\\Delta x\\|)$\", fontsize=12)\n",
    "plt.ylabel(\"Count\")\n",
    "# plt.title(\"局部梯度范数分布 (子样本)\", fontsize=13)\n",
    "plt.title(f\"Local gradient norm distribution (N={N_sub})\", fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"[info] sample number={N_sub}, local gradient log10 mean={np.mean(np.log10(grad_norm)):.3f}, \"\n",
    "      f\"95% percentile={np.percentile(np.log10(grad_norm),95):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# === 1. 子样本选择 ===\n",
    "N_sub = 20000  # 控制在几万，保证速度\n",
    "rng = np.random.default_rng(0)\n",
    "idx_sub = rng.choice(len(logMh), size=N_sub, replace=False)\n",
    "\n",
    "X_sub = np.column_stack([logMh[idx_sub], logMstar[idx_sub], logRe[idx_sub], beta[idx_sub]])\n",
    "y_sub = mu2[idx_sub]  # 也可以改成 mu1[idx_sub] 或 mu2[idx_sub]\n",
    "\n",
    "# === 2. 最近邻搜索 ===\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm=\"auto\").fit(X_sub)\n",
    "distances, indices = nbrs.kneighbors(X_sub)\n",
    "\n",
    "# === 3. 最近邻差分梯度范数 ===\n",
    "dy = np.abs(y_sub - y_sub[indices[:, 1]])\n",
    "grad_norm = dy / (distances[:, 1] + 1e-12)\n",
    "\n",
    "# === 4. 可视化分布 ===\n",
    "plt.figure(figsize=(6, 4), dpi=120)\n",
    "plt.hist(np.log10(grad_norm), bins=100, color=\"steelblue\", alpha=0.7)\n",
    "plt.xlabel(r\"$\\log_{10}(|\\Delta y| / \\|\\Delta x\\|)$\", fontsize=12)\n",
    "plt.ylabel(\"Count\")\n",
    "# plt.title(\"局部梯度范数分布 (子样本)\", fontsize=13)\n",
    "plt.title(f\"Local gradient norm distribution (N={N_sub})\", fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"[info] sample number={N_sub}, local gradient log10 mean={np.mean(np.log10(grad_norm)):.3f}, \"\n",
    "      f\"95% percentile={np.percentile(np.log10(grad_norm),95):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae01149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56a662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cff426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06087eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5226d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91680e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d7bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import numpy as np\n",
    "\n",
    "# === 现有 train_points 已经构建好 ===\n",
    "detJ_train = detJ[idx]\n",
    "\n",
    "# --- 1) 直接插值 detJ（baseline）\n",
    "interp_detJ_linear = LinearNDInterpolator(train_points, detJ_train, fill_value=np.nan)\n",
    "\n",
    "# --- 2) 插值 log|detJ|，反变换回来\n",
    "mask_pos = detJ_train > 0   # detJ<=0 无物理意义的点跳过\n",
    "interp_logdetJ = LinearNDInterpolator(\n",
    "    train_points[mask_pos], np.log(detJ_train[mask_pos]), fill_value=np.nan\n",
    ")\n",
    "def predict_logdetJ(points):\n",
    "    log_pred = interp_logdetJ(points)\n",
    "    return np.exp(log_pred)\n",
    "\n",
    "# --- 3) 插值 sqrt(detJ)，反变换回来（对零附近更平滑）\n",
    "interp_sqrtdetJ = LinearNDInterpolator(\n",
    "    train_points[mask_pos], np.sqrt(detJ_train[mask_pos]), fill_value=np.nan\n",
    ")\n",
    "def predict_sqrtdetJ(points):\n",
    "    sq_pred = interp_sqrtdetJ(points)\n",
    "    return sq_pred**2\n",
    "\n",
    "# --- 4) 插值 1/detJ，再取倒数（强调临界附近变化）\n",
    "interp_invdetJ = LinearNDInterpolator(\n",
    "    train_points[mask_pos], 1.0/np.maximum(detJ_train[mask_pos], 1e-12), fill_value=np.nan\n",
    ")\n",
    "def predict_invdetJ(points):\n",
    "    inv_pred = interp_invdetJ(points)\n",
    "    return 1.0 / np.maximum(inv_pred, 1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b45943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_points 已有\n",
    "true_detJ = detJ[test_idx][mask_valid]\n",
    "\n",
    "pred1 = interp_detJ_linear(test_points)                # 方法 1\n",
    "pred2 = predict_logdetJ(test_points)                  # 方法 2\n",
    "pred3 = predict_sqrtdetJ(test_points)                 # 方法 3\n",
    "pred4 = predict_invdetJ(test_points)                  # 方法 4\n",
    "\n",
    "def rel_err(pred, true):\n",
    "    m = np.isfinite(pred) & np.isfinite(true) & (true > 0)\n",
    "    return np.mean(np.abs((pred[m] - true[m]) / true[m]))\n",
    "\n",
    "print(f\"[Linear] mean rel.err = {rel_err(pred1,true_detJ):.3e}\")\n",
    "print(f\"[log(detJ)] mean rel.err = {rel_err(pred2,true_detJ):.3e}\")\n",
    "print(f\"[sqrt(detJ)] mean rel.err = {rel_err(pred3,true_detJ):.3e}\")\n",
    "print(f\"[1/detJ] mean rel.err = {rel_err(pred4,true_detJ):.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "# === 1) 准备训练和测试数据 ===\n",
    "# 假设 logMh/logMstar/logRe/beta/detJ/train_points/test_points/test_idx/mask_valid 都已准备好\n",
    "detJ_train = detJ[idx]\n",
    "true_detJ  = detJ[test_idx][mask_valid]\n",
    "\n",
    "# === 2) 构建 4 种插值器 ===\n",
    "mask_pos = detJ_train > 0\n",
    "\n",
    "interp_linear = LinearNDInterpolator(train_points, detJ_train, fill_value=np.nan)\n",
    "\n",
    "interp_log = LinearNDInterpolator(\n",
    "    train_points[mask_pos], np.log(detJ_train[mask_pos]), fill_value=np.nan\n",
    ")\n",
    "\n",
    "interp_sqrt = LinearNDInterpolator(\n",
    "    train_points[mask_pos], np.sqrt(detJ_train[mask_pos]), fill_value=np.nan\n",
    ")\n",
    "\n",
    "interp_inv = LinearNDInterpolator(\n",
    "    train_points[mask_pos], 1.0 / np.maximum(detJ_train[mask_pos], 1e-12), fill_value=np.nan\n",
    ")\n",
    "\n",
    "# === 3) 预测 ===\n",
    "pred_linear = interp_linear(test_points)\n",
    "pred_log    = np.exp(interp_log(test_points))\n",
    "pred_sqrt   = interp_sqrt(test_points)**2\n",
    "pred_inv    = 1.0 / np.maximum(interp_inv(test_points), 1e-12)\n",
    "\n",
    "# === 4) 计算相对误差 ===\n",
    "def safe_rel_err(pred, true):\n",
    "    mask = np.isfinite(pred) & np.isfinite(true) & (true > 0)\n",
    "    err  = np.abs((pred[mask] - true[mask]) / true[mask])\n",
    "    return err, mask\n",
    "\n",
    "err_linear, m_linear = safe_rel_err(pred_linear, true_detJ)\n",
    "err_log,    m_log    = safe_rel_err(pred_log,    true_detJ)\n",
    "err_sqrt,   m_sqrt   = safe_rel_err(pred_sqrt,   true_detJ)\n",
    "err_inv,    m_inv    = safe_rel_err(pred_inv,    true_detJ)\n",
    "\n",
    "print(f\"[Linear] mean rel.err = {np.mean(err_linear):.3e}\")\n",
    "print(f\"[log(detJ)] mean rel.err = {np.mean(err_log):.3e}\")\n",
    "print(f\"[sqrt(detJ)] mean rel.err = {np.mean(err_sqrt):.3e}\")\n",
    "print(f\"[1/detJ] mean rel.err = {np.mean(err_inv):.3e}\")\n",
    "\n",
    "# === 5) 可视化比较 ===\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5), dpi=150)\n",
    "\n",
    "methods = [\n",
    "    (\"Linear\", pred_linear[m_linear], true_detJ[m_linear], err_linear, axes[0]),\n",
    "    (\"log(detJ)\", pred_log[m_log], true_detJ[m_log], err_log, axes[1]),\n",
    "    (\"sqrt(detJ)\", pred_sqrt[m_sqrt], true_detJ[m_sqrt], err_sqrt, axes[2]),\n",
    "    (\"1/detJ\", pred_inv[m_inv], true_detJ[m_inv], err_inv, axes[3]),\n",
    "]\n",
    "\n",
    "for name, pred_ok, true_ok, err_ok, ax in methods:\n",
    "    ax.scatter(true_ok, pred_ok, s=10, alpha=0.35, label=\"samples\")\n",
    "    # 画出高误差点\n",
    "    high_err = err_ok > 0.2  # 20% 以上相对误差\n",
    "    ax.scatter(true_ok[high_err], pred_ok[high_err], \n",
    "               facecolors='none', edgecolors='red', s=40, lw=1.2, label=\"rel.err>20%\")\n",
    "    vmin = np.nanmin([true_ok.min(), pred_ok.min()])\n",
    "    vmax = np.nanmax([true_ok.max(), pred_ok.max()])\n",
    "    ax.plot([vmin, vmax], [vmin, vmax], 'r--', lw=1)\n",
    "    ax.set_xlim(vmin, vmax)\n",
    "    ax.set_ylim(vmin, vmax)\n",
    "    ax.set_xlabel(\"True detJ\")\n",
    "    ax.set_ylabel(\"Interpolated detJ\")\n",
    "    ax.set_title(f\"{name}\\nmean rel.err={np.mean(err_ok):.2e}\")\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6526fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d75e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0eb66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def94c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e4a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db398961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. 多子图比较：真值 vs 插值（标出误差前10） ===\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 计算相对误差并找出前 10\n",
    "abs_rel_err = np.abs((pred_vals - true_vals) / true_vals)\n",
    "valid_err_mask = np.isfinite(abs_rel_err)\n",
    "abs_rel_err = abs_rel_err[valid_err_mask]\n",
    "true_vals_valid = true_vals[valid_err_mask]\n",
    "pred_vals_valid = pred_vals[valid_err_mask]\n",
    "\n",
    "top_k = 10\n",
    "worst_idx = np.argsort(abs_rel_err)[-top_k:][::-1]  # 从大到小\n",
    "\n",
    "quantities = {\n",
    "    \"betamax\": (true_vals_valid, pred_vals_valid),\n",
    "    \"mu1\": (mu1_test[mask_valid], mu1_test[mask_valid]),\n",
    "    \"mu2\": (mu2_test[mask_valid], mu2_test[mask_valid]),\n",
    "}\n",
    "\n",
    "n_var = len(quantities)\n",
    "fig, axes = plt.subplots(1, n_var, figsize=(5*n_var, 5), dpi=150)\n",
    "\n",
    "if n_var == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, (tv, pv)) in zip(axes, quantities.items()):\n",
    "    valid = np.isfinite(tv) & np.isfinite(pv)\n",
    "    tv, pv = tv[valid], pv[valid]\n",
    "\n",
    "    ax.scatter(tv, pv, alpha=0.3, s=12, label=\"samples\")\n",
    "    lims = [np.nanmin(tv), np.nanmax(tv)]\n",
    "    ax.plot(lims, lims, 'r--', lw=1)\n",
    "    ax.set_xlabel(f\"True {name}\")\n",
    "    ax.set_ylabel(f\"Predicted {name}\" if name == \"betamax\" else \"True\")\n",
    "\n",
    "    if name == \"betamax\":\n",
    "        # 标出前10误差最大点\n",
    "        ax.scatter(\n",
    "            true_vals_valid[worst_idx],\n",
    "            pred_vals_valid[worst_idx],\n",
    "            color=\"red\",\n",
    "            s=40,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            label=f\"Top {top_k} worst\"\n",
    "        )\n",
    "        mean_rel_err = np.nanmean(np.abs((pv - tv)/tv))\n",
    "        ax.set_title(f\"{name} (mean rel.err={mean_rel_err:.2e})\")\n",
    "        ax.legend(fontsize=8)\n",
    "    else:\n",
    "        ax.set_title(f\"{name} (diag check)\")\n",
    "\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cadc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 5. 计算误差并筛掉无效点 ===\n",
    "valid_err_mask = np.isfinite(true_vals) & np.isfinite(pred_vals) & (true_vals != 0)\n",
    "true_vals_valid = true_vals[valid_err_mask]\n",
    "pred_vals_valid = pred_vals[valid_err_mask]\n",
    "test_points_valid = test_points[valid_err_mask]\n",
    "\n",
    "abs_rel_err = np.abs((pred_vals_valid - true_vals_valid) / true_vals_valid)\n",
    "sorted_idx = np.argsort(abs_rel_err)[::-1]\n",
    "top_k = 5\n",
    "worst_idx = sorted_idx[:top_k]\n",
    "\n",
    "print(f\"[info] Top {top_k} worst points, 平均相对误差={abs_rel_err[worst_idx].mean():.3e}\")\n",
    "\n",
    "# === 6. 组合 DataFrame (随机抽样训练集) ===\n",
    "N_subsample = 5000  # 训练集子样本数量，控制绘图速度\n",
    "sub_idx = np.random.choice(len(train_points), size=N_subsample, replace=False)\n",
    "\n",
    "df_train = pd.DataFrame(\n",
    "    train_points[sub_idx],\n",
    "    columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"]\n",
    ")\n",
    "df_train[\"type\"] = \"train\"\n",
    "\n",
    "df_worst = pd.DataFrame(\n",
    "    test_points_valid[worst_idx],\n",
    "    columns=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"]\n",
    ")\n",
    "df_worst[\"type\"] = \"worst\"\n",
    "\n",
    "df_all = pd.concat([df_train, df_worst], ignore_index=True)\n",
    "\n",
    "\n",
    "# === 7. 可视化 ===\n",
    "# 指定高分辨率：dpi=200 或 300；放大画布尺寸\n",
    "g = sns.pairplot(\n",
    "    df_all,\n",
    "    vars=[\"logMh\", \"logMstar\", \"logRe\", \"beta\"],\n",
    "    hue=\"type\",\n",
    "    palette={\"train\": \"lightblue\", \"worst\": \"red\"},\n",
    "    plot_kws={\"alpha\": 0.5, \"s\": 8},\n",
    "    diag_kind=\"kde\",\n",
    "    height=2.2   # 每个子图的尺寸，调大提高总分辨率\n",
    ")\n",
    "\n",
    "g.fig.set_dpi(500)  # 设置高分辨率\n",
    "g.fig.suptitle(f\"Train (subsample={N_subsample}) vs Worst {top_k} points\", y=1.02)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. 重新计算误差 ===\n",
    "abs_rel_err = np.abs((pred_vals_valid - true_vals_valid) / true_vals_valid)\n",
    "\n",
    "# 排序索引\n",
    "sorted_idx = np.argsort(abs_rel_err)[::-1]\n",
    "\n",
    "# 去掉前 5 个最大误差点\n",
    "mask_exclude = np.ones_like(abs_rel_err, dtype=bool)\n",
    "mask_exclude[sorted_idx[:5]] = False\n",
    "\n",
    "abs_rel_err_clean = abs_rel_err[mask_exclude]\n",
    "\n",
    "# === 2. 输出均值、标准差、分位数 ===\n",
    "print(f\"[去掉前 5 个最大误差后]\")\n",
    "print(f\"平均相对误差: {np.nanmean(abs_rel_err_clean):.3e}\")\n",
    "print(f\"标准差:       {np.nanstd(abs_rel_err_clean):.3e}\")\n",
    "print(f\"中位数:       {np.nanmedian(abs_rel_err_clean):.3e}\")\n",
    "print(f\"95%分位:      {np.nanquantile(abs_rel_err_clean, 0.95):.3e}\")\n",
    "print(f\"99%分位:      {np.nanquantile(abs_rel_err_clean, 0.99):.3e}\")\n",
    "\n",
    "# === 3. 可视化误差分布 ===\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(abs_rel_err_clean, bins=50, log=True, color='steelblue', alpha=0.7)\n",
    "plt.xlabel(\"Relative Error\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.title(\"Distribution of relative error (top 5 removed)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaa1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fddf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbc60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def read_run_mock(path: str) -> pd.DataFrame | None:\n",
    "    df_lens = None\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        if \"lens\" in f and \"table\" in f[\"lens\"]:\n",
    "            data = f[\"lens\"][\"table\"][:]  # structured array\n",
    "            df_lens = pd.DataFrame.from_records(data)\n",
    "    return df_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_run_mock(\"../chains/chains_511lens_20250905T132621Z.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['logM_halo'], bins=30)\n",
    "print(np.min(df['logM_halo']), np.max(df['logM_halo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a98c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdc533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ea64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0a957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013b36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bbe04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1908ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce591d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a0893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82ab62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e83e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 修改为你实际使用的 Aeta 文件路径 ===\n",
    "aeta_file = \"../aeta_tables/Aeta_20250901T173507Z.h5\"  # 或你实际加载的文件\n",
    "\n",
    "with h5py.File(aeta_file, \"r\") as f:\n",
    "    mu_DM_grid = f[\"grids/mu_DM_grid\"][...]\n",
    "    A_grid = f[\"grids/A_grid\"][...]\n",
    "\n",
    "# 转为 numpy 数组\n",
    "mu_DM_grid = np.asarray(mu_DM_grid)\n",
    "A_grid = np.asarray(A_grid)\n",
    "\n",
    "# 归一化到最大值 = 1，便于比较形状\n",
    "A_grid_norm = A_grid / np.max(A_grid)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(mu_DM_grid, A_grid_norm, label=\"A(μ_DM) (normalized)\")\n",
    "plt.axvline(12.91, color=\"red\", linestyle=\"--\", label=\"True μ_DM = 12.91\")\n",
    "plt.axvline(12.5, color=\"blue\", linestyle=\"--\", label=\"Posterior peak ~ 12.5\")\n",
    "plt.xlabel(r\"$\\mu_{\\rm DM}$\")\n",
    "plt.ylabel(r\"$A(\\mu_{\\rm DM})$ (normalized)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.title(\"Selection-function normalization A(μ_DM)\")\n",
    "plt.show()\n",
    "\n",
    "# 打印数值对比\n",
    "from numpy import interp\n",
    "A_125 = float(np.interp(12.5, mu_DM_grid, A_grid_norm))\n",
    "A_129 = float(np.interp(12.91, mu_DM_grid, A_grid_norm))\n",
    "print(f\"A(12.5) = {A_125:.3g}, A(12.91) = {A_129:.3g}, ratio = {A_129/A_125:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba891f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915454b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b15f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_pairplot_with_truth(\n",
    "    hdf5_file,\n",
    "    discard=1000,\n",
    "    thin=10,\n",
    "    labels=None,\n",
    "    truths=None,\n",
    "    width_factor=3,\n",
    "    tol=100\n",
    "):\n",
    "    \"\"\"\n",
    "    自动剔除“僵尸链”，然后画 posterior 可视化。\n",
    "    - n_dim == 1 时：画单变量直方图 + KDE（不使用 PairGrid）\n",
    "    - n_dim >= 2 时：使用 PairGrid（保留原逻辑）\n",
    "    tol: 判断链子是否不动的标准差阈值\n",
    "    \"\"\"\n",
    "    # 读取链\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    raw_chain = backend.get_chain(discard=discard, thin=thin, flat=False)  # (n_steps, n_chains, n_dim)\n",
    "    if raw_chain.ndim != 3:\n",
    "        raise ValueError(f\"Unexpected chain shape: {raw_chain.shape}\")\n",
    "    n_steps, n_chains, n_dim = raw_chain.shape\n",
    "\n",
    "    # 默认标签\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(n_dim)]\n",
    "    if len(labels) != n_dim:\n",
    "        raise ValueError(f\"labels length ({len(labels)}) must equal n_dim ({n_dim}).\")\n",
    "\n",
    "    # # 识别僵尸链（逐参数 std > tol）\n",
    "    # chain_std = np.std(raw_chain, axis=0)  # (n_chains, n_dim)\n",
    "    # valid_mask = np.all(chain_std > tol, axis=1)\n",
    "    # n_zombies = int(np.sum(~valid_mask))\n",
    "    # print(f\"总链数: {n_chains}, 剔除 {n_zombies} 条僵尸链\")\n",
    "\n",
    "    # # 如果全被判定为僵尸链，则回退：全部保留，但发出警告\n",
    "    # if not np.any(valid_mask):\n",
    "    #     print(\"警告：所有链都被判定为僵尸链。将回退为使用全部链（不做剔除）。\")\n",
    "    #     valid_mask = np.ones(n_chains, dtype=bool)\n",
    "\n",
    "    # 展平有效链\n",
    "    filtered_chain = raw_chain.reshape(-1, n_dim)\n",
    "\n",
    "    # 处理 truths：允许 None / 标量 / 列表\n",
    "    if truths is None:\n",
    "        truths_list = None\n",
    "    else:\n",
    "        if np.isscalar(truths):\n",
    "            truths_list = [float(truths)]\n",
    "        else:\n",
    "            truths_list = list(truths)\n",
    "        if len(truths_list) < n_dim:\n",
    "            # 不足则忽略\n",
    "            truths_list = None\n",
    "\n",
    "    # 一维情况：直接画单变量直方图 + KDE\n",
    "    if n_dim == 1:\n",
    "        x = filtered_chain[:, 0]\n",
    "        label = labels[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        sns.histplot(x=x, kde=True, bins=30, color=\"#518FDF\", edgecolor=\"none\", alpha=0.5, ax=ax)\n",
    "\n",
    "        mean_x = float(np.mean(x))\n",
    "        std_x = float(np.std(x, ddof=1)) if x.size > 1 else 0.0\n",
    "\n",
    "        # 自动限幅：以样本均值±width_factor*std\n",
    "        if std_x > 0:\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "        # 真值线\n",
    "        if truths_list is not None:\n",
    "            ax.axvline(truths_list[0], color=\"red\", linestyle=\"--\", lw=1, label=\"Truth\")\n",
    "\n",
    "        # 均值线（可选）\n",
    "        ax.axvline(mean_x, color=\"#46B48E\", linestyle=\"-\", lw=1, label=\"Mean\")\n",
    "\n",
    "        ax.set_xlabel(label, fontsize=12)\n",
    "        ax.set_ylabel(\"Count\", fontsize=12)\n",
    "        ax.legend(loc=\"best\", fontsize=16)\n",
    "        plt.title(f\"Posterior (1D)\\n discard={discard}, samples={x.size}\", fontsize=13)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # 多维情况：保留原 PairGrid 逻辑并做健壮性修正\n",
    "    df = pd.DataFrame(filtered_chain, columns=labels)\n",
    "    means = df.mean()\n",
    "    stds = df.std(ddof=1)\n",
    "\n",
    "    g = sns.PairGrid(df, corner=True)\n",
    "    # 散点\n",
    "    g.map_lower(sns.scatterplot, s=2, alpha=0.4, color=\"#46B48E\")\n",
    "    # 对角：直方图 + KDE\n",
    "    g.map_diag(sns.histplot, kde=True, bins=20, color=\"#518FDF\", edgecolor=\"none\", alpha=0.5)\n",
    "    # 叠加等密度区（下三角）\n",
    "    def _kde_fill(x, y, **kwargs):\n",
    "        try:\n",
    "            sns.kdeplot(\n",
    "                x=x, y=y, fill=True, thresh=1,\n",
    "                levels=[0.001, 0.05, 0.35, 1],  # 99.9%, 95%, 65%（最后1只是形状闭合）\n",
    "                colors=[\"gold\", \"skyblue\", \"midnightblue\", \"none\"],\n",
    "                alpha=0.6, **kwargs\n",
    "            )\n",
    "        except Exception:\n",
    "            # KDE 失败时忽略，不中断\n",
    "            pass\n",
    "    g.map_lower(_kde_fill)\n",
    "\n",
    "    # 轴限与真值标记\n",
    "    for i in range(n_dim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            if np.isfinite(std_x) and std_x > 0:\n",
    "                ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "            if i == j:\n",
    "                if truths_list is not None:\n",
    "                    ax.axvline(truths_list[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "            else:\n",
    "                mean_y = means[labels[i]]\n",
    "                std_y = stds[labels[i]]\n",
    "                if np.isfinite(std_y) and std_y > 0:\n",
    "                    ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "                if truths_list is not None:\n",
    "                    ax.plot(truths_list[j], truths_list[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "\n",
    "    # 在标题上标出最大概率值\n",
    "    print(\"参数均值 ± 标准差：\")\n",
    "    for d in range(n_dim):\n",
    "        print(f\"  {labels[d]}: {means[d]:.2f} ± {stds[d]:.2f}\")\n",
    "    # print(\"参数中位数 ± 1σ：\")\n",
    "    # for d in range(n_dim):\n",
    "\n",
    "    # 置信区图例（放在合适位置，避免越界）\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=\"gold\", label=\"~99.9% iso-density\"),\n",
    "        mpatches.Patch(color=\"skyblue\", label=\"~95% iso-density\"),\n",
    "        mpatches.Patch(color=\"midnightblue\", label=\"~65% iso-density\"),\n",
    "    ]\n",
    "    plt.legend(handles=legend_patches, loc='upper right', bbox_to_anchor=(1.05, 1.05),\n",
    "               title=\"Iso-density\", fontsize=10)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Posterior Distributions with Truths\\n discard={discard}, samples={filtered_chain.shape[0]}\",\n",
    "        fontsize=14, y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ef871",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"../chains/chains_199lens_noeta_20250906T181603Z_7175.h5\",\n",
    "    discard=1000,\n",
    "    thin=10,\n",
    "    truths=[12.91, 0.37, 0.15],  # 若实际是一维，这里可给标量 truths=12.91 或 truths=[12.91]\n",
    "    width_factor=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c02966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11a90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cf661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e8666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa06661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "\n",
    "def plot_pairplot_with_truth(\n",
    "    hdf5_file, discard=1000, thin=10, labels=None, truths=None, width_factor=1.5\n",
    "):\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    samples = backend.get_chain(discard=discard, thin=thin, flat=True)\n",
    "    print(f\"[INFO] samples.shape = {samples.shape}\")\n",
    "\n",
    "    ndim = samples.shape[1]\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "\n",
    "    # PairGrid: 自定义绘图\n",
    "    g = sns.PairGrid(df, corner=True, diag_sharey=False)\n",
    "    g.map_lower(sns.kdeplot, fill=False, levels=[0.393, 0.864], color=\"C0\", linewidths=1)\n",
    "    g.map_lower(sns.scatterplot, s=5, alpha=0.1, color=\"gray\")\n",
    "    # g.map_diag(sns.kdeplot, fill=True, color=\"skyblue\", linewidth=1)\n",
    "    g.map_diag(sns.histplot, kde=True, stat=\"density\", color=\"C0\", linewidth=1)\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "            # if i == j:\n",
    "            #     if truths:\n",
    "            #         ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "\n",
    "            #         # 添加 65% 和 95% 的置信区间线\n",
    "            #         vals = df[labels[i]].values\n",
    "            #         low_65, high_65 = np.percentile(vals, [17.5, 82.5])\n",
    "            #         low_95, high_95 = np.percentile(vals, [2.5, 97.5])\n",
    "            #         ax.axvline(low_65, color=\"orange\", linestyle=\":\", lw=1)\n",
    "            #         ax.axvline(high_65, color=\"orange\", linestyle=\":\", lw=1)\n",
    "            #         ax.axvline(low_95, color=\"green\", linestyle=\":\", lw=1)\n",
    "            #         ax.axvline(high_95, color=\"green\", linestyle=\":\", lw=1)\n",
    "            # else:\n",
    "            #     mean_y = means[labels[i]]\n",
    "            #     std_y = stds[labels[i]]\n",
    "            #     ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "\n",
    "            if truths and i!= j:\n",
    "                ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e238082",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"../chains/chains_eta_new_table.h5\",\n",
    "    discard=100,\n",
    "    thin=10,\n",
    "    labels=[r\"$\\mu_{DM0}$\", r\"$\\beta_{DM}$\", r\"$\\sigma_{DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    truths=[12.91, 2.04, 0.37, 0.1, 0.05],\n",
    "    width_factor=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8293376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2182690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c75b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98567c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40947a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_pairplot_with_truth(hdf5_file, discard=1000, thin=10, labels=None, truths=None, width_factor=3, tol=1e-6):\n",
    "    \"\"\"\n",
    "    自动剔除“僵尸链”，然后画 pairplot。\n",
    "    tol: 判断链子是否不动的标准差阈值\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    \n",
    "    # shape: (n_steps, n_chains, n_dim)\n",
    "    raw_chain = backend.get_chain(discard=discard, thin=thin, flat=False)\n",
    "    n_steps, n_chains, n_dim = raw_chain.shape\n",
    "    \n",
    "    # 找出正常链：每条链每个参数的 std 都要 > tol\n",
    "    chain_std = np.std(raw_chain, axis=0)  # (n_chains, n_dim)\n",
    "    valid_mask = np.all(chain_std > tol, axis=1)\n",
    "    print(f\"总链数: {n_chains}, 剔除 {np.sum(~valid_mask)} 条僵尸链\")\n",
    "    \n",
    "    # 保留有效链并展平\n",
    "    filtered_chain = raw_chain[:, valid_mask, :].reshape(-1, n_dim)\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(n_dim)]\n",
    "    df = pd.DataFrame(filtered_chain, columns=labels)\n",
    "    \n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    \n",
    "    g = sns.PairGrid(df, corner=True)\n",
    "    g.map_lower(sns.scatterplot, s=2, alpha=0.4, color=\"#46B48E\")\n",
    "    g.map_diag(sns.histplot, kde=True, bins=20, color=\"#518FDF\", edgecolor=\"none\", alpha=0.5)\n",
    "    g.map_lower(lambda x, y, **kwargs: sns.kdeplot(\n",
    "        x=x, y=y, fill=True, thresh=1,\n",
    "        levels=[0.001, 0.05, 0.35, 1],  # 99.9%, 95%, 65%\n",
    "        colors=[\"gold\", \"skyblue\", \"midnightblue\"],\n",
    "        alpha=0.6, **kwargs\n",
    "    ))\n",
    "    \n",
    "    for i in range(n_dim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "            if i == j:\n",
    "                if truths:\n",
    "                    ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "            else:\n",
    "                mean_y = means[labels[i]]\n",
    "                std_y = stds[labels[i]]\n",
    "                ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "                if truths:\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "    \n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=\"gold\", label=\"99.9% CI\"),\n",
    "        mpatches.Patch(color=\"skyblue\", label=\"95% CI\"),\n",
    "        mpatches.Patch(color=\"midnightblue\", label=\"65% CI\"),\n",
    "    ]\n",
    "    legend = plt.legend(handles=legend_patches, loc='center right', bbox_to_anchor=(2.5, 2.5), title=\"Confidence Regions\", fontsize=15)\n",
    "    plt.gcf().add_artist(legend)\n",
    "    \n",
    "    plt.suptitle(f\"Posterior Distributions with True Values, discard={discard}, number#={filtered_chain.shape[0]}\", fontsize=14, y=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"../chains/chains_199lens_noeta.h5\",\n",
    "    discard=1000,\n",
    "    thin=10,\n",
    "    truths=[12.91, 0.37, 0.15],\n",
    "    width_factor=4\n",
    "    # labels=[r\"$\\mu_{DM0}$\", r\"$\\beta_{DM}$\", r\"$\\sigma_{DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    # max_steps=None # 可选\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03131f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ac931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84722c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a00338",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.1, 0.2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff088046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d491b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1 * u.min * 10 * 10 * 10\n",
    "print(\"h:\", f\"{t.to(u.h):.2f}, day\", f\"{t.to(u.d):.2f}\")  # 输出转换后的时间单位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5f2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7f506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402fa35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "\n",
    "def plot_chains(hdf5_file, discard=0, thin=1, labels=None, max_steps=None):\n",
    "    \"\"\"\n",
    "    画出 emcee 的采样链条（trace plot），每个 walker 一条线。\n",
    "\n",
    "    参数：\n",
    "    - hdf5_file: HDFBackend 文件路径\n",
    "    - discard: 丢弃 burn-in 步数\n",
    "    - thin: 稀疏采样间隔\n",
    "    - labels: 每个参数的名字（list[str]）\n",
    "    - max_steps: 最多显示多少步（用于调试）\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    chain = backend.get_chain(discard=discard, thin=thin)  # shape: (nsteps, nwalkers, ndim)\n",
    "\n",
    "    nsteps, nwalkers, ndim = chain.shape\n",
    "    print(ndim)\n",
    "\n",
    "    if max_steps:\n",
    "        chain = chain[:max_steps]\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim, 1, figsize=(10, 2.2 * ndim), sharex=True)\n",
    "\n",
    "    if ndim == 1:\n",
    "        ax = axes\n",
    "        for walker in range(nwalkers):\n",
    "            ax.plot(chain[:, walker, 0], alpha=0.4, lw=0.5)\n",
    "        ax.set_ylabel(labels[0])\n",
    "        return 0\n",
    "        \n",
    "    for i in range(ndim):\n",
    "        ax = axes[i]\n",
    "        for walker in range(nwalkers):\n",
    "            ax.plot(chain[:, walker, i], alpha=0.4, lw=0.5)\n",
    "        ax.set_ylabel(labels[i])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Step number\")\n",
    "    plt.suptitle(\"MCMC Chains (Trace Plot)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chains(\n",
    "    hdf5_file=\"../chains/chains_interp_debug.h5.h5\",\n",
    "    discard=0,\n",
    "    thin=1,\n",
    "    # labels=[r\"$\\mu_{DM0}$\", r\"$\\beta_{DM}$\", r\"$\\sigma_{DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    max_steps=None # 可选\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53587f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from sl_inference_3Dinfer.cached_A import cached_A_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_A_interp(12.9,0.3,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fbef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_pairplot_with_truth(hdf5_file, discard=1000, thin=10, labels=None, truths=None, width_factor=1.5):\n",
    "\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    samples = backend.get_chain(discard=discard, thin=thin, flat=True)\n",
    "    print(samples.shape)\n",
    "    ndim = samples.shape[1]\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    g = sns.PairGrid(df, corner=True)\n",
    "    g.map_lower(sns.scatterplot, s=2, alpha=0.4, color=\"#46B48E\")  # steelblue\n",
    "    g.map_diag(sns.histplot, kde=True, bins=20, color=\"#518FDF\", edgecolor=\"none\", alpha=0.5, )\n",
    "\n",
    "    g.map_lower(lambda x, y, **kwargs: sns.kdeplot(x=x, y=y, fill=True, thresh=1,\n",
    "                                                    levels=[0.01, 0.05, 0.65,1],  # 95%, 65%\n",
    "                                                    colors=[\"gold\", \"skyblue\", \"midnightblue\"], #only need two colors for two levels\n",
    "                                                    alpha=0.5, **kwargs))\n",
    "\n",
    "    logp = backend.get_log_prob(discard=discard, thin=thin, flat=True)\n",
    "    for i in range(ndim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "\n",
    "            # 设置 xlim 和 ylim 居中\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "            if i == j:\n",
    "                if truths:\n",
    "                    ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "                # ax.set_title(f\"best: {best_sample[i]:.3f}\", fontsize=10)\n",
    "            else:\n",
    "                mean_y = means[labels[i]]\n",
    "                std_y = stds[labels[i]]\n",
    "                ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "                if truths:\n",
    "                    # 添加红色十字真值点\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "    legend_patches = [\n",
    "    mpatches.Patch(color=\"gold\", label=\"99.9% CI\"),\n",
    "    mpatches.Patch(color=\"skyblue\", label=\"95% CI\"),\n",
    "    mpatches.Patch(color=\"midnightblue\", label=\"65% CI\"),\n",
    "    ]\n",
    "    legend = plt.legend(handles=legend_patches, loc='center right', bbox_to_anchor=(3.5, 1), title=\"Confidence Regions\", fontsize=10)\n",
    "    plt.gcf().add_artist(legend)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"../chains/chains_199lens_noeta.h5\",\n",
    "    discard=800,\n",
    "    thin=1,\n",
    "    truths=[12.91, 0.15],\n",
    "    width_factor=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c1221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# ===== 读表并准备数值 =====\n",
    "df = pd.read_csv(\"../A_eta_table_alpha.csv\")\n",
    "for col in [\"mu_DM\", \"beta_DM\", \"sigma_DM\", \"alpha\", \"A\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"mu_DM\", \"beta_DM\", \"sigma_DM\", \"alpha\", \"A\"])\n",
    "\n",
    "# 为数值稳定起见，对 A 取 log10 再插值（A 可能跨好几个量级）\n",
    "df[\"A_pos\"] = df[\"A\"].clip(lower=1e-300)\n",
    "df[\"logA\"] = np.log10(df[\"A_pos\"])\n",
    "\n",
    "# ===== 重建规则 4D 网格 =====\n",
    "mu_vals    = np.sort(df[\"mu_DM\"].unique())\n",
    "beta_vals  = np.sort(df[\"beta_DM\"].unique())\n",
    "sigma_vals = np.sort(df[\"sigma_DM\"].unique())\n",
    "alpha_vals = np.sort(df[\"alpha\"].unique())\n",
    "\n",
    "# 按 (mu, beta, sigma, alpha) 的字典序排序后 reshape 成 4D\n",
    "df_sorted = df.sort_values([\"mu_DM\", \"beta_DM\", \"sigma_DM\", \"alpha\"])\n",
    "expected = len(mu_vals) * len(beta_vals) * len(sigma_vals) * len(alpha_vals)\n",
    "assert len(df_sorted) == expected, \"网格不完整：CSV 里缺项，无法直接做规则网格插值\"\n",
    "\n",
    "A4_log = df_sorted[\"logA\"].to_numpy().reshape(\n",
    "    len(mu_vals), len(beta_vals), len(sigma_vals), len(alpha_vals)\n",
    ")\n",
    "\n",
    "# 4D 插值器（对 log10(A)）\n",
    "interp4d = RegularGridInterpolator(\n",
    "    (mu_vals, beta_vals, sigma_vals, alpha_vals),\n",
    "    A4_log,\n",
    "    bounds_error=False,\n",
    "    fill_value=np.nan,\n",
    ")\n",
    "\n",
    "# ===== 画二维切片的插值等高图 =====\n",
    "def plot_slice_interpolated(\n",
    "    x=\"mu_DM\",\n",
    "    y=\"beta_DM\",\n",
    "    fixed=None,\n",
    "    n=200,\n",
    "    cmap=\"viridis\",\n",
    "    levels=20,\n",
    "    use_imshow=False,   # True 用 imshow 视觉更像“热力图”，False 用 contourf\n",
    "):\n",
    "    \"\"\"\n",
    "    x, y: 选择要作为横纵轴的两个维度（'mu_DM'/'beta_DM'/'sigma_DM'/'alpha'）\n",
    "    fixed: dict，给出其余两维的固定值，如 {'sigma_DM': 0.37, 'alpha': 0.15}\n",
    "           若未提供，则自动取该维网格的中位数\n",
    "    n:    插值后的细网格边长\n",
    "    \"\"\"\n",
    "    axes = {\n",
    "        \"mu_DM\": mu_vals,\n",
    "        \"beta_DM\": beta_vals,\n",
    "        \"sigma_DM\": sigma_vals,\n",
    "        \"alpha\": alpha_vals,\n",
    "    }\n",
    "\n",
    "    # 默认把未指定的固定维取中位数（就近贴合到网格内部）\n",
    "    if fixed is None:\n",
    "        fixed = {}\n",
    "    fixed_full = {}\n",
    "    for k, arr in axes.items():\n",
    "        if k in (x, y):\n",
    "            continue\n",
    "        if k in fixed:\n",
    "            fixed_full[k] = float(fixed[k])\n",
    "        else:\n",
    "            fixed_full[k] = float(arr[len(arr)//2])\n",
    "\n",
    "    # 构造细网格\n",
    "    x_lin = np.linspace(axes[x].min(), axes[x].max(), n)\n",
    "    y_lin = np.linspace(axes[y].min(), axes[y].max(), n)\n",
    "    X, Y = np.meshgrid(x_lin, y_lin, indexing=\"ij\")\n",
    "\n",
    "    # 组装 4D 坐标点（按 [mu, beta, sigma, alpha] 顺序）\n",
    "    order = [\"mu_DM\", \"beta_DM\", \"sigma_DM\", \"alpha\"]\n",
    "    pts = np.zeros((X.size, 4), dtype=float)\n",
    "    name_to_axis = {x: (X, True), y: (Y, True)}\n",
    "    for i, name in enumerate(order):\n",
    "        if name in name_to_axis:\n",
    "            pts[:, i] = name_to_axis[name][0].ravel()\n",
    "        else:\n",
    "            pts[:, i] = fixed_full[name]\n",
    "\n",
    "    Z_logA = interp4d(pts).reshape(X.shape)   # log10(A) 的插值\n",
    "    # 可选：把 nan 区域屏蔽\n",
    "    Z_masked = np.ma.masked_invalid(Z_logA)\n",
    "\n",
    "    # 绘图\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 5.2))\n",
    "    if use_imshow:\n",
    "        im = ax.imshow(\n",
    "            Z_masked.T,\n",
    "            origin=\"lower\",\n",
    "            extent=[x_lin.min(), x_lin.max(), y_lin.min(), y_lin.max()],\n",
    "            aspect=\"auto\",\n",
    "            cmap=cmap,\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "    else:\n",
    "        cs = ax.contourf(X, Y, Z_masked, levels=levels, cmap=cmap)\n",
    "        cbar = fig.colorbar(cs, ax=ax)\n",
    "\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    title_fixed = \", \".join(f\"{k}={fixed_full[k]:.3g}\" for k in order if k not in (x, y))\n",
    "    ax.set_title(rf\"{x} vs {y} @ {title_fixed}  (showing $\\log_{{10}} A$)\")\n",
    "    cbar.set_label(r\"$\\log_{10} A$\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===== 使用示例 =====\n",
    "# 1) 固定 sigma_DM 和 alpha，画 mu_DM vs beta_DM 的插值切片\n",
    "plot_slice_interpolated(\n",
    "    x=\"mu_DM\",\n",
    "    y=\"beta_DM\",\n",
    "    fixed={\"sigma_DM\": 0.37, \"alpha\": 0.15},\n",
    "    n=220,\n",
    "    levels=24,\n",
    ")\n",
    "\n",
    "# 2) 固定 beta_DM 和 sigma_DM，画 mu_DM vs alpha\n",
    "plot_slice_interpolated(\n",
    "    x=\"mu_DM\",\n",
    "    y=\"alpha\",\n",
    "    fixed={\"beta_DM\": 2.0, \"sigma_DM\": 0.37},\n",
    "    n=220,\n",
    "    levels=24,\n",
    ")\n",
    "\n",
    "# 3) 固定 mu_DM 和 alpha，画 beta_DM vs sigma_DM，并用 imshow 风格\n",
    "plot_slice_interpolated(\n",
    "    x=\"beta_DM\",\n",
    "    y=\"sigma_DM\",\n",
    "    fixed={\"mu_DM\": 12.9, \"alpha\": 0.15},\n",
    "    n=220,\n",
    "    use_imshow=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6505f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2250e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b373bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0a0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_pairplot_with_truth(hdf5_file, discard=1000, thin=10, labels=None, truths=None, width_factor=1.5):\n",
    "    \"\"\"\n",
    "    使用 Seaborn pairplot 绘制后验图，并以变量均值为中心设置 xlim/ylim，同时标出真值。\n",
    "\n",
    "    参数：\n",
    "    - hdf5_file: emcee 的 HDF5 后端路径\n",
    "    - discard: 前期丢弃步数\n",
    "    - thin: 抽样间隔\n",
    "    - labels: 变量标签列表\n",
    "    - truths: list[float]，真值点，会在每个图上标注\n",
    "    - width_factor: 控制坐标轴范围宽度因子\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    samples = backend.get_chain(discard=discard, thin=thin, flat=True)\n",
    "    print(samples.shape)\n",
    "\n",
    "    ndim = samples.shape[1]\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    # fig = plt.figure(dpi=300)\n",
    "    # g = sns.pairplot(df, corner=True, diag_kind=\"hist\", plot_kws=dict(s=5, alpha=0.2))\n",
    "    # g.map_lower(sns.scatterplot, s=5, alpha=0.2)\n",
    "    # g.map_diag(sns.histplot, kde=True)  # ✅ 这里 hist+KDE 都画\n",
    "    # 创建 PairGrid\n",
    "    g = sns.PairGrid(df, corner=True)\n",
    "\n",
    "    # 下三角：散点图（柔和蓝色点）\n",
    "    g.map_lower(sns.scatterplot, s=2, alpha=0.4, color=\"#46B48E\")  # steelblue\n",
    "\n",
    "    # 对角线：直方图 + KDE（海军蓝线 + 浅灰柱）\n",
    "    g.map_diag(\n",
    "        sns.histplot,\n",
    "        kde=True,\n",
    "        bins=20,\n",
    "        color=\"#518FDF\",         # lightsteelblue（bar）\n",
    "        # kde_kws={\"color\": \"#1E3F66\", \"lw\": 1.2},  # navyish KDE\n",
    "        edgecolor=\"none\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    # 下三角 KDE fill：置信区间（深蓝填充）\n",
    "    g.map_lower(\n",
    "        lambda x, y, **kwargs: sns.kdeplot(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            fill=True,\n",
    "            thresh=1,\n",
    "            levels=[0.01, 0.05, 0.65,1],  # 95%, 65%\n",
    "            colors=[\"gold\", \"skyblue\", \"midnightblue\"], #only need two colors for two levels\n",
    "            alpha=0.5,\n",
    "            # shade=True,\n",
    "            # palette=\"crest\",\n",
    "            # linewidths=0,\n",
    "            **kwargs\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    # for i in range(ndim):\n",
    "    #     for j in range(i + 1):\n",
    "    #         ax = g.axes[i, j]\n",
    "    #         if ax is None:\n",
    "    #             continue\n",
    "\n",
    "    #         # 设置 xlim 和 ylim 居中\n",
    "    #         mean_x = means[labels[j]]\n",
    "    #         std_x = stds[labels[j]]\n",
    "    #         ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "    #         if i == j:\n",
    "    #             if truths:\n",
    "    #                 ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "    #         else:\n",
    "    #             mean_y = means[labels[i]]\n",
    "    #             std_y = stds[labels[i]]\n",
    "    #             ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "    #             if truths:\n",
    "    #                 # 添加红色十字真值点\n",
    "    #                 ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "    # 找到后验最大点\n",
    "    logp = backend.get_log_prob(discard=discard, thin=thin, flat=True)\n",
    "    # best_index = np.argmax(logp)\n",
    "    # best_sample = samples[best_index]  # shape (ndim,)\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "\n",
    "            # 设置 xlim 和 ylim 居中\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "            if i == j:\n",
    "                if truths:\n",
    "                    ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "                # ax.set_title(f\"best: {best_sample[i]:.3f}\", fontsize=10)\n",
    "            else:\n",
    "                mean_y = means[labels[i]]\n",
    "                std_y = stds[labels[i]]\n",
    "                ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "                if truths:\n",
    "                    # 添加红色十字真值点\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "\n",
    "    # plt.savefig(\"post.png\",dpi=300)\n",
    "    # plt.title(\"Pairplot with Truth Values\", fontsize=16)\n",
    "    # plt.suptitle(f\"Posterior Distributions with True Values,discard={discard},number#={samples.shape[0]}\", fontsize=18, y=1)\n",
    "\n",
    "\n",
    "    \n",
    "    # 添加自定义图例说明置信区间\n",
    "    legend_patches = [\n",
    "    mpatches.Patch(color=\"gold\", label=\"99.9% CI\"),\n",
    "    mpatches.Patch(color=\"skyblue\", label=\"95% CI\"),\n",
    "    mpatches.Patch(color=\"midnightblue\", label=\"65% CI\"),\n",
    "    ]\n",
    "    # # fig = plt.gcf()\n",
    "    # fig.legend(\n",
    "    #     handles=legend_patches,\n",
    "    #     loc=\"center right\",       # 相对于整个图像\n",
    "    #     bbox_to_anchor=(0.5, 1.02),\n",
    "    #     ncol=1,\n",
    "    #     fontsize=20\n",
    "    # )\n",
    "\n",
    "    # plt.legend(handles=legend_patches, title=\"Confidence Regions\", loc=\"upper left\", borderaxespad=0.)\n",
    "    \n",
    "    legend = plt.legend(handles=legend_patches, loc='center right', bbox_to_anchor=(3.5, 1), title=\"Confidence Regions\", fontsize=10)\n",
    "    plt.gcf().add_artist(legend)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"../chains/chains.h5\",\n",
    "    discard=3000,\n",
    "    thin=1,\n",
    "    # labels=[r\"$\\mu_{DM0}$\", r\"$\\beta_{DM}$\", r\"$\\sigma_{DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],  # Example parameter names\n",
    "    # truths=[12.575, 0.381, 0.073, 0.075],\n",
    "    # truths=[12.91, 2.04, 0.37, 0.1, 0.05],\n",
    "    truths=[12.91, 0.3],\n",
    "    width_factor=3,\n",
    "    # max_steps=4000  # 可选，None表示不限制最大步数\n",
    "\n",
    ")\n",
    "#  np.array([12.5, 0.38, 0.05, 0.05])\n",
    "\n",
    "# true\n",
    "# (12.484790712318109,\n",
    "#  0.4365785108229181,\n",
    "#  0.07376295948617051,\n",
    "#  0.0748739106362481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77db237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bfb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a1950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dec1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7c4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb6bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69418aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emcee.backends import HDFBackend\n",
    "\n",
    "backend = HDFBackend(\"../chains/chains_eta_new_table_no_eta_variedms1.h5\", read_only=True)\n",
    "print(\"Chain shape:\", backend.get_chain().shape)  # → (nsteps, nwalkers, ndim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97246379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70915328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a14cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd838af",
   "metadata": {},
   "outputs": [],
   "source": [
    "(12.455266668203242,\n",
    " 0.4285884092108674,\n",
    " 0.08515148383256033,\n",
    " 0.06776620586484147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(\"../../chains_eta.h5\", \"r\") as f:\n",
    "    print(list(f.keys()))               # 应该有 mcmc\n",
    "    print(list(f[\"mcmc\"].keys()))        # 应该有 chain, log_prob\n",
    "    print(f[\"mcmc/chain\"].shape)         # (n_steps, n_walkers, ndim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b38dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6671f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设数据已经存在 CSV 文件中，这里直接读取 repo 中的文件\n",
    "file_path = '../A_eta_table_alpha0.01.csv'\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(file_path, sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc23615",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['mu_DM'], df['alpha'], c=df['A'], cmap='viridis', s=10, )\n",
    "# plt.xlim(12,13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(df['A']),np.min(df['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda6fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从数据中提取坐标和值\n",
    "mu_DM_vals = df['mu_DM'].values\n",
    "alpha_vals = df['alpha'].values  \n",
    "A_vals = df['A'].values\n",
    "\n",
    "# 创建更密集的网格用于插值\n",
    "mu_DM_min, mu_DM_max = mu_DM_vals.min(), mu_DM_vals.max()\n",
    "alpha_min, alpha_max = alpha_vals.min(), alpha_vals.max()\n",
    "\n",
    "# 创建规则网格\n",
    "mu_DM_grid = np.linspace(mu_DM_min, mu_DM_max, 100)\n",
    "alpha_grid = np.linspace(alpha_min, alpha_max, 100)\n",
    "mu_DM_mesh, alpha_mesh = np.meshgrid(mu_DM_grid, alpha_grid)\n",
    "\n",
    "# 进行二维插值\n",
    "points = np.column_stack((mu_DM_vals, alpha_vals))\n",
    "A_interpolated = griddata(points, A_vals, (mu_DM_mesh, alpha_mesh), method='cubic')\n",
    "\n",
    "print(f\"原始数据点数量: {len(df)}\")\n",
    "print(f\"插值网格大小: {mu_DM_mesh.shape}\")\n",
    "print(f\"mu_DM 范围: [{mu_DM_min:.1f}, {mu_DM_max:.1f}]\")\n",
    "print(f\"alpha 范围: [{alpha_min:.3f}, {alpha_max:.3f}]\")\n",
    "print(f\"A 值范围: [{A_vals.min():.6f}, {A_vals.max():.6f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88195fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建包含多个子图的综合可视化\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 子图1: 原始数据散点图\n",
    "scatter = axes[0,0].scatter(mu_DM_vals, alpha_vals, c=A_vals, cmap='viridis', s=50)\n",
    "axes[0,0].set_xlabel('mu_DM')\n",
    "axes[0,0].set_ylabel('alpha')\n",
    "axes[0,0].set_title('original data points')\n",
    "plt.colorbar(scatter, ax=axes[0,0], label='A值')\n",
    "\n",
    "# 子图2: 插值结果等高线图\n",
    "contour = axes[0,1].contourf(mu_DM_mesh, alpha_mesh, A_interpolated, levels=20, cmap='viridis')\n",
    "axes[0,1].scatter(mu_DM_vals, alpha_vals, c='red', s=0.01, alpha=0.7, label='original data points')\n",
    "axes[0,1].set_xlabel('mu_DM')\n",
    "axes[0,1].set_ylabel('alpha')\n",
    "axes[0,1].set_title('interpolated results (contour)')\n",
    "axes[0,1].legend()\n",
    "plt.colorbar(contour, ax=axes[0,1], label='A values')\n",
    "\n",
    "# 子图3: 插值结果3D表面图(投影到2D)\n",
    "im = axes[1,0].imshow(A_interpolated, extent=[mu_DM_min, mu_DM_max, alpha_min, alpha_max], \n",
    "                      origin='lower', aspect='auto', cmap='viridis')\n",
    "axes[1,0].scatter(mu_DM_vals, alpha_vals, c='red', s=0.01, alpha=0.7)\n",
    "axes[1,0].set_xlabel('mu_DM')\n",
    "axes[1,0].set_ylabel('alpha')\n",
    "axes[1,0].set_title('interpolated results (heatmap)')\n",
    "plt.colorbar(im, ax=axes[1,0], label='A values')\n",
    "\n",
    "# 子图4: 沿某个切面的比较\n",
    "# 选择一个固定的mu_DM值进行切面分析\n",
    "mu_DM_slice = 13.0\n",
    "idx_original = np.abs(mu_DM_vals - mu_DM_slice) < 0.1\n",
    "alpha_slice_orig = alpha_vals[idx_original]\n",
    "A_slice_orig = A_vals[idx_original]\n",
    "\n",
    "# 从插值结果中提取对应的切面\n",
    "mu_idx = np.argmin(np.abs(mu_DM_grid - mu_DM_slice))\n",
    "A_slice_interp = A_interpolated[:, mu_idx]\n",
    "\n",
    "axes[1,1].plot(alpha_grid, A_slice_interp, 'b-', linewidth=2, label=f'interpolated results (mu_DM≈{mu_DM_slice})')\n",
    "axes[1,1].scatter(alpha_slice_orig, A_slice_orig, c='red', s=0.5, zorder=5, label='original data points')\n",
    "axes[1,1].set_xlabel('alpha')\n",
    "axes[1,1].set_ylabel('A values')\n",
    "axes[1,1].set_title(f'mu_DM = {mu_DM_slice} ')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b0682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c35a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Try to load the A(eta) table from several likely locations ----------\n",
    "candidate_paths = [\n",
    "    \"../A_eta_table_alpha_nonsel1.csv\",\n",
    "    \"./A_eta_table_alpha_nonsel1.csv\",\n",
    "    \"/mnt/data/A_eta_table_alpha_nonsel1.csv\",\n",
    "]\n",
    "\n",
    "df = None\n",
    "for p in candidate_paths:\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_csv(p)\n",
    "        src_path = p\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(f\"Could not find A_eta_table_alpha_nonsel1.csv in: {candidate_paths}\")\n",
    "\n",
    "print(f\"Loaded A(eta) table from: {src_path}\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# ---------- Pivot to 2D grids ----------\n",
    "mu_vals = np.sort(df[\"mu_DM\"].unique())\n",
    "alpha_vals = np.sort(df[\"alpha\"].unique())\n",
    "A_mat = df.pivot(index=\"mu_DM\", columns=\"alpha\", values=\"A\").sort_index().values  # shape (n_mu, n_alpha)\n",
    "\n",
    "# Sanity constants\n",
    "eps = 1e-300\n",
    "logA = np.log(A_mat + eps)\n",
    "\n",
    "# ========== Check 1: separability of log A(mu, alpha) ≈ f(mu) + g(alpha) ==========\n",
    "# Center by subtracting row/col means; if separable, residual should be ~0\n",
    "row_mean = logA.mean(axis=1, keepdims=True)\n",
    "col_mean = logA.mean(axis=0, keepdims=True)\n",
    "global_mean = logA.mean()\n",
    "resid = logA - row_mean - col_mean + global_mean\n",
    "\n",
    "resid_rms = np.sqrt(np.mean(resid**2))\n",
    "total_rms = np.sqrt(np.mean((logA - global_mean)**2))\n",
    "ratio = resid_rms / (total_rms + 1e-16)\n",
    "\n",
    "print(f\"[Separability check] Residual RMS / total RMS = {ratio:.4e} (closer to 0 -> more separable)\")\n",
    "\n",
    "# Plot residual heatmap\n",
    "plt.figure(figsize=(7, 4), dpi=140)\n",
    "plt.imshow(resid.T, origin=\"lower\",\n",
    "           extent=[mu_vals.min(), mu_vals.max(), alpha_vals.min(), alpha_vals.max()],\n",
    "           aspect=\"auto\")\n",
    "plt.colorbar(label=\"Residual of log A\")\n",
    "plt.xlabel(\"mu_DM\")\n",
    "plt.ylabel(\"alpha\")\n",
    "plt.title(\"Separability residual: log A - <log A>_mu - <log A>_alpha + <log A>\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== Check 2: monotonicity of A w.r.t mu (for each alpha) ==========\n",
    "# Compute the fraction of alphas for which dA/dmu >= 0 everywhere (non-decreasing)\n",
    "dA_dmu = np.diff(A_mat, axis=0)  # shape (n_mu-1, n_alpha)\n",
    "nondec_mask = (dA_dmu >= 0).all(axis=0)  # per alpha\n",
    "frac_nondec = nondec_mask.mean()\n",
    "print(f\"[Monotonicity check] Fraction of alphas with non-decreasing A(mu): {frac_nondec:.3f}\")\n",
    "\n",
    "# Plot a few alpha-slices\n",
    "alpha_quantiles = [0.05, 0.35, 0.65, 0.95]\n",
    "alpha_idx = [int(q * (len(alpha_vals)-1)) for q in alpha_quantiles]\n",
    "plt.figure(figsize=(7, 4), dpi=140)\n",
    "for idx in alpha_idx:\n",
    "    plt.plot(mu_vals, A_mat[:, idx], label=f\"alpha≈{alpha_vals[idx]:.2f}\")\n",
    "plt.xlabel(\"mu_DM\")\n",
    "plt.ylabel(\"A\")\n",
    "plt.title(\"A(mu | alpha slices)\")\n",
    "plt.legend(frameon=False, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show A vs alpha for a few mu slices\n",
    "mu_quantiles = [0.05, 0.35, 0.65, 0.95]\n",
    "mu_idx = [int(q * (len(mu_vals)-1)) for q in mu_quantiles]\n",
    "plt.figure(figsize=(7, 4), dpi=140)\n",
    "for idx in mu_idx:\n",
    "    plt.plot(alpha_vals, A_mat[idx, :], label=f\"mu≈{mu_vals[idx]:.2f}\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"A\")\n",
    "plt.title(\"A(alpha | mu slices)\")\n",
    "plt.legend(frameon=False, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18ee01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecca808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c1371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e08d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec42bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d121c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f80aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c4b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70699568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19826e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29e7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716738ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16af17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad7e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea6577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0ebeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d560ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb623b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "\n",
    "def plot_pairplot_with_truth(hdf5_file, discard=1000, thin=10, labels=None, truths=None, width_factor=1.5):\n",
    "    \"\"\"\n",
    "    使用 Seaborn pairplot 绘制后验图，并以变量均值为中心设置 xlim/ylim，同时标出真值。\n",
    "\n",
    "    参数：\n",
    "    - hdf5_file: emcee 的 HDF5 后端路径\n",
    "    - discard: 前期丢弃步数\n",
    "    - thin: 抽样间隔\n",
    "    - labels: 变量标签列表\n",
    "    - truths: list[float]，真值点，会在每个图上标注\n",
    "    - width_factor: 控制坐标轴范围宽度因子\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    samples = backend.get_chain(discard=discard, thin=thin, flat=True)\n",
    "    print(samples.shape)\n",
    "\n",
    "    ndim = samples.shape[1]\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    plt.figure(dpi=300)\n",
    "    g = sns.pairplot(df, corner=True, diag_kind=\"hist\", plot_kws=dict(s=5, alpha=0.2))\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "\n",
    "            # 设置 xlim 和 ylim 居中\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "            if i == j:\n",
    "                if truths:\n",
    "                    ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "            else:\n",
    "                mean_y = means[labels[i]]\n",
    "                std_y = stds[labels[i]]\n",
    "                ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "                if truths:\n",
    "                    # 添加红色十字真值点\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "    # plt.savefig(\"post.png\",dpi=300)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"./mcmc_checkpoint_eta_new_nomutab.h5\",\n",
    "    discard=2200,\n",
    "    thin=1,\n",
    "    labels=[r\"$\\mu_{\\rm DM}$\", r\"$\\sigma_{\\rm DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    truths=(12.484790712318109,\n",
    " 0.4365785108229181,\n",
    " 0.07376295948617051,\n",
    " 0.0748739106362481),\n",
    "    width_factor=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emcee.backends import HDFBackend\n",
    "import numpy as np\n",
    "\n",
    "# 读取链和 log posterior\n",
    "backend = HDFBackend(\"mcmc_checkpoint_eta_new_nomutab.h5\", read_only=True)\n",
    "samples = backend.get_chain(discard=2200, thin=1, flat=True)\n",
    "log_probs = backend.get_log_prob(discard=2200, thin=1, flat=True)\n",
    "\n",
    "# 找到最大后验的位置（MAP）\n",
    "max_idx = np.argmax(log_probs)\n",
    "map_estimate = samples[max_idx]\n",
    "print(\"MAP estimate:\", map_estimate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.normal(loc=0.0, scale=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b5ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905a140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54119206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from emcee.backends import HDFBackend\n",
    "\n",
    "def plot_posterior_from_hdf5(hdf5_file, discard=1000, thin=10, labels=None, truths=None):\n",
    "    \"\"\"\n",
    "    从 emcee 的 HDF5 后端文件中读取样本并绘制后验角图（corner plot）。\n",
    "\n",
    "    参数：\n",
    "    - hdf5_file: str，HDF5 文件路径\n",
    "    - discard: int，前期 burn-in 步数将被丢弃\n",
    "    - thin: int，抽样间隔，用于降低样本相关性\n",
    "    - labels: list[str]，每个维度的标签，用于 corner 图\n",
    "    - truths: list[float]，真值（如果有），用于在图上标注参考线\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file)\n",
    "\n",
    "    # 获取扁平化样本链\n",
    "    samples = backend.get_chain(discard=discard, thin=thin, flat=True)\n",
    "\n",
    "    ndim = samples.shape[1]\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "\n",
    "    fig = corner.corner(\n",
    "        samples,\n",
    "        labels=[r\"$\\mu_{\\rm DM}$\", r\"$\\sigma_{\\rm DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "        truths=truths,  # 可选\n",
    "        show_titles=True,\n",
    "        title_fmt=\".3f\",\n",
    "        title_kwargs={\"fontsize\": 12},\n",
    "        quantiles=[0.16, 0.5, 0.84],\n",
    "        # plot_contours=False,\n",
    "        plot_density=False,\n",
    "        fill_contours=False,\n",
    "        max_n_ticks=3,\n",
    "        hist_bin_factor=1,\n",
    "        bins=30,\n",
    "        smooth=0.0,\n",
    "        color=\"black\",\n",
    "        scatter_kwargs=dict(s=3, alpha=0.2)  # 控制散点大小和透明度\n",
    "    )\n",
    "\n",
    "\n",
    "# 示例调用（你可替换文件路径和标签）\n",
    "plot_posterior_from_hdf5(\n",
    "    \"./mcmc_checkpoint_eta_new_nomutab.h5\",\n",
    "    discard=2000,\n",
    "    thin=10,\n",
    "    labels=[r\"$\\mu_{\\rm DM}$\", r\"$\\sigma_{\\rm DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    truths=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff73187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from emcee.backends import HDFBackend\n",
    "\n",
    "def plot_pairplot_with_truth(hdf5_file, discard=1000, thin=10, labels=None, truths=None, width_factor=1.5):\n",
    "    \"\"\"\n",
    "    使用 Seaborn pairplot 绘制后验图，并以变量均值为中心设置 xlim/ylim，同时标出真值。\n",
    "\n",
    "    参数：\n",
    "    - hdf5_file: emcee 的 HDF5 后端路径\n",
    "    - discard: 前期丢弃步数\n",
    "    - thin: 抽样间隔\n",
    "    - labels: 变量标签列表\n",
    "    - truths: list[float]，真值点，会在每个图上标注\n",
    "    - width_factor: 控制坐标轴范围宽度因子\n",
    "    \"\"\"\n",
    "    backend = HDFBackend(hdf5_file, read_only=True)\n",
    "    samples = backend.get_chain(discard=discard, thin=thin, flat=True)\n",
    "    print(samples.shape)\n",
    "\n",
    "    ndim = samples.shape[1]\n",
    "    if labels is None:\n",
    "        labels = [f\"$\\\\theta_{i}$\" for i in range(ndim)]\n",
    "    df = pd.DataFrame(samples, columns=labels)\n",
    "\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    plt.figure(dpi=300)\n",
    "    g = sns.pairplot(df, corner=True, diag_kind=\"hist\", plot_kws=dict(s=5, alpha=0.2))\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(i + 1):\n",
    "            ax = g.axes[i, j]\n",
    "            if ax is None:\n",
    "                continue\n",
    "\n",
    "            # 设置 xlim 和 ylim 居中\n",
    "            mean_x = means[labels[j]]\n",
    "            std_x = stds[labels[j]]\n",
    "            ax.set_xlim(mean_x - width_factor * std_x, mean_x + width_factor * std_x)\n",
    "\n",
    "            if i == j:\n",
    "                if truths:\n",
    "                    ax.axvline(truths[i], color=\"red\", linestyle=\"--\", lw=1)\n",
    "            else:\n",
    "                mean_y = means[labels[i]]\n",
    "                std_y = stds[labels[i]]\n",
    "                ax.set_ylim(mean_y - width_factor * std_y, mean_y + width_factor * std_y)\n",
    "                if truths:\n",
    "                    # 添加红色十字真值点\n",
    "                    ax.plot(truths[j], truths[i], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "    # plt.savefig(\"post.png\",dpi=300)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_pairplot_with_truth(\n",
    "    hdf5_file=\"./mcmc_checkpoint_eta_origin2.h5\",\n",
    "    discard=00,\n",
    "    thin=1,\n",
    "    labels=[r\"$\\mu_{\\rm DM}$\", r\"$\\sigma_{\\rm DM}$\", r\"$\\mu_\\alpha$\", r\"$\\sigma_\\alpha$\"],\n",
    "    truths=(12.484790712318109,\n",
    " 0.4365785108229181,\n",
    " 0.07376295948617051,\n",
    " 0.0748739106362481),\n",
    "    width_factor=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6c935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (sys)",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
